{
 "webpage": [
  {
   "title": "1934 - hottest year on record",
   "paragraph": "1934 is the hottest year on record\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nGlobally the year 1934 was cooler than the 20th century average.\nClimate Myth...\n1934 - hottest year on record\nSteve McIntyre noticed a strange discontinuity in US temperature data, occurring around January 2000. McIntyre notified NASA which acknowledged the problem as an 'oversight' that would be fixed in the next data refresh. As a result, \"The warmest year on US record is now 1934. 1998 (long trumpeted by the media as record-breaking) moves to second place.\" (Daily Tech).\nThe year 1934 was a very hot year in the United States, ranking sixth behind 2012, 2016, 2015, 2006, and 1998. However, global warming takes into account temperatures over the entire planet, including the oceans. The land area of the U.S. accounts for only 2% of Earth's total surface area. Despite the U.S. sweltering in 1934, that year was not especially hot over the rest of the planet, as you can see on the 1934 map below. Globally, 1934 temperatures were actually cooler than average for the 20th century.\n1934\n2016\nFigure 1. Global temperature maps for 1934 (top) and 2016 (bottom). Source NASA.\nClimate change skeptics have pointed to 1934 in the U.S. as proof that recent hot years are not unusual. Choosing the year 1934 is an obvious example of \"cherry-picking\" a single fact that supports a claim, while ignoring the rest of the data. In fact they have to cherry pick both a location (the U.S.) and a year (1934) to find data that is far from the global trend. Globally, the years 2014, 2015 and 2016 are the hottest on record, so far.\nFigure 2. Global land and ocean temperatures from 1880 to 2015. Source: National Climate Data Center\nThe fact that there were hot years in some parts of the world in the past is not an argument against global climate change. Regional and year-to-year temperature variations will always occur. The reason we are worried about climate change is that on average, over the entire world, the long term trend shows an undeniable increase in global surface temperatures and global ocean temperatures. This rapid global heating is dramatically altering the planet we live on.\nLast updated on 7 August 2017 by Sarah. View Archives"
  },
  {
   "title": "2nd law of thermodynamics contradicts greenhouse theory",
   "paragraph": "The greenhouse effect and the 2nd law of thermodynamics\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe 2nd law of thermodynamics is consistent with the greenhouse effect which is directly observed.\nClimate Myth...\n2nd law of thermodynamics contradicts greenhouse theory\n\"The atmospheric greenhouse effect, an idea that many authors trace back to the traditional works of Fourier 1824, Tyndall 1861, and Arrhenius 1896, and which is still supported in global climatology, essentially describes a fictitious mechanism, in which a planetary atmosphere acts as a heat pump driven by an environment that is radiatively interacting with but radiatively equilibrated to the atmospheric system. According to the second law of thermodynamics such a planetary machine can never exist.\" (Gerhard Gerlich)\nSkeptics sometimes claim that the explanation for global warming contradicts the second law of thermodynamics. But does it? To answer that, first, we need to know how global warming works. Then, we need to know what the second law of thermodynamics is, and how it applies to global warming. Global warming, in a nutshell, works like this:\nThe sun warms the Earth. The Earth and its atmosphere radiate heat away into space. They radiate most of the heat that is received from the sun, so the average temperature of the Earth stays more or less constant. Greenhouse gases trap some of the escaping heat closer to the Earth's surface, making it harder for it to shed that heat, so the Earth warms up in order to radiate the heat more effectively. So the greenhouse gases make the Earth warmer - like a blanket conserving body heat - and voila, you have global warming. See What is Global Warming and the Greenhouse Effect for a more detailed explanation.\nThe second law of thermodynamics has been stated in many ways. For us, Rudolf Clausius said it best:\n\"Heat generally cannot flow spontaneously from a material at lower temperature to a material at higher temperature.\"\nSo if you put something hot next to something cold, the hot thing won't get hotter, and the cold thing won't get colder. That's so obvious that it hardly needs a scientist to say it, we know this from our daily lives. If you put an ice-cube into your drink, the drink doesn't boil!\nThe skeptic tells us that, because the air, including the greenhouse gasses, is cooler than the surface of the Earth, it cannot warm the Earth. If it did, they say, that means heat would have to flow from cold to hot, in apparent violation of the second law of thermodynamics.\nSo have climate scientists made an elementary mistake? Of course not! The skeptic is ignoring the fact that the Earth is being warmed by the sun, which makes all the difference.\nTo see why, consider that blanket that keeps you warm. If your skin feels cold, wrapping yourself in a blanket can make you warmer. Why? Because your body is generating heat, and that heat is escaping from your body into the environment. When you wrap yourself in a blanket, the loss of heat is reduced, some is retained at the surface of your body, and you warm up. You get warmer because the heat that your body is generating cannot escape as fast as before.\nIf you put the blanket on a tailors dummy, which does not generate heat, it will have no effect. The dummy will not spontaneously get warmer. That's obvious too!\nIs using a blanket an accurate model for global warming by greenhouse gases? Certainly there are differences in how the heat is created and lost, and our body can produce varying amounts of heat, unlike the near-constant heat we receive from the sun. But as far as the second law of thermodynamics goes, where we are only talking about the flow of heat, the comparison is good. The second law says nothing about how the heat is produced, only about how it flows between things.\nTo summarise: Heat from the sun warms the Earth, as heat from your body keeps you warm. The Earth loses heat to space, and your body loses heat to the environment. Greenhouse gases slow down the rate of heat-loss from the surface of the Earth, like a blanket that slows down the rate at which your body loses heat. The result is the same in both cases, the surface of the Earth, or of your body, gets warmer.\nSo global warming does not violate the second law of thermodynamics. And if someone tells you otherwise, just remember that you're a warm human being, and certainly nobody's dummy.\nBasic rebuttal written by Tony Wildish\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nUpdate October 2017:\nHere is a walk-through explanation of the Greenhouse Effect for bunnies, by none other than Eli, over at Rabbit Run.\nLast updated on 7 October 2017 by skeptickev. View Archives"
  },
  {
   "title": "500 scientists refute the consensus",
   "paragraph": "Do 500 scientists refute the consensus?\nLink to this page\nWhat the science says...\nClose inspection of the studies alleged to refute man-made global warming finds that many of these papers do no such thing. Of the few studies that do claim to refute man-made global warming, these repeat well debunked myths.\nClimate Myth...\n500 scientists refute the consensus\n\"According to a report in the WorldNet Daily, more than 500 scientists have published evidence refuting the current man-made global warming scare, according to a new analysis of peer-reviewed literature by the Hudson Institute.\" (American Conservative Daily)\nThe latest attack on global warming consensus comes from Dennis Avery and Fred Singer who claim to have found 500 peer reviewed papers refuting that the last few decades of global warming are primarily anthropogenic. Previous attempts to find peer reviewed skeptic studies tend to miscategorise as skeptic despite the intent of the author or indeed the content of the paper. Avery and Singer appear to carry on this tradition.\nWhile their press release peddles many skeptic myths, the major recurring theme is that over 300 studies have found climate has changed in the past and/or that the sun is connected. Tamino at Open Mind does a good job explaining the 1500 year natural cycles (or Dansgaard-Oeschger events) along with some useful links to relevant peer reviewed studies. I've also touched on the notion that climate has changed naturally in the past so it must be natural now.\nRegarding the sun's connection to global warming (or lack thereof), there is much empirical data and many studies on the topic that have concluded the sun's contribution to global warming has been minimal. Nevertheless, blaming climate change on the sun is intuitive - to paraphrase the Great Global Warming Swindle: \"human small... sun big\". Hard to refute that kind of barnstorming logic. Nevertheless, I'll have another crack at breaking down the logical steps of why we know solar variations aren't causing global warming:\nThe sun has closely correlated with temperature in the past and been a major driver of climate\nThe correlation ended in the 70's when the modern global warming trend began\nTherefore the sun cannot be the driving force of global warming over the past few decades\nFigure 1: Annual global temperature change (thin light red) with 11 year moving average of temperature (thick dark red). Temperature from NASA GISS. Annual Total Solar Irradiance (thin light blue) with 11 year moving average of TSI (thick dark blue). TSI from 1880 to 1978 from Solanki. TSI from 1979 to 2009 from PMOD.\nIn other words, all the studies showing past correlation between solar activity and temperature only serve to emphasise the fact that the correlation no longer exists. Rather than refute the consensus, they reinforce it.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 15 July 2015 by pattimer. View Archives"
  },
  {
   "title": "97% consensus on human-caused global warming has been disproven",
   "paragraph": "The Cook et al. (2013) 97% consensus result is robust\nLink to this page\nWhat the science says...\nThe 97% consensus has been independently confirmed by a number of different approaches and lines of evidence.\nClimate Myth...\n97% consensus on human-caused global warming has been disproven\nCooks ’97% consensus’ disproven by a new peer reviewed paper showing major math errors (Anthony Watts)\nCommunicating the expert consensus is very important in terms of increasing public awareness of human-caused climate change and support for climate solutions. Thus it's perhaps not surprising that Cook et al. (2013) and its 97% consensus result have been the subject of extensive denial among the usual climate contrarian suspects. After all, the fossil fuel industry, right-wing think tanks, and climate contrarians have been engaged in a disinformation campaign regarding the expert climate consensus for over two decades. For example, Western Fuels Association conducted a half-million dollar campaign in 1991 designed to ‘reposition global warming as theory (not fact).’\nThe 97% Consensus is a Robust Result\nNevertheless, the existence of the expert consensus on human-caused global warming is a reality, as is clear from an examination of the full body of evidence. For example, Naomi Oreskes found no rejections of the consensus in a survey of 928 abstracts performed in 2004. Doran & Zimmerman (2009) found a 97% consensus among scientists actively publishing climate research. Anderegg et al. (2010) reviewed publicly signed declarations supporting or rejecting human-caused global warming, and again found over 97% consensus among climate experts. Cook et al. (2013) found the same 97% result through a survey of over 12,000 climate abstracts from peer-reviewed journals, as well as from over 2,000 scientist author self-ratings, among abstracts and papers taking a position on the causes of global warming.\nIn addition to these studies, we have the National Academies of Science from 33 different countries all endorsing the consensus. Dozens of scientific organizations have endorsed the consensus on human-caused global warming. Only one has ever rejected the consensus - the American Association of Petroleum Geologists - and even they shifted to a neutral position when members threatened to not renew their memberships due to its position of climate denial.\nIn short, the 97% consensus on human-caused global warming is a robust result, found using several different methods in various studies over the past decade. It really shouldn't be a surprise at this point, and denying it is, well, denial.\nQuantifying the Human Global Warming Contribution\nThere have also been various studies quantifying the human contribution to global warming, as we have previously documented.\nFigure 1: Net human and natural percent contributions to the observed global surface warming over the past 50-65 years according to Tett et al. 2000 (T00, dark blue), Meehl et al. 2004 (M04, red), Stone et al. 2007 (S07, light green), Lean and Rind 2008 (LR08, purple), Huber and Knutti 2011 (HK11, light blue), Gillett et al. 2012 (G12, orange), Wigley and Santer 2012 (WS12, dark green), and Jones et al. 2013 (J12, pink).\nAgain, there's very little controversy here. The scientific literature is quite clear that humans have caused most of the global surface warming over the past half century, as the 2013 IPCC report stated with 95% confidence.\nIn Cook et al. (2013), we broadened the focus beyond definitions that quantify the human contribution, because there's a consensus gap on the mere question of whether humans are causing global warming. Nevertheless, we used the 2007 IPCC position as one of our consensus position definitions:\n\"We examined a large sample of the scientific literature on global [climate change], published over a 21 year period, in order to determine the level of scientific consensus that human activity is very likely causing most of the current GW (anthropogenic global warming, or AGW).\"\nThe IPCC position (humans causing most global warming) was represented in our categories 1 and 7, which include papers that explicitly endorse or reject/minimize human-caused global warming, and also quantify the human contribution. Among the relatively few abstracts (75 in total) falling in these two categories, 65 (87%) endorsed the consensus view. Among the larger sample size of author self-rated papers in categories 1 and 7 (237 in total), 228 (96%) endorsed the consensus view that humans are causing most of the current global warming.\nThe self-ratings offer a larger sample size on this quantification question because of the limited real estate in a paper's abstract. Most journals have strict word limits on their abstracts, so authors have to focus on the specifics of their research. On the other hand, the author self-ratings are based on the full papers, which have much more real estate and are thus more likely to both take a position on the cause of global warming, and quantify the human contribution.\nConfused Contrarians Think they are Included in the 97%\nThere have been a number of contrarians claiming that they are part of the 97% consensus, which they believe is limited to the position that humans are causing some global warming. The first error in this argument is in ignoring the fact that the data collected in Cook et al. (2013) included categories that quantify the human contribution, as Andrew Montford and the GWPF recently did, for example.\nThe second error has been made by individuals claiming they're in the 97%, but failing to actually check the data. For example, Roy Spencer claimed in testimony to US Congress that he is included in the 97% consensus. Since we made all of our data available to the public, you can see our ratings of Spencer's abstracts here. Five of his papers were captured in our literature search; we categorized four as 'no opinion' on the cause of global warming, and one as implicitly minimizing the human influence. Thus Spencer's research was included in the fewer than 3 percent of papers that either rejected or minimized the human contribution to global warming. Bjorn Lomborg made a similar error, claiming:\n\"Virtually everyone I know in the debate would automatically be included in the 97% (including me, but also many, much more skeptical).\"\nIn reality Lomborg is included neither in the 97+% nor the less than 3% because as far as we can tell, he has not published any peer-reviewed climate research, and thus none of his writings were captured in our literature search. The 97% is a consensus of climate science experts, and that, Lomborg is not.\nNir Shaviv took the opposite approach, claiming he was wrongly included in the 97%. Though Shaviv also admitted that Cook et al. correctly classified his abstracts based on their content, but claimed that he worded the text in a way to slip it past the journal reviewers and editors.\n\"I couldn’t write these things more explicitly in the paper because of the refereeing, however, you don’t have to be a genius to reach these conclusions from the paper.\"\nHowever, Shaviv, Spencer, and all other authors were invited to participate in the self-ratings process that resulted in the sae 97% consensus conclusion.\nTol's Rejected Comment\nRichard Tol has also advanced various criticisms of Cook et al. (2013). It's worth noting that Tol does not dispute the existence of the consensus, writing:\n\"There is no doubt in my mind that the literature on climate change overwhelmingly supports the hypothesis that climate change is caused by humans. I have very little reason to doubt that the consensus is indeed correct.\"\nTol has nevertheless criticized the methods applied during the Cook et al. survey. For example, he has argued that the literature search should have been conducted with Scopus rather than the Web of Science in order to capture more papers, but also that fewer papers should have been included in the survey in order to focus on those specifically researching the causes of global warming. Tol has also applied various statistical tests comparing the abstract ratings to the author self-ratings, but these tests are invalid because the two phases of the survey considered different information (abstracts only vs. full papers) and are thus not comparable.\nIn fact, when we released the self-rating data, we explicitly discussed the difference between the two datasets and how the difference was actually instructive. As John Cook wrote,\n\"That's not to say our ratings of abstracts exactly matched the self-ratings by the papers' authors. On the contrary, the two sets measure different things and not only are differences expected, they're instructive.\"\nUltimately Tol submitted his criticisms to Environmental Research Letters as a comment, but the submission was summarily rejected by the editor who described it as a speculative opinion piece that does not identify any clear errors that would call the paper's conclusions into question.\nIn short, the 97% consensus has passed peer-review, while Tol's criticisms have not. Moreover, all of Tol's criticisms only apply to the abstract ratings, while the self-ratings also found the same 97% consensus result, completely independent from the abstract ratings.\nTaking Consensus Denial to the Extreme\nOne critique of the consensus has been published in a paper in the journal Science & Education. The argument made in the paper was first published by Christopher Monckton on a climate contrarian blog. Monckton has also suggested the conspiracy theory that the journal Environmental Research Letters was created (in 2006) specifically for the purpose of publishing Cook et al. (2013).\nThe Monckton paper takes the point about quantification above to the extreme. It focuses exclusively on the papers that quantified human-caused global warming, and takes these as a percentage of all 12,000 abstracts captured in the literature search, thus claiming the consensus is not 97%, but rather 0.3%. The logical flaws in this argument should be obvious, and thus should not have passed through the peer-review process.\nApproximately two-thirds of abstracts did not take a position on the causes of global warming, for various reasons (e.g. the causes were simply not relevant to or a key component of their specific research paper). Thus in order to estimate the consensus on human-caused global warming, it's necessary to focus on the abstracts that actually stated a position on human-caused global warming.\nWhen addressing the consensus regarding humans being responsible for the majority of recent global warming, the same argument holds true for abstracts that do not quantify the human contribution. We simply can't know their position on the issue - that doesn't mean they endorse or reject the consensus position; they simply don't provide that information, and thus must first be removed before estimating the quantified consensus.\nAs noted above, when we perform this calculation, the consensus position that humans are the main cause of global warming is endorsed in 87% of abstracts and 96% of full papers. Monckton's argument is very similar to the myth that CO2 can't cause significant global warming because it only comprises 0.04% of the atmosphere. 99% of the atmosphere is comprised of non-greenhouse gases, but these other gases are irrelevant to the question of the CO2 greenhouse effect. The percentage of CO2 as a fraction of all gases in the atmosphere is an irrelevant figure, as is the percentage of abstracts quantifying human-caused global warming as a percentage of all abstracts captured in our literature search.\nIt's also worth noting that based on Monckton's logic, only 0.08% of abstracts reject human-caused global warming.\nClimate Consensus Denialism\nOverall, the critiques of Cook et al. (2013) have all exhibited the characteristics of scientific denialism. Given the long history of consensus denial campaigns by fossil fuel interests and climate contrarians, continued resistance to the consensus is an expected result. Nevertheless, the 97% consensus is a robust result from several different studies taking a variety of approaches, including two independent methods used by Cook et al. (abstract ratings and author self-ratings). The criticisms of the paper have all exhibited the same few logical flaws, some more extreme than others, but all erroneous.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 13 June 2016 by pattimer. View Archives"
  },
  {
   "title": "A grand solar minimum could trigger another ice age",
   "paragraph": "A grand solar minimum would barely make a dent in human-caused global warming\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nPeer-reviewed research, physics, and math all tell us that a grand solar minimum would have no more than a 0.3°C cooling effect, barely enough to put a dent in human-caused global warming.\nClimate Myth...\nA grand solar minimum could trigger another ice age\n\"The sun is acting bizarrely and scientists have no idea why. Solar activity is in gradual decline, a change from the norm which in the past triggered a 300-year-long mini ice age\" (Dick Ahlstrom, The Irish Times)\nThe Maunder Minimum was a period of very low solar activity between 1645 and 1715, and the Dalton Minimum was a period of low (but not as low as the Maunder Minimum) solar activity between 1790 and 1830. Solar research suggests that we may have a similar period of low solar activity sometime this century.\n400 years of sunspot observations data, via Wikipedia\nArticles in the Danish newspaper Jyllands-Posten (translation available here) and in the Irish Times both ran headlines claiming that another grand solar minimum could potentially trigger an \"ice age\" or \"mini ice age\" this century. These articles actually refer to the Little Ice Age (LIA) – a period about 500 to 150 years ago when global surface temperatures were about 1°C colder than they are today.\nThus a grand solar minimum would have to cause about 1°C cooling, plus it would have to offset the continued human-caused global warming between 1 and 5°C by 2100, depending on how our greenhouse gas emissions change over the next century.\nFortunately, Solar Output is Stable\nWe're fortunate that the amount of solar radiation reaching the Earth's surface is very stable. Climate contrarians will often ask if we'd prefer if the planet were warming or cooling, suggesting that global warming is a good thing because at least the planet isn't getting colder. This is a false dichotomy - an ideal climate is a stable one. The relatively stable climate over the past 10,000 years has allowed establishment of human civilization, by making it possible to create large stationary agricultural farms because we could rely on stable weather patterns.\nWhat difference would a grand solar minimum make in the amount of solar energy reaching us? Relative to current levels, the Dalton Minimum represents a 0.08% decrease, and the Maunder Minimum represents a 0.25% decline in solar radiation at the Earth's surface. That's how stable solar activity is. That's also why we're playing with fire by increasing the greenhouse effect so much and so quickly. We're threatening the stability of the climate that has been so favorable to our development.\nPeer-Reviewed Research Says Global Warming will Continue\nThere have been several studies in recent years using climate models to see what impact another grand solar minimum would have on global surface temperatures, since solar research suggests it's possible we could be due for another extended minimum. Feulner & Rahmstorf (2010) (PDF available here) estimated that another solar minimum equivalent to the Dalton and Maunder minima would cause 0.09°C and 0.26°C cooling, respectively.\nThe global mean temperature difference is shown for the time period 1900 to 2100 for the IPCC A2 emissions scenario (relative to zero for the average temperature during the years 1961 to 1990). The red line shows predicted temperature change for the current level of solar activity, the blue line shows predicted temperature change for solar activity at the much lower level of the Maunder Minimum, and the black line shows observed temperatures from the NASA GISS dataset through 2010. Adapted from Feulner & Rahmstorf (2010).\nJones et al. (2012) (PDF available here), Anet et al. (2013), and Meehl et al. (2013) (PDF available here arrived at nearly identical results, with cooling from a grand solar minimum causing no more than 0.3°C cooling over the 21st century. Meehl et al. also point out that as soon as solar activity began to rise again, that cooling would be offset by solar warming. This is a key point, because a grand solar minimum would not be a permanent change. These minima last for a few decades, but eventually solar activity rises once again. Thus any cooling caused by a solar minimum would only be temporary.\nThe cooling effect of a grand solar minimum can also be estimated very easily without the aid of climate models. To see the calculation, check the Intermediate level of this myth rebuttal.\nHuman Influence on Climate Change is Bigger than the Sun's\nThe bottom line is that the sun and the amount of solar radiation reaching Earth are very stable. Even during the Maunder and Dalton grand solar minima, global cooling was relatively small - smaller than the amount of global warming caused by human greenhouse gas emissions over the past century.\nA new grand solar minimum would not trigger another LIA; in fact, the maximum 0.3°C cooling would barely make a dent in the human-caused global warming over the next century, likely between 1 and 5°C, depending on how much we manage to reduce our fossil fuel consumption and greenhouse gas emissions. While this is equivalent to about a decade's worth of human-caused warming, it's also important to bear in mind that any solar cooling would only be temporary, until the end of the solar minimum.\nThe science is quite clear that the human influence on climate change has become bigger than the sun's.\nAlso see this excellent video by Peter Sinclair of Climate Crocks debunking this myth.\nLast updated on 4 March 2018 by dana1981. View Archives"
  },
  {
   "title": "Adapting to global warming is cheaper than preventing it",
   "paragraph": "What's cheaper, mitigation or adaptation?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nWhile preventing global warming is relatively cheap, economists can't even accurately estimate the accelerating costs of climate damages if we continue with business-as-usual.\nClimate Myth...\nAdapting to global warming is cheaper than preventing it\n\"If we don’t do anything, the damages caused by climate change will cost less than 2 per cent of GDP in about 2070. Yet the cost of doing something will likely be higher than 6 per cent of GDP\" (Bjorn Lomborg)\nSome in the media have incorrectly argued that the IPCC reports conclude it's cheaper to adapt than avoid climate change. This error stems from the fact that the second report says about the costs of climate damages,\n\"the incomplete estimates of global annual economic losses for additional temperature increases of ~2°C are between 0.2 and 2.0% of income ... Losses are more likely than not to be greater, rather than smaller, than this range ... Losses accelerate with greater warming, but few quantitative estimates have been completed for additional warming around 3°C or above.\"\nThe third report then said about the costs of avoiding global warming,\n\"mitigation scenarios that reach atmospheric concentrations of about 450ppm CO2eq by 2100 entail losses in global consumption—not including benefits of reduced climate change as well as cobenefits and adverse side‐effects of mitigation ... [that] correspond to an annualized reduction of consumption growth by 0.04 to 0.14 (median: 0.06) percentage points over the century relative to annualized consumption growth in the baseline that is between 1.6% and 3% per year.\"\nThe challenge is that these two numbers aren't directly comparable. One deals with annual global economic losses, while the other is expressed as a slightly slowed global consumption growth. While these numbers can be put in terms of their net impact on economic growth, the next problem is that the first is not a proper estimate of the costs of climate damages. The IPCC was only able to estimate the costs of climate damages for another 2°C warming, but limiting global warming to another 2°C will require substantial mitigation efforts.\nThus this estimate only tells us the costs of global warming in a scenario where we also act to significantly reduce greenhouse gas emissions. As the second report notes, economists can't even accurately estimate the costs of climate damages in a business-as-usual scenario with global warming well above an additional 3°C. So how do we determine the economically optimal path?\nSorting Out the Numbers with Chris Hope\nTo answer this question, I spoke with Cambridge climate economist Chris Hope, who told me that if the goal is to figure out the economically optimal amount of global warming mitigation, the IPCC reports \"don't take us far down this road.\" To do this comparison properly, the benefits of reduced climate damages and the costs of reduced greenhouse gas emissions in various scenarios need to be compared. That's the sort of estimate Integrated Assessment Models like Hope's PAGE were set up to make.\nAccording to Hope's model, the economically optimal peak atmospheric carbon dioxide concentration is around 500 ppm, with a peak global surface warming of about 3°C above pre-industrial temperatures (about 2°C warmer than present). In his book The Climate Casino, Yale economist William Nordhaus notes that he has arrived at a similar conclusion in his modeling research.\nTo limit global warming to that level would require major efforts to reduce greenhouse gas emissions, but as the IPCC report on mitigation noted, that would only slow the global economic growth rate from about 2.3% per year to about 2.24% per year. According to these economic models, this slowed economic growth rate would be more than offset by the savings from avoiding climate damages above 3°C global warming.\nAlthough the IPCC didn't make this comparison, these economic modeling results are consistent with its reports. As shown in the quote above, the second report was only able to estimate the costs of climate damages for an additional 2°C of global warming, and noted that beyond that point, the costs accelerate to a point where they become very difficult to estimate. Nordhaus has similarly noted,\n\"In reality, estimates of damage functions are virtually non-existent for temperature increases above 3°C.\"\nNote that these estimates also only take economic factors into account, and don't account for other social, cultural, or ethical concerns like species extinctions or human suffering and deaths.\nLast updated on 24 July 2016 by dana1981. View Archives"
  },
  {
   "title": "Akasofu Proved Global Warming is Just a Recovery from the Little Ice Age",
   "paragraph": "Akasofu's Magical Thinking was Wrong\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nAkasofu's argument is based on magical thinking and curve fitting without any physical explanation. Climate changes must have a physical cause, for example the increased greenhouse effect.\nClimate Myth...\nAkasofu Proved Global Warming is Just a Recovery from the Little Ice Age\n\"The rise in global average temperature over the last century has halted since roughly the year 2000, despite the fact that the release of CO2 into the atmosphere is still increasing. It is suggested here that this interruption has been caused by the suspension of the near linear (+ 0.5 °C/100 years or 0.05 °C/10 years) temperature increase over the last two centuries, due to recovery from the Little Ice Age...\" (Syun-Ichi Akasofu)\nOne of the most important concepts to understand when trying to grasp how the Earth’s climate works, is that every climate change must have a physical cause. Over the past century, climate scientists have developed a solid understanding about how the climate works and the physical mechanisms that cause it to change. By building that knowledge into complex climate models, scientists have been able to accurately reproduce past observed global surface temperature changes.\nGlobal mean near-surface temperatures from observations (black) and as obtained from 58 simulations produced by 14 different climate models driven by both natural and human-caused factors that influence climate in the 2007 IPCC report (yellow). The mean of all these runs is also shown (thick red line). Vertical grey lines indicate the timing of major volcanic eruptions.\nFor example, we know the increased greenhouse effect is creating a global energy imbalance that will cause the Earth's surface temperature to rise. Any alternative explanation has to identify why the increased greenhouse effect isn't causing the warming we expect based on fundamental physics, and why the climate change 'fingerprints' are consistent with the increased greenhouse effect.\nA brand new scientific journal called Climate published a paper by Syun-Ichi Akasofu, a retired geophysicist and former director of the International Arctic Research Center at the University of Alaska-Fairbanks. Despite having a background in physical sciences, Akasofu made a very unphysical argument in that paper. He claimed that the current global warming is merely a result of the planet “recovering” from the Little Ice Age – a cool period (the cooling mostly isolated in Europe) that lasted between the years of about 1550 and 1850.\nProblem – Akasofu didn’t identify any physical cause for this supposed ‘recovery.’ Instead he engaged in what’s known as “curve fitting,” in which you take data that is correlated to your desired graph and scale it to match, then argue you’ve proven that your data is the cause of the changes shown in that graph. In other words, it confuses correlation with causation. If I can take data regarding the number of pirates in the Caribbean and consumption of spaghetti in Ireland and make it fit the global temperature data, that doesn’t mean that pirates and Irish spaghetti are causing global warming. A physical cause must be identified.\nAkasofu didn’t do that. He just roughly fit some ocean cycle data to the global temperature measurements and decided that a linear global warming trend was left over. He then declared that linear trend was the “recovery” from the Little Ice Age, and that it would continue indefinitely into the future, despite not knowing its cause.\nUnfortunately the peer-review process isn’t perfect. It’s necessary but insufficient in separating the good from the flawed research. Sometimes a bad paper will slip through the cracks, whether due to a poor choice of reviewers, or the judgment of the journal editor. Akasofu’s paper was published in the very first edition of Climate, which caused great concern amongst its editorial staff (many of whom recognized the poor quality of the paper), and even caused one editor to resign from the journal.\nClimate also published Nuccitelli et al. (2013), which debunked Akasofu's paper mainly by pointing out that it had no physical basis. Akasofu’s paper also focused on the claim that global warming has “halted,” but Nuccitelli et al. (2013) also pointed out that studies that have accounted for the warming of the entire climate (oceans, air, ice, and land) have shown that if anything, global warming is accelerating.\nGlobal heat accumulation data (ocean heating in blue; land, atmosphere, and ice heating in red) from Nuccitelli et al. (2012)\nThe problem with many anything but carbon (ABC) climate contrarian hypotheses like Akasofu’s is that they throw out what we know about how the Earth’s climate works. It’s fine to try and account the influences of ocean cycles – that’s what mainstream climate scientists are doing – but we’ve known how the Earth warms in response to the increased greenhouse effect for over a century. We also know that any long-term global warming must be caused by a global energy imbalance.\nAny valid climate research has to work within that known framework. When you throw out everything we know about the Earth’s climate, you’re stuck making unphysical arguments based on nothing more than correlations and curve fitting, as Akasofu did. The problem for climate contrarians is that our existing climate framework is very solid. We understand the fundamentals about how the climate operates well enough to accurately reproduce the observed changes, based on solid, well-understood physical mechanisms like the increased greenhouse effect. That’s not about to get overturned by magical thinking and curve fitting.\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 23 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "Al Gore got it wrong",
   "paragraph": "Is Al Gore's An Inconvenient Truth accurate?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nAl Gore's film was \"broadly accurate\" according to an expert witness called when an attempt was made through the courts to prevent the film being shown in schools.\nClimate Myth...\nAl Gore got it wrong\n“Al Gore's Oscar-winning documentary on global warming, An Inconvenient Truth, was […] criticised by a high court judge who highlighted what he said were \"nine scientific errors\" in the film.\nMr Justice Barton yesterday said that while the film was \"broadly accurate\" in its presentation of climate change, he identified nine significant errors in the film, some of which, he said, had arisen in \"the context of alarmism and exaggeration\" to support the former US vice-president's views on climate change.” (The Guardian)\nAl Gore, certainly the most vilified proponent of climate change anywhere in the world, earned most of this enmity through the success of a film he presented called An Inconvenient Truth (AIT). The film was a staid presentation of climate science to date, a round-up of research, science and projections, with many cinematic sequences employed to harness the power of the medium.\nThe majority of the film, covering issues like Himalayan Glaciers, Greenland and Antarctica losing ice, the severity of hurricanes and other weather phenomena, was accurate and represented the science as it stood. Since the release of the film, considerably more evidence has been found in support of the science and projections in the film.\nOne claim was in error, as was one attribution of a graph. The error was in the claim that climate change had caused the shrinking of Mount Kilimanjaro, although the evidence that the shrinkage was most likely caused by deforestation did not appear until after the film was made. The error of attribution was in reference to a graph of temperature and attributes it mistakenly to a Dr. Thompson, when it was actually a combination of Mann’s hockey stick and CRU surface temperature data.\nThe Legal Case\nThe film is also subject to attack on the grounds that Al Gore was prosecuted in the UK and a judge found many errors in the film. This is untrue.\nThe case, heard in the civil court, was brought by a school governor against the Secretary of State for Education, in an attempt to prevent the film being distributed to schools. Mr. Justice Burton, in his judgement, ordered that teaching notes accompanying the film should be modified to clarify the speculative (and occasionally hyperbolic) presentation of some issues.\nMr. Justice Burton found no errors at all in the science. In his written judgement, the word error appears in quotes each time it is used – nine points formed the entirety of his judgement - indicating that he did not support the assertion the points were erroneous. About the film in general, he said this:\n17. I turn to AIT, the film. The following is clear:\ni) It is substantially founded upon scientific research and fact, albeit that the science is used, in the hands of a talented politician and communicator, to make a political statement and to support a political programme.\n22. I have no doubt that Dr Stott, the Defendant's expert, is right when he says that:\n\"Al Gore's presentation of the causes and likely effects of climate change in the film was broadly accurate.\"\nThe judge did identify statements that had political implications he felt needed qualification in the guidance notes for teachers, and ordered that both qualifications on the science and the political implications should be included in the notes. Al Gore was not involved in the case, was not prosecuted, and because the trial was not a criminal case, there was no jury, and no guilty verdict was handed down.\nNote: the vilification of Al Gore is best understood in the context of personalisation. When opponents attack something abstract - like science - the public may not associate with the argument. By giving a name and a face and a set of behavioural characteristics - being a rich politician, for example - it is easy to create a fictional enemy through inference and association. Al Gore is a successful politician who presented a film, his training and experience suitable to the task. To invoke Gore is a way to obfuscate about climate science, for which Gore has neither responsibility, claim nor blame.\nBasic rebuttal written by GPWayne\nLast updated on 7 January 2014 by Bob Lacatena. View Archives"
  },
  {
   "title": "An exponential increase in CO2 will result in a linear increase in temperature",
   "paragraph": "Fast-rising CO2 levels accelerating global warming\nLink to this page\nWhat the science says...\nDespite the logarithmic relationship between CO2 and surface temperatures, atmospheric CO2 levels are rising so fast that unless we dramatically decrease our emissions, global warming will accelerate over the 21st Century.\nClimate Myth...\nAn exponential increase in CO2 will result in a linear increase in temperature\nThere is a logarithmic relationship between radiative forcing (which is directly proportional to the change in surface temperature at equilibrium) and the atmospheric CO2 increase. Note that we are not currently at equilibrium as there is a planetary energy imbalance, and thus further warming 'in the pipeline' from the carbon we've already emitted. Therefore, estimates of the rate of warming due to CO2 thus far will will be underestimates, unless accounting for this 'warming in the pipeline'.\nThis logarithmic relationship means that each doubling of atmospheric CO2 will cause the same amount of warming at the Earth's surface. Thus if it takes as long to increase atmospheric CO2 from 560 to 1120 parts per million by volume (ppmv) as it did to rise from 280 to 560 ppmv, for example, then the associated warming at the Earth's surface will be roughly linear. So the question then becomes, how fast do we expect atmospheric CO2 to rise over the next century?\nHow Fast will Atmospheric CO2 Rise?\nThe IPCC addressed this question by examining a number of different anthropogenic emissions scenarios. Scenario A1F1 assumes high global economic growth and continued heavy reliance on fossil fuels for the remainder of the century. Scenario B1 assumes a major move away from fossil fuels toward alternative and renewable energy as the century progresses. Scenario A2 is a middling scenario, with less even economic growth and some adoption of alternative and renewable energy sources as the century unfolds. The projected atmospheric CO2 levels for these scenarios is shown in Figure 1.\nFigure 1: Atmospheric CO2 concentrations as observed at Mauna Loa from 1958 to 2008 (black dashed line) and projected under the 6 IPCC emission scenarios (solid coloured lines). (IPCC Data Distribution Centre)\nIn short, following the 'business as usual' approach without major steps to move away from fossil fuels and limit greenhouse gas emissions, we will likely reach 850 to 950 ppmv of atmospheric CO2 by the year 2100. It will have taken approximately 200 years (from 1850 to 2050) for the first doubling of atmospheric CO2 from 280 to 560 ppmv, but it will only take another 70 years or so to double the levels again to 1120 ppmv. This will result in an accelerating rate of global warming, not a linear rate. Under Scenarios A2 and A1F1, the IPCC report projects that the global temperature in 2095 will be 2.0–6.4°C above 1990 levels (2.6-7.0°C above pre-industrial), with a best estimate of 3.4 and 4.0°C warmer (4.0 and 4.6°C above pre-industrial average surface temperatures), respectively.\nFigure 2: Global surface temperature projections for IPCC Scenarios. Shading denotes the ±1 standard deviation range of individual model annual averages. The orange line is constant CO2 concentrations at year 2000 values. The grey bars at right indicate the best estimate (solid line within each bar) and the likely range. (Source: IPCC).\nLife in the Fast Lane\nSome skeptics have claimed that these projected amounts of warming have not been borne out in the surface temperature changes over the past decade. But there are many factors which impact short-term global temperatures, which may conceal the long-term warming caused by increasing atmospheric CO2. So if we want to know if the IPCC projections are realistic, rather than examining noisy short-term temperature data, we should examine how much atmospheric CO2 is increasing.\nWhen we look at this data, we find that observed CO2 emissions in recent years have actually been tracking close to or above the worst case (A1F1) scenario.\nFigure 3: Observed global CO2 emissions from fossil fuel burning and cement production compared with IPCC emissions scenarios. The coloured area covers all scenarios used to project climate change by the IPCC (Copenhagen Diagnosis).\nWhat Lies Ahead\nSo if we continue in a business-as-usual scenario, we should expect to see atmospheric CO2 levels accelerate rapidly enough to more than offset the logarithmic relationship with temperature, and cause the surface temperature warming to accelerate as well. Cllaims of a linear increase in temperature ignore that in the 'business as usual' scenario, we are currently on pace to double the current atmospheric CO2 concentration (390 to 780 ppmv) within the next 60 to 80 years, and we have not yet even come close to doubling the pre-industrial concentration (280 ppmv) in the past 150 years. Thus the exponential increase in CO2 will outpace its logarithmic relationship with surface temperature, causing global warming to accelerate unless we take serious steps to reduce greenhouse gas emissions. In fact, to continue the current rate of warming over the 21st Century, we would need to achieve IPCC scenario B1 - a major move away from fossil fuels toward alternative and renewable energy.\nLast updated on 28 January 2011 by dana1981."
  },
  {
   "title": "Animal agriculture and eating meat are the biggest causes of global warming",
   "paragraph": "How much does animal agriculture and eating meat contribute to global warming?\nLink to this page\nWhat the science says...\nAnimal agriculture is responsible for 13–18% of human-caused greenhouse gas emissions globally, and less in developed countries (e.g. 3% in the USA). Fossil fuel combustion for energy and transportation is responsible for approximately 64% of human-caused greenhouse gas emissions globally, and more in developed countries (e.g. 80% in the USA).\nClimate Myth...\nAnimal agriculture and eating meat are the biggest causes of global warming\nBecoming Vegan or cutting down on your own personal meat consumption could be the single most effective action that you can do to help reduce green house gas emissions.\nPlanet Earth Herald\nThe burning of fossil fuels for energy and animal agriculture are two of the biggest contributors to global warming, along with deforestation. Globally, fossil fuel-based energy is responsible for about 64% of human greenhouse gas emissions, with deforestation at about 18%, and animal agriculture between 13% and 18% (estimates from the World Resources Institute, UN Food and Agriculture Organization, and Pitesky et al. 2009).\nGlobal human greenhouse gas emissions breakdown, from the World Resources Institute.\nSo, animal agriculture and meat consumption are significant contributors to global warming, but far less so than fossil fuel combustion. Moreover, fossil fuels are an even bigger contributor to the problem in developed countries, which use more energy and have increased livestock production efficiency (Pitesky et al. 2009). For example, in the United States, fossil fuel-based energy is responsible for about 80% of total greenhouse gas emissions as compared to about 3% from animal agriculture (estimates from the World Resources Institute and Pitesky et al. 2009).\nUS human greenhouse gas emissions flowchart, from the World Resources Institute.\nHow does animal agriculture cause global warming?\nOne of the main ways in which the livestock sector contributes to global warming is through deforestation caused by expansion of pasture land and arable land used to grow feedcrops. Overall, animal agriculture is responsible for about 9% of human-caused carbon dioxide emissions globally (UN FAO).\nAnimal agriculture is also a significant source of other greenhouse gases. For example, ruminant animals like cattle produce methane, which is a greenhouse gas about 20 times more potent than carbon dioxide. The livestock sector is responsible for about 37% of human-caused methane emissions, and about 65% of human nitrous oxide emissions (mainly from manure), globally (UN FAO).\nBeef is a bigger problem than other sources of meat\nProducing beef requires significantly more resources (e.g. land, fertilizer, and water) than other sources of meat. As ruminant animals, cattle also produce methane that other sources (e.g. pigs and chickens) don't.\nEschel et al. 2014 estimated that producing beef requires 28 times more land, 6 times more fertilizer and 11 times more water than producing pork or chicken. As a result, the study estimated that producing beef releases 4 times more greenhouse gases than a calorie-equivalent amount of pork, and 5 times as much as an equivalent amount of poultry.\nEating vegetables produces lower greenhouse gas emissions yet. For example, potatoes, rice, and broccoli produce approximately 3–5 times lower emissions than an equivalent mass of poultry and pork (Environmental Working Group 2011). The reason is simple – it's more efficient to grow a crop and eat it than to grow a crop, feed it to an animal as it builds up muscle mass, then eat the animal.\nGreenhouse gas lifecycle assessment for common proteins and vegetables (EWG 2011).\nHow do the numbers get misrepresented?\nThere are often suggestions that going vegan is the most important step people can take to solve the global warming problem. While reducing meat consumption (particularly beef and lamb) reduces greenhouse gas emissions, this claim is an exaggeration.\nAn oft-used comparison is that globally, animal agriculture is responsible for a larger proportion of human-caused greenhouse gas emissions (14-18%) than transportation (13.5%). While this is true, transportation is just one of the many sources of human fossil fuel combustion. Electricity and heat generation account for about 25% of global human greenhouse gas emissions alone.\nMoreover, in developed countries where the 'veganism will solve the problem' argument is most frequently made, animal agriculture is responsible for an even smaller share of the global warming problem than fossil fuels. For example, in the USA, fossil fuels are responsible for over 10 times more human-caused greenhouse gas emissions than animal agriculture.\nThat's not to minimize the significant global warming impact of animal agriculture (as well as its other adverse environmental impacts), especially from beef and lamb, but it's also important not to exaggerate its contribution or minimize the much larger contribution of fossil fuels.\nLast updated on 28 September 2017 by dana1981. View Archives"
  },
  {
   "title": "Animals and plants can adapt",
   "paragraph": "Can animals and plants adapt to global warming?\nLink to this page\nWhat the science says...\nA large number of ancient mass extinction events have been strongly linked to global climate change. Because current climate change is so rapid, the way species typically adapt (eg - migration) is, in most cases, simply not be possible. Global change is simply too pervasive and occurring too rapidly.\nClimate Myth...\nAnimals and plants can adapt\n[C]orals, trees, birds, mammals, and butterflies are adapting well to the routine reality of changing climate.\" (source: Hudson Institute)\nHumans are transforming the global environment. Great swathes of temperate forest in Europe, Asia and North America have been cleared over the past few centuries for agriculture, timber and urban development. Tropical forests are now on the front line. Human-assisted species invasions of pests, competitors and predators are rising exponentially, and over-exploitation of fisheries, and forest animals for bush meat, to the point of collapse, continues to be the rule rather than the exception.\nDriving this has been a six-fold expansion of the human population since 1800 and a 50-fold increase in the size of the global economy. The great modern human enterprise was built on exploitation of the natural environment. Today, up to 83% of the Earth’s land area is under direct human influence and we entirely dominate 36% of the bioproductive surface. Up to half the world’s freshwater runoff is now captured for human use. More nitrogen is now converted into reactive forms by industry than all by all the planet’s natural processes and our industrial and agricultural processes are causing a continual build-up of long-lived greenhouse gases to levels unprecedented in at least the last 800,000 years and possibly much longer.\nClearly, this planet-wide domination by human society will have implications for biological diversity. Indeed, a recent review on the topic, the 2005 Millennium Ecosystem Assessment report (an environmental report of similar scale to the Intergovernmental Panel on Climate Change Assessment Reports), drew some bleak conclusions – 60% of the world’s ecosystems are now degraded and the extinction rate is now 100 to 1000 times higher than the “background” rate of long spans of geological time. For instance, a study I conducted in 2003 showed that up to 42% of species in the Southeast Asian region could be consigned to extinction by the year 2100 due to deforestation and habitat fragmentation alone.\nFigure 1: Southeast Asian extinctions projected due to habitat loss (source: Sodhi, N. S., Koh, L. P., Brook, B. W. & Ng, P. K. L. 2004)\nGiven these existing pressures and upheavals, it is a reasonable question to ask whether global warming will make any further meaningful contribution to this mess. Some, such as the sceptics S. Fred Singer and Dennis Avery, see no danger at all, maintaining that a warmer planet will be beneficial for mankind and other species on the planet and that “corals, trees, birds, mammals, and butterflies are adapting well to the routine reality of changing climate”. Also, although climate change is a concern for conservation biologists, it is not the focus for most researchers (at present), largely I think because of the severity and immediacy of the damage caused by other threats.\nGlobal warming to date has certainly affected species’ geographical distributional ranges and the timing of breeding, migration, flowering, and so on. But extrapolating these observed impacts to predictions of future extinction risk is challenging. The most well known study to date, by a team from the UK, estimated that 18 and 35% of plant and animal species will be committed to extinction by 2050 due to climate change. This study, which used a simple approach of estimating changes in species geographical ranges after fitting to current bioclimatic conditions, caused a flurry of debate. Some argued that it was overly optimistic or too uncertain because it left out most ecological detail, while others said it was possibly overly pessimistic, based on what we know from species responses and apparent resilience to previous climate change in the fossil record – see below.\nA large number of ancient mass extinction events have indeed been strongly linked to global climate change, including the most sweeping die-off that ended the Palaeozoic Era, 250 million years ago and the somewhat less cataclysmic, but still damaging, Palaeocene–Eocene Thermal Maximum, 55 million years ago. Yet in the more recent past, during the Quaternary glacial cycles spanning the last million years, there were apparently few climate-related extinctions. This curious paradox of few ice age extinctions even has a name – it is called ‘the Quaternary Conundrum’.\nOver that time, the globally averaged temperature difference between the depth of an ice age and a warm interglacial period was 4 to 6°C – comparable to that predicted for the coming century due to anthropogenic global warming under the fossil-fuel-intensive, business-as-usual scenario. Most species appear to have persisted across these multiple glacial–interglacial cycles. This can be inferred from the fossil record, and from genetic evidence in modern species. In Europe and North America, populations shifted ranges southwards as the great northern hemisphere ice sheets advanced, and reinvaded northern realms when the glaciers retreated. Some species may have also persisted in locally favourable regions that were otherwise isolated within the tundra and ice-strewn landscapes. In Australia, a recently discovered cave site has shown that large-bodied mammals (‘megafauna’) were able to persist even in the arid landscape of the Nullarbor in conditions similar to now.\nHowever, although the geological record is essential for understanding how species respond to natural climate change, there are a number of reasons why future impacts on biodiversity will be particularly severe:\nA) Human-induced warming is already rapid and is expected to further accelerate. The IPCC storyline scenarios such as A1FI and A2 imply a rate of warming of 0.2 to 0.6°C per decade. By comparison, the average change from 15 to 7 thousand years ago was ~0.005°C per decade, although this was occasionally punctuated by short-lived (and possibly regional-scale) abrupt climatic jolts, such as the Younger Dryas, Dansgaard-Oeschger and Heinrich events.\nB) A low-range optimistic estimate of 2°C of 21st century warming will shift the Earth’s global mean surface temperature into conditions which have not existed since the middle Pliocene, 3 million years ago. More than 4°C of atmospheric heating will take the planet’s climate back, within a century, to the largely ice-free world that existed prior to about 35 million years ago. The average ‘species’ lifetime’ is only 1 to 3 million years. So it is quite possible that in the comparative geological instant of a century, planetary conditions will be transformed to a state unlike anything that most of the world’s modern species have encountered.\nC) As noted above, it is critical to understand that ecosystems in the 21st century start from an already massively ‘shifted baseline’ and so have lost resilience. Most habitats are already degraded and their populations depleted, to a lesser or greater extent, by past human activities. For millennia our impacts have been localised although often severe, but during the last few centuries we have unleashed physical and biological transformations on a global scale. In this context, synergies (positive or self-reinforcing feedbacks) from global warming, ocean acidification, habitat loss, habitat fragmentation, invasive species, chemical pollution (Figure 2) are likely lead to cascading extinctions. For instance, over-harvest, habitat loss and changed fire regimes will likely enhance the direct impacts of climate change and make it difficult for species to move to undamaged areas or to maintain a ‘buffer’ population size. One threat reinforces the other, or multiple impacts play off on each other, which makes the overall impact far greater than if each individual threats occurred in isolation (Brook et al 2008).\nFigure 2: Figure from the Millennium Ecosystem Assessment\nD) Past adaptation to climate change by species was mainly through shifting their geographic range to higher or lower latitudes (depending on whether the climate was warming or cooling), or up and down mountain slopes. There were also evolutionary responses – individuals that were most tolerant to new conditions survived and so made future generations more intrinsically resilient. Now, because of points A to C described above, this type of adaptation will, in most cases, simply not be possible or will be inadequate to cope. Global change is simply too pervasive and occurring too rapidly. Time’s up and there is nowhere for species to run or hide.\nIntermediate rebuttal written by Daniel Bailey\nUpdate July 2015:\nHere are related lecture-videos from Denial101x - Making Sense of Climate Science Denial\nAdditional videos from the MOOC\nInterviews with various experts on Coral bleaching and Ocean acidification\nExpert interview with Annamieke Van De Heuvel\n(experiments in the lab on coral adaption)\nExpert interview with Gregg Webb\n(obtaining data in the field about past coral adaption)\nExpert interview with Christine Hosking\n(impacts of climate change on wildlife ecosystems.... research into koalas)\nExpert interview with Charlie Veron\n(on the notion that the oceans were (thought to be) indestructible)\nExpert interview with Ove Hoegh-Guldberg\n(overview of impacts on coral reefs)\nLast updated on 11 October 2016 by pattimer. View Archives"
  },
  {
   "title": "Antarctica is gaining ice",
   "paragraph": "Is Antarctica losing or gaining ice?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nAntarctic sea ice is gaining sea ice but Antarctica is losing land ice at an accelerating rate, which has implications for sea level rise.\nClimate Myth...\nAntarctica is gaining ice\n\"[Ice] is expanding in much of Antarctica, contrary to the widespread public belief that global warming is melting the continental ice cap.\" (Greg Roberts, The Australian)\nArguments that we needn't worry about loss of ice in the Antarctic because sea ice is growing or even that sea ice in the Antarctic disproves that global warming is a real concern hinge on confusion about differences between sea and land ice, and what our best information about Antarctic ice tells us.\nAs well, the trend in Antarctic sea ice is not a permanent feature, as we'll see. But let's look at the main issues first.\nSea ice doesn't play a role in sea level rise or fall.\nMelting land ice contributes to sea level rise.\nThe net, total behavior of all ice in the Antarctic is causing a significant and accelerating rise in sea level.\nAntarctic sea ice is ice which forms in salt water mostly during winter months. When sea ice melts, sea level does not change.\nAntarctic land ice is the ice which has accumulated over thousands of years in Antarctica by snowfall. This land ice is stored ocean water that once fell as precipitation. When this ice melts, the resulting water returns to the ocean, raising sea level.\nWhat's up with Antarctic sea ice?\nAt both poles, sea ice grows and shrinks on an annual basis. While the maximum amount of cover varies from year to year, there is no effect on sea level due to this cyclic process.\nFigure 1: Coverage of sea ice in both the Arctic (Top) and Antarctica (Bottom) for both summer minimums and winter maximums. Source: National Snow and Ice Data Center\nTrends in Antarctic sea ice are easily deceptive. For many years, Antarctic sea was increasing overall, bu that shows signs of changing as ice extent has sharply declined more recently. Meanwhile, what's the relationship of sea ice to our activities? Ironically, plausible reasons for change may be of our own making:\nOzone levels over Antarctica have dropped causing stratospheric cooling and increasing winds which lead to more areas of open water that can be frozen (Gillet 2003, Thompson 2002, Turner 2009).\nThe Southern Ocean is freshening because of increased rain and snowfall as well as an increase in meltwater coming from the edges of Antarctica's land ice (Zhang 2007, Bintanga et al. 2013). Together, these change the composition of the different layers in the ocean there causing less mixing between warm and cold layers and thus less melted sea and coastal land ice.\nAgainst those factors, we continue to search for final answers to why certain areas of Antarctic sea ice grew over the past few decades (Turner et al, 2015).\nMore lately, sea ice in southern latitudes has shown a precipitous year-on-year decline. (Parkinson, 2019) While there's a remaining net increase in annual high point sea ice, the total increase has been sharply reduced and continues to decline.\nHow is Antarctic land ice doing?\nWe've seen that Antarctic sea ice is irrelevant to the main problem we're facing with overall loss of ice in the Antarctic: rising sea level. That leaves land ice to consider.\nFigure 2: Total Antarctic land ice changes and approximate sea level contributions using a combination of different measurement techniques (IMBIE, 2017). Shaded areas represent measurement uncertainty.\nEstimates of recent changes in Antarctic land ice (Figure 2) show an increasing contribution to sea level. Between 1992 and 2017, the Antarctic Ice Sheets overall lost 2,720 giga-tonnes (Gt) or 2,720,000,000,000 tonnes into the oceans, at an average rate of 108 Gt per year (Gt/yr). Because a reduction in mass of 360 Gt/year represents an annual global-average sea level rise of 1 mm, these estimates equate to an increase in global-average sea levels by 0.3 mm/yr.\nThere is variation between regions within Antarctica as can be seen in Figure 2. The West Antarctic Ice Sheet and the Antarctic Peninsula Ice Sheet are losing a lot of ice mass, at an overall increasing rate. The East Antarctic Ice Sheet has grown slightly over the period shown. The net result is a massive loss of ice.\nTakeaway\nIndependent data from multiple measurement techniques (explained here) show the same thing: Antarctica is losing land ice as a whole and these losses are accelerating. Meanwhile, Antarctic sea ice is irrelevant to what's important about Antarctic ice in general.\nBasic rebuttal written by mattking\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 31 January 2020 by BaerbelW . View Archives"
  },
  {
   "title": "Antarctica is too cold to lose ice",
   "paragraph": "Why do glaciers lose ice?\nLink to this page\nWhat the science says...\nAntarctica is losing ice because its glaciers are speeding up. This is due to melt water lubricating the base of the glaciers and the removal of ice shelves which act as a \"speed bump\" slowing the glacier flow. The ice shelves are thinning due to warming ocean waters.\nClimate Myth...\nAntarctica is too cold to lose ice\n\"The real issue is absolute temperatures. Some of the regions in which GRACE claims ice loss in East Antarctica average colder than -30°C during the summer, and never, ever get above freezing. How can you melt ice at those temperatures?\" (Steve Goddard)\nGlaciers are large viscous masses of ice which creep naturally through a process called internal deformation. This “creep” or movement is caused by gravity and the weight of accumulated snow and ice forcing the ice to deform like plastic.\n(Figure 1: Midtsdalsbreen, Southern Norway, Personal Photograph)\nGlaciers gain mass through accumulation of snowfall and through re-freezing of meltwater but lose mass (termed ablation) through surface melt, basal melt, sublimation and iceberg calving (Figure 2, right). The accumulation of ice primarily occurs in the glacier’s accumulation zone and ice loss generally originates in the glacier’s ablation zone (Figure 2, left).\n(Figure2, accumulation/ablation zones, www.physicalgeography.net, image of calving glacier on right)\nFor an ideal glacier, ice flow through a cross-section must exactly balance the accumulation and ablation taking place (Benn and Evans, 1998, 142). The difference between the total gains and losses measured over a specified time refers to the mass balance. Mass balance is usually measured over the course of a year which computes the sum of all the annual accumulation and ablation (Benn and Evans, 1998, 75). The velocity at which a glacier moves whereby its Mass Balance is 0 represents the point at which its inputs (through accumulation) equals its outputs (through ablation) and is termed the Balance Velocity (Figure 3).\nFigure 3: (Left) Balance Velocities for Antarctica as illustrated in Bamber et al (2009). (Right) Actual velocities across the ice sheet as measured by Rignot and Thomas (2002).\nAs every individual basin is rarely in balance, the actual velocities of glaciers/ice streams across Antarctica shows that many glaciers have velocities in excess of their balance velocity and many are less than their balance velocities (Figure 3, right).\nThe question of balance velocities brings us to one of the most important points of this post. When a glacier is in balance or flowing at its balance velocity, net mass will remain balanced. However, when a glacier accelerates while near or at its balance velocity, the outputs resultantly increase but the inputs do not, thereby shifting the glacier regime to one of negative mass balance or net ice loss. This situation is particularly important because accelerated ice flow is the key method through which the Antarctic ice sheets incur a net ice loss. Accelerations such as these occur through two primary mechanisms. The first of which is caused by surface melt water reaching the glacial bed causing basal lubrication therefore reducing the frictional forces at the bed and thus increasing ice flow (Bell 2008).\nThe second mechanism refers to when the forces at the downstream terminus of a glacier or ice stream are disturbed or altered. This can occur through removing buttressing ice shelves or by shifting the glacier’s grounding line (point where glacier ice reaches floatation). The presence of an ice shelf provides a longitudinal compressive force which slows the flow of ice streams. If removal of this compressive force occurs, velocity of ice streams increase. This has been observed directly by Scambos et al (2004) and Rignot et al (2004) through both visual observations (Scambos) and radar interferometry (Rignot).\nIn terms of a grounding line retreat, an inland shift of the grounding line causes less backpressure through increased calving and basal melting. This process results in increased glacier velocities and subsequent inland thinning as more ice is being pulled from the accumulation zone (Bell 2008). In a warmer climate, one would expect that surface melting would increase, making the first mechanism more likely, however because of Antarctica’s climate and the omnipresence of ice shelves and calving glaciers there, the second mechanism actually dictates the ice losses from Antarctica. Evidence has already been presented which supports the theory that it is warm ocean water in West Antarctica which is in actuality enabling this second mechanism (Shepherd, Wingham and Rignot, 2002).\nWe should all now at least remotely understand that mass balance changes in Antarctica aren’t reliant on surface melting but rather depend on dynamic responses such as the 2nd mechanism.\nIntermediate rebuttal written by robert way\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nAdditional videos from the MOOC\nExpert interview with Eric Rignot\nExpert interview with Isabella Velicogna\nLast updated on 8 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Arctic icemelt is a natural cycle",
   "paragraph": "Human activity is driving retreat of Arctic sea ice\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThick arctic sea ice is undergoing a rapid retreat.\nClimate Myth...\nArctic icemelt is a natural cycle\n\"In 2007, the Northern Hemisphere reached a record low in ice coverage and the Northwest Passage was opened. At that point, we were told melting was occurring faster than expected. What you were not told was that the data that triggered this record is only available back to the late 1970s. We know the Northwest Passage had been open before.\" (Matt Rogers)\nArctic sea ice has aptly been termed a \"canary in the global warming coal mine,\" a sensitive indicator of climate change; because of its importance as a diagnostic of global warming, climate change skeptics struggle to explain the decline of Arctic sea ice as a natural phenomenon.\nSatellite measurements of Arctic sea ice extent reveal a rapid decline over the past 30 years, particularly at the end of each year's annual melt season. The downward trend and the increasing difference between seasons are in keeping with predictions of the effects of global warming. As the Arctic warms, the volume of ice in the region gradually declines, making it less likely ice will survive more than one year and thus exposing more open water at the end of each melt season.\n(from National Snow and Ice Data Center )\nAs an explanation for the decline of Arctic sea ice, skeptics hypothesize we're seeing the effects of natural cycles causing deep, decades-long swings in Arctic ice coverage and volume. Lending observational support for such cycles is much more difficult than relying on direct observations of ice extent with contemporary instruments. Still, thanks to ocean sediment cores and some other physical clues left by past climate regimes we have reasonable insight into past Arctic sea ice extent. Combining various information about past climate behavior, we can better understand why changes in ice coverage have occurred in past times, whether those natural variations are happening today, and how those changes compare to today's sea ice trend.\nWhile it's true that natural variations of the climate have caused significant changes in Arctic ice extent in the past, it's important to note that such changes are not airtight arguments against anthropogenic global warming causing today's loss of ice. After all, events of the past do not describe newly identified influences by human culture on today's climate. Indeed, comparisons between past and present Arctic climate reveal different reasons for yesterday's and today's Arctic sea ice changes and strongly suggest that today's changes are largely anthropogenic ( Overpeck et al ). Meanwhile, analysis of several hundred indicators of past Arctic sea ice extent tells us that recent losses appear to have no parallel in records going back many thousands of years ( Polyak et al ).\nThe past 200 years offers an example of how natural and anthropogenic influences on Arctic sea ice can be distinguished. The Arctic appears to have undergone an unusually cool period in the early 19th century, certainly natural, with recovery to more normal conditions extending into the 20th century leading to the warming we see today. Referring to the graph above, we can see that after the earlier cool period sea ice extent in the Arctic appears to have largely stabilized, later to begin a steady decline in chorus with other emerging observations of global warming such as increasing air and ocean temperatures. This decline in ice extent is happening even though the causes for natural recovery from the unusual cold of the 19th century are no longer in play, while research strongly suggests these recent reductions in Arctic sea ice are caused by a new, anthropogenic mechanism ( Johannessen et al ).\nAlthough natural factors have always influenced the state of Arctic sea ice, research strongly suggests that today's decline is driven by the novel influence of anthropogenic CO2 we've added to the atmosphere and thus is unique in Earth's history.\nABCDE\nLast updated on 23 June 2013 by doug_bostrom. View Archives"
  },
  {
   "title": "Arctic sea ice extent was lower in the past",
   "paragraph": "Arctic sea ice extent in the past\nLink to this page\nWhat the science says...\nWhile there have been times in the distant past when Arctic sea ice extent was lower than today's, the current sea ice extent is the lowest in the past several thousand years.\nClimate Myth...\nArctic sea ice extent was lower in the past\n\"...there [is] anecdotal and other evidence suggesting similar melts from 1938-43 and on other occasions.\" (John Christy via Jonathan Leake)\nIt's important to note that we expect the Arctic to have been cooling over the past ~6,000 years due to the Earth's orbital cycles. Thus if we look back far enough in the past, we can certainly find a period during which the Arctic was hotter and Arctic sea ice extent was lower. However, this actually contradicts the argument that the current sea ice decline could be natural, because that long-term orbital forcing has not reversed, and thus cannot account for the sudden and rapid Arctic warming and concurrent sea ice decline.\nKaufman et al. (2009) reconstructed past Arctic temperatures, and confirmed that the Arctic had been cooling for at least the past 2,000 years prior to the 20th Century, and found an Arctic temperature 'hockey stick' (Figure 1).\nFigure 1: Arctic temperature change reconstructed by Kaufmann et al. (2009) including data updated for corrigendum and including instrumental measurements for the Arctic region (60 to 90° N) from NASA.\nPerhaps the authoritative paper on Arctic sea ice extent over the past 1,450 years is Kinnard et al. (2011), which used a combination of Arctic ice core, tree ring, and lake sediment data to reconstruct past Arctic conditions. The results are shown in Figure 2.\nFigure 2: Arctic sea ice extent over the past 1,450 years reconstructed from proxy data by Kinnard et al., with a 40-year low pass filter applied. Note that the modern observational data in this figure extend through 2008, and thus it is a close approximation of current conditions, even though the extent is not as low as current annual data due to the 40-year smoothing.\nBased on the Kinnard results, Arctic sea ice extent is currently lower than at any time in the past 1,450 years.\nPolyak et al. (2010) looked at Arctic sea ice changes throughout geologic history and noted that the current rate of loss appears to be more rapid than natural variability can account for in the historical record.\n\"The current reduction in Arctic ice cover started in the late 19th century, consistent with the rapidly warming climate, and became very pronounced over the last three decades. This ice loss appears to be unmatched over at least the last few thousand years and unexplainable by any of the known natural variabilities.\"\nLast updated on 19 September 2013 by dana1981. View Archives"
  },
  {
   "title": "Arctic sea ice has recovered",
   "paragraph": "Has Arctic sea ice returned to normal?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThick arctic sea ice is in rapid retreat.\nClimate Myth...\nArctic sea ice has recovered\n\"Those who have been following NSIDC and JAXA sea ice plots have noted that this has been an extraordinary year so far, with Arctic sea ice hitting the “normal” line on some datasets. ...\nAs of today,JAXA shows that we have more ice than any time on this date for the past 8 years of Aqua satellite measurement for this AMSRE dataset.\" (Anthony Watts, 22 April 2010)\nDiscussions about the amount of sea ice in the Arctic often confuse two very different measures of how much ice there is. One measure is sea-ice extent which, as the name implies, is a measure of coverage of the ocean where ice covers 15% or more of the surface. It is a two-dimensional measurement; extent does not tell us how thick the ice is. The other measure of Arctic ice, using all three dimensions, is volume, the measure of how much ice there really is.\nSea-ice consists of first-year ice, which is thin, and older ice which has accumulated volume, called multi-year ice. Multi-year ice is very important because it makes up most of the volume of ice at the North Pole. Volume is also the important measure when it comes to climate change, because it is the volume of the ice – the sheer amount of the stuff – that science is concerned about, rather than how much of the sea is covered in a thin layer of ice*.\nOver time, sea ice reflects the fast-changing circumstances of weather. It is driven principally by changes in surface temperature, forming and melting according to the seasons, the winds, cloud cover and ocean currents. In 2010, for example, sea ice extent recovered dramatically in March, only to melt again by May.\nSea-ice is subject to powerful short-term effects so while we can't conclude anything about the health of the ice from just a few years' data, an obvious trend emerges over the space of a decade or more, showing a decrease of about 5% of average sea-ice cover per decade.\nSource: Rayner et.al, 2004, updated\nWhere has the thick ice gone?\nWhen we consider the multi-year ice and look at the various measurements of it, we see a steep decline in this thick ice. As you might imagine, thick ice takes a lot more heat to melt, so the fact that it is disappearing so fast is of great concern.\nSource: Polar Science Centre, University of Washington\nIt is clear from the various data sets, terrestrial and satellite, that both the sea ice extent and multi-year ice volume are reducing. Sea ice extent recovered slightly during the Arctic winters of 2008-09, but the full extent of annual ice reduction or gain is seen in September of each year, at the end of the Arctic summer. The volume of multi-year ice has not recovered at all, and is showing a steeply negative trend.\n* Footnote: Although a thin layer of ice doesn’t tell us much about the overall state of ice loss at the Arctic, it does tell us a great deal about Albedo, the property of ice to reflect heat back into space. When the sea ice diminishes, more heat passes into the oceans. That heat melts the thick ice and speeds up the melting of thinner sea ice, which in turns allows more heat to accumulate in the oceans. This is an example of a positive feedback.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 14 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "Arctic sea ice loss is matched by Antarctic sea ice gain",
   "paragraph": "How does Arctic sea ice loss compare to Antarctic sea ice gain?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nArctic sea ice loss is three times greater than Antarctic sea ice gain.\nClimate Myth...\nArctic sea ice loss is matched by Antarctic sea ice gain\nIn fact, the global sea-ice record shows virtually no change throughout the past 30 years, because the quite rapid loss of Arctic sea ice since the satellites were watching has been matched by a near-equally rapid gain of Antarctic sea ice. Indeed, when the summer extent of Arctic sea ice reached its lowest point in the 30-year record in mid-September 2007, just three weeks later the Antarctic sea extent reached a 30-year record high. The record low was widely reported; the corresponding record high was almost entirely unreported. (Chris Monckton)\nHave Arctic ice losses truly been balanced by Antarctic gains? The first point to clarify is that we are talking about floating sea ice, not to be confused with land ice. Land ice at both poles and in glaciers around the world is sliding into the ocean at an accelerating rate. This net loss of land ice is contributing to sea level rise.\nHowever, Monckton is clearly referring to sea ice. The rapid decline of Arctic sea ice has indeed coincided with an increase in Antarctic sea ice. But do these two opposite trends cancel out as Monckton suggests? In reality, the upward Antarctic trend is only slight compared to the plummeting Arctic trend. Tamino has crunched the numbers and found the Arctic trend is in fact more than three times faster than the Antarctic one. The net result is a statistically significant global decrease of more than a million km2 or a few percent – would you agree with Monckton that this is “virtually no change”?\nFigure 1: Global sea ice extent since 1979. (Image source: Tamino. Data is from US National Snow and Ice Data Center.)\nFigure 2: National Snow and Ice Data Center (NSIDC) Antarctic, Arctic, and global (sum of the two) sea ice extents with linear trends. The data is smoothed with a 12-month running average.\nSea ice area data shows the same thing as extent data.\nSummer and Winter, Apples and Oranges\nMonckton compares the Arctic summer to the Antarctic winter, not the most appropriate comparison. Sea ice grows and shrinks seasonally because polar latitudes have vastly more daylight hours in summer than in winter. When ice melts, it makes the surface less reflective and amplifies the warming (as is currently occurring in the Arctic), but this effect can only make a difference when the Sun is up. Thus the most important time of year for sea ice is its annual minimum which occurs at the end of the summer: September in the Arctic but February in the Antarctic.\nSo how do the two compare?\nFigure 3: Minimum sea ice extent since 1979 in the Arctic and Antarctic. (Image source: James Hansen. Data is from US National Snow and Ice Data Center.)\nWhile the summer Arctic has lost an extent of about 2.5 million km2 (equivalent to the area of Western Australia), the summer Antarctic growth is only 0.3 million km2 (about the size of Victoria). Even that slight upward trend is less than the year-to-year variability; although 2003 and 2008 tied for the highest February extent, 2006 was third lowest. Again, the real world contradicts Monckton’s assertion that changes in the Arctic are being balanced out by the Antarctic.\nThe Third Dimension\nFurthermore, Monckton fails to mention that Arctic sea ice is not only shrinking in extent but also has been thinning rapidly. Although its lowest extent was in 2007, its volume has continued declining since then, hitting another record low in 2010:\nFigure 4: Arctic sea ice volume since 1979. (Image source: Wikipedia. Based on data from University of Washington Polar Science Center.)\nThe volume data is supported by a sharp decline in thick multiyear ice (Figure 5).\nFigure 5: NSIDC Arctic sea ice age from 1983 through 2011 (Source)\nMeanwhile there has been a slight increase in Antarctic sea ice volume, but only by about 5,000 km3 (insufficient to offset the Arctic decline shown in Figure 4), and most of it in a few years at the start of the record.\nThe Polar Prognosis\nAs thinner and younger ice is easier to melt, the rapid Arctic melt is set to continue; ice-free summers are now probably inevitable. In contrast, the Antarctic increase is occurring despite the warming of the Southern Ocean and is expected to reverse as the warming continues. Antarctic sea ice is just a distraction from the accelerating losses from ice sheets and the looming specter of a sea-ice-free Arctic.\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Arctic Storm Caused the 2012 Record Sea Ice Minimum",
   "paragraph": "Arctic Storm Caused the 2012 Record Sea Ice Minimum\nLink to this page\nWhat the science says...\nSimilar summer Arctic storms to the one which occurred in 2012 had very little impact on the eventual annual sea ice minimum. The 2012 Arctic storm only had a significant impact on the annual minimum because of the long-term, primarily human-caused deterioration of Arctic sea ice.\nClimate Myth...\nArctic Storm Caused the 2012 Record Sea Ice Minimum\n\"...the massive Arctic storm that broke up huge amounts of sea ice [made] this new record low possible.\" (Anthony Watts)\nWith 2012 shattering the previous minimum Arctic sea ice extent record by more than three quarters of a million square kilometers, climate contrarians have predictably been scrambling to find an excuse why the Arctic death spiral is still nothing to worry about. The Arctic storm which first hit on 05 August 2012 has been used as a convenient scapegoat. This despite the fact that the evidence indicates that most of the long-term loss of Arctic sea ice is human-caused.\nCan this single storm really be responsible for breaking the previous Arctic sea ice record by such a large margin? The short answer is no.\nSea Ice Decline Prior to August\nSince the Arctic storm first hit on August 5th, we can first evaluate its impact by looking at the state of the sea ice extent leading up to that point. For example, the average July sea ice extent in 2012 was the second-lowest on record, just slightly behind 2011 (Figure 1).\nFigure 1: July average sea ice extent, data from NSIDC. July 2012 had a lower extent than 2007, and the second-lowest July average extent behind 2011.\nAccording to JAXA, as of August 4th, the 2012 Arctic sea ice extent was also the 2nd-lowest on record, behind only 2007. 2012 and 2007 were also essentially tied for the largest sea ice decline between early June and early August (2011 having less June-August ice loss because its extent in June was already low) (Figure 2).\nFigure 2: Arctic sea ice extent from 01 January to 04 August, for 2003 through 2012, data from JAXA.\nThus we see that Arctic sea ice in 2012 was experiencing substantial decline even before the Arctic storm hit.\nComments from Arctic Experts\nSeveral Arctic sea ice experts have weighed in on the record-breaking minimum in 2012 and the effects of the summer storm on this record. According to NSIDC Director Mark Serreze,\n\"The previous record, set in 2007, occurred because of near perfect summer weather for melting ice. Apart from one big storm in early August, weather patterns this year were unremarkable. The ice is so thin and weak now, it doesn't matter how the winds blow.\"\nNSIDC scientist Walt Meier said,\n\"...in the context of what's happened in the last several years and throughout the satellite record, it's an indication that the Arctic sea ice cover is fundamentally changing....The Arctic used to be dominated by multiyear ice, or ice that stayed around for several years. Now it's becoming more of a seasonal ice cover and large areas are now prone to melting out in summer.\"\nDr. Julienne Stroeve (another NSIDC scientist) said,\n\"The acceleration of the loss of the extent of the ice is mostly because the ice has been so thin. This would explain why it has melted so much this year. By June the ice edge had pulled back to where it normally is in September,\"\n\"The 2007 record was set when you had weather conditions which were perfect for melting. This year we didn't have those. It was mixed. So this suggests the ice has got to a point where it's so thin it doesn't matter what the weather is, it's going to melt in the summer. This could become the new normal\"\nNSIDC Arctic sea ice news and analysis:\n\"Other than the August storm, the pressure pattern in 2012 does not appear to have been as favorable in promoting ice loss as was the case in 2007, and yet a new record low occurred.\"\nClaire Parkinson, a climate scientist at NASA Goddard Space Flight Center said,\n\"The storm definitely seems to have played a role in this year's unusually large retreat of the ice. But that exact same storm, had it occurred decades ago when the ice was thicker and more extensive, likely wouldn't have had as prominent an impact, because the ice wasn't as vulnerable then as it is now.\"\nPaul A. Newman, chief scientist for Atmospheric Sciences at NASA's Goddard Space Flight Center estimated that there have been about eight storms of similar strength during the month of August in the last 34 years of satellite records - one every 4 to 5 years.\nHow Strong was the 2012 Storm?\nWhile the 2012 Arctic storm was a strong one, as Newman noted, it was not unprecedented. To confirm Newman's claim, we examined daily surface pressure maps from NCEP-NCAR and NCEP-II DOE reanalysis data. Low pressure systems are typically associated with a tight pressure gradient that results in strong surface winds.\nIn the 34 years between 1979 and 2012, we identified 19 deep low pressure systems (defined here as having a central pressure less than 980 hectopascals [hPa]) in the Arctic between 15 July and 15 September. Most of these deep lows occurred over the Barents, Laptev, and Kara seas, so they are not suitable analogs for the 2012 storm.\nHowever, there were five previous storms during this timeframe similar to the 2012 event, occurring near the center of the sea ice pack. These occurred in 1980, 1990, 1991, 1994, and 1997 (Figure 3 - click for a larger version).\nFigure 3: NCEP/NCAR sea level pressure maps in (from right to left and top to bottom) on 07 August 2012, 28 August 1980, 12 September 1990, 06 August 1991, 17 August 1994, and 03 September 1997.\nWhat Effect Did Similar Previous Storms Have on Sea Ice Extent?\nIf this type of deep Arctic storm has a major influence on annual Arctic sea ice minimum as claimed by contrarians, then we would expect to see similar declines in 1980, 1990, 1991, 1994, and 1997. Let's see what the data shows (Figure 4).\nFigure 4: September Arctic sea ice extent data from NSIDC (blue), years with Arctic summer storms similar to the 2012 storm noted in red.\nThere was a decrease in September Arctic sea ice extent from 1989 to 1990 and 1996 to 1997 (and of course from 2011 to 2012), but an increase from 1979 to 1980, from 1990 to 1991, and from 1993 to 1994. Half increases and half decreases in September Arctic sea ice extent during one of these summer Arctic storm years compared to the prior year - thus no clear sign that these storms had a big impact on sea ice extent.\nWe should also note that the Arctic storm in 1990 occurred on 12 September, just days before the annual minimum, and there was very little sea ice decline between 12 September and the minumum that year. Thus the 1990 storm cannot be responsible for most of the sea ice decline between 1989 and 1990. In fact, there wasn't a particularly large decline between the date of any of the Arctic storms and the annual minimum, with the exception of 2012.\nFigure 5 shows the change in sea ice extent at the time of the summer storm (left frame) vs. the annual minimum (right frame) for each of these years (note that the year in the date on the right frame in Figure 5 is cut off - the year is the same as in the left frame).\nFigure 5: Arctic sea ice extent maps on the date of each summer Arctic storm in 1980, 1990, 1991, 1994, 1997, and 2012 and at the annual minimum each of these years.\nThus in the five previous examples of summer Arctic storms similar to that in August 2012, there was no clear impact on the final Arctic sea ice minimum.\nAdditionally, Screen et al. (2011) found that it was the frequency of cyclones between May and July, and not those in August or September, that are responsible for conditioning the sea ice in September for either a significant loss or gain over the previous year.\nSo What Factors Contributed to the 2012 Record?\nAs the ice experts mentioned above, the 2012 summer storm very likely made an already bad situation worse. The reason is that because of the long-term decline and thinning of the Arctic sea ice in response to the dramatic warming, the ice pack is now very vulnerable to events that would have historically had little or no impact.\nNot only does the earlier retreat of the sea ice result in a positive feedback, it also means that when there are storms such as in 2012, the winds are capable of generating large swells that can penetrate hundreds of kilometers into the pack and break up the sea ice, thereby hastening the decline. Figure 6 shows the long fetch (the distance along which the winds are able to interact with the ocean surface and produce wind waves and swells) available to winds during the 2012 storm. These conditions were ideal for generating large swells.\nFigure 6: Wind field (coloured arrows, with warm colours representing strong winds) and sea ice (grey areas) on 9 August 2012. Credit: NASA/Goddard Science Visualization Studio.\nA similar event was witnessed by Canadian scientists aboard an ice breaker in the Beaufort Sea in September 2009 (Asplin et al. 2012). They observed swells over 250 kilometers (km) from the ice edge following the transit of two lows across the Arctic Basin in short succession. The large swells caused the breakup of large (>1 km) multi-year ice flows up to 5 meters thick, into much smaller flows (100-150 meters). On that occasion the long stretch of open water was also key in permitting the storms to generate a large swell.\nPeer-Reviewed Literature on Arctic Storms\nWe might also ask whether these summar Arctic storms have become stronger and/or more frequent, and if so, why?\nHakkinen et al. (2008) analyzed the wind stress data from the NCEP/NCAR reanalysis to test for a change in Arctic storm intensity/frequency and found that summer storms are becoming more intense (Figure 7).\nFigure 7: Trends of the NCAR/NCEP Reanalysis wind stresses for 1948–2006 for (a) annual, (b) winter, and (c) summer values. Units are 10-4 N/m2 per year. Figure 3 from Hakkinen et al. (2008).\nThe authors conclude that the increasing Arctic wind stress trend in the summer has higher significance than the winter trend due to enhanced storminess.\n\"Our results show a gradual acceleration in sea ice drift over the 50 years analyzed. The accelerating trend in the central Arctic is present both in winter and summer observations, with nearly equal statistical significance. Since the atmosphere is responsible for the main forcing of the sea ice, the obvious conclusion is that the increased sea ice speeds relate to increased storm frequency and/or intensity.\"\nLong and Perrie (2012) investigated the impacts of increased open water in the Beaufort Sea for a summer Arctic storm in 2008 using a coupled atmosphere-ice-ocean model. They found that a reduction in ice cover and more open water results in stronger Arctic storms.\n\"The model simulations suggest that the lack of ice cover in the Beaufort Sea during the 2008 storm results in increased local surface wind and surface air temperature, compared to enhanced ice cover extents.... These changes result in enhanced surface winds, by as much as ∼4 m/s during the 2008 storm, compared to higher ice concentration conditions (typical of past decades).\"\nScreen et al. (2011) found no significant trends in late spring or summer Arctic cyclone frequency over the period 1979–2009. So it appears that the intensity of summer Arctic storms may be increasing due in part to the reduction in sea ice cover, which is primarily human-caused, but there is no clear indication of an increase in Arctic storm frequency.\nPeer-Reviewed Literature on the Long-Term Sea Ice Decline\nAs noted above, the long-term thinning and decline of the Arctic sea ice allowed the 2012 summer storm to impact this year's record minimum. Thus it's also important to note that there is very strong evidence in the peer-reviewed literature that the long-term sea ice decline is primarily human-caused. For example, Day et al. (2012) concluded,\n\"despite increased observational uncertainty in the pre-satellite era, the trend in [Arctic sea ice extent] over this longer period [1953–2010] is more likely to be representative of the anthropogenically forced component.\"\nStroeve et al. (2012) concluded,\n\"Based on the CMIP5 multi-model ensemble mean, approximately 60% of the observed rate of decline from 1979–2011 is externally forced\"\nWhile Notz and Marotzke (2012) found very poor correlation between oceanic cycles and sea ice extent, but very good correlation between CO2 and ice extent (Figure 8).\nFigure 8: Correlation between September sea ice extent and CO2 forcing (red), solar forcing (blue), PDO index (green), and AO index (yellow). Figure 4 from Notz and Marotzke (2012).\nAnd based on the results of Vinnikov et al. (1999), there is less than 0.1% probability that the long-term decline in Arctic sea ice extent is due solely to natural variability.\nSummary\nThe 2012 summer Arctic storm simply cannot be blamed for the record-shattering Arctic sea ice extent minimum.\nArctic sea ice extent was already declining rapidly prior to the storm formation, to the second-lowest level on record.\nThe scientific literature shows that the Arctic is undergoing a fundamental change, with the remaining ice continually becoming thinner, weaker, and more vulnerable, and that this decline is primarily human-caused.\nSea ice experts agree that unlike in 2007, the 2012 Arctic weather conditions were not ideal for melting ice, and while the the summer storm played a part in the record minimum, it was not the main cause.\nIn the past, Arctic summer storms similar to the 2012 event did not have a major impact on sea ice extent or September sea ice minimum.\nThe 2012 record-breaking minimum can be attributed to a number of factors. The summer storm likely played a role, but primarily because the ice was thinner, weaker, and less extensive to begin with than in prior years due to its long-term human-caused decline.\nThe scientific literature indicates that Arctic storms may be becoming more intense, in part due to increased open water as the sea ice continues its long-term human-caused decline.\nLast updated on 19 January 2019 by dana1981. View Archives"
  },
  {
   "title": "Arctic was warmer in 1940",
   "paragraph": "Comparing 1940s Arctic to today\nLink to this page\nWhat the science says...\nMonckton appears to have cherry-picked temperature data from a few stations. The full data for latitudes 64-90°N reveal the Arctic is warmer today than in 1940.\nClimate Myth...\nArctic was warmer in 1940\n\"Temperatures in the Arctic and in Greenland were warmer by up to 3 Fahrenheit degrees in the late 1930s and early 1940s than they are at present\" (Christopher Monckton)\nA common misconception that is found to exist within the blogosphere is that the Arctic (hereby defined at 64 to 90 °N) was warmer than present during the early-to-mid 20th century. In particular this claim has been supported by anecdotal evidence of reduced sea ice cover and pronounced warming at *some* Arctic stations at that time. Monckton provides no evidence that he conducted an Arctic-wide analysis of air temperatures but rather seems to suggest that he selected a few stations which supported his narrative rather than examining all of the evidence. As real scientists tend to be very skeptical beasts, it is important that we assess his claim for quality to ensure that it is not based solely on a cherry-pick.\nIn order to do so I ran clear climate code’s Gistemp reimplementation using Python and also their scripts for adding in more data from Environment Canada that was otherwise not included in the Gistemp analysis. This implementation is likely the best representation of the trends in the Arctic as it assumes that stations in the high Arctic follow similar trends to stations in the low Arctic whereas other methods (HadCrut) exclude these regions thereby assuming they have similar trends to the global trend. The accuracy of the reconstruction can be validated through comparison with the various reanalysis datasets which are reassessments of temperature changes combining meteorological stations, satellite measurements, weather balloon data and meteorological models. I have included the National Centers for Environmental Prediction (NCEP) reanalysis dataset for comparison.\nSo what do the results show?\nFigure 1: Temperature Anomalies (1951-1981 Baseline) for the Arctic region (64-90°N) over the past 130 years according to ccc-gistemp analysis and NCEP reanalysis.\nIt is evident based upon Figure 1 that the late 20th to early 21st century warming in the Arctic far exceeds the warming experienced during the 1930s and 1940s. This is supported by calculating a 10-year running means which shows that the 10-year period from 2001 to 2010 was 0.79°C warmer than the warmest 10-year period during the early-to-mid 20th century. Monckton's claim of that period being 1°C greater than the current warming is therefore proven to be incorrect. It should also be noted that the warming over the last 30 years (1981-2010) shows an extraordinarily fast rate of warming (6.3°C/century) using ccc-gistemp.\nAnother way of looking at the data\nWhat about when we order the anomalies by year?\nTable 1: Top 10 Warmest Years in the Arctic according to ccc-gistemp analysis.\nAs is readily apparent, the Arctic was NOT warmer than it is currently and in fact 2010 was the warmest year the Arctic has experienced since instrumental records began. Out of the top 10 warmest years only 2 years predate the 2000s. This post should provide a definitive answer as to whether the warming of the early century was greater in the Arctic than currently. The answer is a resounding no.\nAs a side note, temperature reconstructions for the Arctic are now for the most part widely accessible and I was able to run this one in 20 minutes. There is no practical reason why Monckton selects but a few stations for his analysis and ignores the overwhelming majority of evidence presented here and elsewhere. There are just no excuses.\nLast updated on 11 April 2011 by robert way."
  },
  {
   "title": "Ben Santer rewrote the 1995 IPCC report",
   "paragraph": "The consensus-building process of the IPCC\nLink to this page\nWhat the science says...\nThe IPCC operates by consensus. Ben Santer could not have and did not single-handedly alter the 1995 IPCC report. Accusations to the contrary are simply an attempt to re-write history.\nClimate Myth...\nBen Santer rewrote the 1995 IPCC report\n\"However, a single scientist, Dr. Ben Santer of Lawrence Livermore National Laboratory, rewrote the draft at the IPCC’s request, deleting all five statements, replacing them with a single statement to the effect that a human influence on global climate was now discernible, and making some 200 consequential amendments. These changes were considered by a political contact group, but they were not referred back to the vast majority of the authors whose texts Dr. Santer had tampered with, and whose five-times-stated principal conclusion he had single-handedly and unjustifiably negated.\" (Christopher Monckton)\nCitizen's Challenge has documented the actual events involving the statement in the 1995 IPCC report attributing global warming to human effects, as did the late Stephen Schneider in his excellent book Science as a Contact Sport.\nWhat actually happened is that the scientific literature at the time clearly showed a number of 'fingerprints' of human-caused global warming, as Dr. Ben Santer showed. The Saudi Arabian and Kuwaiti delegations - for obvious reasons - claimed this was 'bad science', and were joined by a few delegates from other small countries like Kenya. As a result of the disagreement, a Contact Group was held to negotiate the language that would eventually go into the report.\nThe Saudis and Kuwaitis did not even send representatives to the Contact Group - they were uninterested in discussing the science. A Kenyan scientist joined the group, which discussed the scientific evidence, and agreed that a clear human signal could be found in the observational data. When the Kenyan joined the group calling for this language to be included in the report, the Saudis and Kuwaitis finally dropped their opposition, and the language attributing global warming to human effects was added into the report (by consensus).\nSanter was subsequently slandered by Frederick Seitz in an opinion-editorial published in the Wall Street Journal (WSJ). Seitz did not participate in the IPCC process in question, and yet originated this myth by accusing Santer of single-handedly re-writing the report. The IPCC chairman and co-chairmen subsequently sent a letter to the WSJ noting that Seitz's accusations were \"completely without foundation.\"\nIt was not a matter of one scientist re-writing the IPCC report. That's not how the organization functions; it's a consensus document. As the first link above discusses, there are now many clear fingerprints of global warming, so why this argument 16 years ago is relevant to the science today is a mystery.\nLast updated on 23 February 2012 by dana1981. View Archives"
  },
  {
   "title": "BEST hides the decline in global temperature",
   "paragraph": "Berkeley Earth temperature record confirms other data-sets\nLink to this page\nWhat the science says...\nThe BEST results confirm that the long-term global warming trend over land is approximately 0.3°C per decade, consistent with the other surface temperature datasets. Those who claim that BEST shows global warming has stopped are relying on a cherrypicking of short-term data, confusing noise with signal, and lying with statistics.\nClimate Myth...\nBEST hides the decline in global temperature\n\"There is no scientific basis for saying that warming hasn’t stopped...This is “hide the decline” stuff. Our data show the pause, just as the other sets of data do. Muller is hiding the decline\" (Judith Curry)\nThe Berkeley Earth Surface Temperature (BEST) results were entirely expected, consistent with the existing body of peer-reviewed science. We expected that the BEST results would be a minor footnote in climate science history - one more study affirming the accuracy of the surface temperature record, and that the \"skeptics\" would finally stop denying its accuracy, as they promised they would. After all, the study was privately funded, including by the anti-climate science Koch Brothers, and involved Richard Muller and Judith Curry, two scientists quite skeptical of the global warming theory.\nUnfortunately, so many \"skeptics\" have devoted so much time and effort into disputing the accuracy of the surface temperature record that the BEST results seem to have short-circuited their collective brains. The schizophrenic \"skeptic\" reaction has been quite the spectacle to behold. First they attacked the BEST team for purely superficial reasons, including some rather appalling insults. But then they seemed to briefly accept the results, attempting to diminish their impact by claiming they had expected BEST to confirm global warming all along, and then doubling-down on other climate myths. Most recently, these same \"skeptics\" have attempted to argue that the BEST results show that global warming has stopped, and that their results are still biased high due to the urban heat island (UHI) effect.\nSince the so-called \"skeptics\" decided to toss true skepticism aside and instead throw as many climate myths at the BEST results as they could think of to see what would stick, we had to devote many posts to debunking this myth mish mash. Below we summarize what we have learned from the BEST results.\nGlobal Warming Continues\nThe main (and entirely expected) conclusion we can draw from the BEST results is that although certain parties tried to hide the incline in global temperatures, global warming continues. Some \"skeptics\" have cherrypicked small portions of the BEST data in an attempt to argue that global warming has stopped. Dr. Richard Muller described these efforts quite accurately:\n\"what they have done is an old trick. It’s how to lie with statistics\"\nThis statistical lie is illustrated in Figures 1 and 2.\nFigure 1: BEST land-only surface temperature data (green) with linear trends applied to the timeframes 1973 to 1980, 1980 to 1988, 1988 to 1995, 1995 to 2001, 1998 to 2005, 2002 to 2010 (blue), and 1973 to 2010 (red). Hat-tip to Skeptical Science contributor Sphaerica for identifying all of these \"cooling trends.\"\nFigure 2: Entire BEST record vs. the recent cherrypicked portion used by \"skeptics\" to wrongly claim that global warming has stopped\nThe Rapid Rate of Global Warming is as Expected\nThe BEST results also confirmed the accuracy of the surface temperature record, with a warming rate (approximately 0.3°C per decade over recent decades) consistent with that estimated by NOAA and NASA GISS (Figure 3). The BEST results also confirmed that HadCRUT is biased low.\nFigure 3: Comparison of land-only surface station and satellite temperature measurements (1981-2010 baseline). Annual data is plotted for BEST, NOAA, and HadCRU, while a 12-month running average is plotted for the GISS, UAH, and RSS.\nThe BEST results are also approximately consistent with the amount of land-only surface warming estimated by satellite data as analyzed by Fu et al., Vinnikov & Grody (V&G), and Zou et al. The UAH and RSS surface warming estimates are lower than these other groups, which suggests they may be biased low, and reveals that the satellite datasets are not the 'gold standard' the \"skeptics\" would have us believe they are. Figure 4 compares the various datasets, with the satellites 'upscaled' to reflect the fact that models project the lower troposphere warming rate above land should be approximately 5% lower than the surface land warming rate. The Fu and Zou trends are SkS estimates.\nFigure 4: Comparison of various measurements of the land-only global surface temperature. The Fu, V&G, and Zou values are estimated by SkS, and the 0.95 amplification \"upscaling\" factor has been incorporated into the satellite trends to estimate the surface trend.\nAnd much to the dismay of the \"skeptics,\" BEST has confirmed once again that the UHI effect is not biasing the surface temperature measurements (Wickham et al. 2011):\n\"urban warming does not unduly bias estimates of recent global temperature change.\"\nExpectedness\nWhile the BEST results were entirely expected, we probably should not have been surprised by the inability of those who claim to be \"skeptics,\" but who clearly are not, to accept their findings. Reactions to the BEST results (including logical fallacies, contradictions, cherrypicks, ad hominem attacks, and outright distortions) have certainly given us an insight into who the true skeptics are.\nLast updated on 11 November 2011 by dana1981."
  },
  {
   "title": "CERN CLOUD experiment proved cosmic rays are causing global warming",
   "paragraph": "What do the CERN experiments tell us about global warming?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nEven the CERN scientist who ran the experiment admits that it \"says nothing about a possible cosmic-ray effect on clouds and climate.\"\nClimate Myth...\nCERN CLOUD experiment proved cosmic rays are causing global warming\n\"The new [CERN] findings point to cosmic rays and the sun — not human activities — as the dominant controller of climate on Earth...CERN, and the Danes, have in all likelihood found the path to the Holy Grail of climate science\" [Lawrence Solomon]\nCERN scientist Jasper Kirkby, about his recent cosmic ray experiment:\n\"At the moment, it actually says nothing about a possible cosmic-ray effect on clouds and climate, but it's a very important first step\"\nAt CERN, Europe's high-energy physics laboratory near Geneva, Switzerland, scientists created an experiment to test how clouds are formed. The experiment ties in with a climate \"skeptic\" hypothesis that cosmic rays (charged particles from space) are causing global warming. As the hypothesis goes:\nSolar magnetic field gets stronger => More cosmic rays are blocked from reaching Earth => Clouds, which are hypothetically seeded by cosmic rays, are less likely to form => Fewer clouds means more sunlight reaches Earth's surface => More sunlight means warmer temperatures => global warming!\nMany climate \"skeptic\" bloggers and commenters have claimed that the CERN experiment has proven that cosmic rays are causing global warming, and that the experiment is \"the final nail in the man-made global warming coffin\" (i.e. here and here and here and here).\nIn reality, the CERN experiment only tests the bolded step in this list of requirements for cosmic rays to be causing global warming:\nSolar magnetic field must be getting stronger\nThe number of cosmic rays reaching Earth must be dropping\nCosmic rays must successfully seed clouds, which requires:\nCosmic rays must trigger aerosol (liquid droplet) formation\nThese newly-formed aerosols must grow sufficiently through condensation to form cloud-condensation nuclei (CCN)\nThe CCN must lead to increased cloud formation\nCloud cover on Earth must be declining\nIn short, the CERN experiment only tested one-third of one out of four requirements to blame global warming on cosmic rays. Additionally scientists have measured solar activity and the number of cosmic rays reaching Earth, and neither meets the first two requirements listed above. Both solar magentic field strength and the number of cosmic rays reaching Earth have been flat over the past 50+ years (Figure 1).\nFigure 1: Solar Magnetic Field Strength from 1967 to 2009 (Vieira and Solanki 2010)\nA number of other recent studies have also found that cosmic rays have minimal influence on cloud formation, and thus minimal influence on global warming.\nAs Dr. Kirby said in the quote above, it is an important first step, just like buying eggs is an important first step in baking a soufflé. But just having some eggs doesn't mean I can bake a successful soufflé. There are a whole lot of other requirements necessary for me to bake a soufflé, and believe me, I don't meet them!\nLast updated on 2 September 2011 by dana1981."
  },
  {
   "title": "Climate 'Skeptics' are like Galileo",
   "paragraph": "Contrasting climate denial with evidence-based Galileo\nLink to this page\nWhat the science says...\nThe comparison is exactly backwards. Modern scientists follow the evidence-based scientific method that Galileo pioneered. Skeptics who oppose scientific findings that threaten their world view are far closer to Galileo's belief-based critics in the Catholic Church.\nClimate Myth...\nClimate 'Skeptics' are like Galileo\n\"I mean, it - I mean - and I tell somebody, I said, just because you have a group of scientists that have stood up and said here is the fact, Galileo got outvoted for a spell\" (Texas Governor Rick Perry)\nSome climate change skeptics compare themselves to Galileo, who in the early 17th century challenged the Church’s view that the sun revolves around the earth and was later vindicated.\nThe comparison to Galileo is not only flawed; the very opposite is true.\n1. Galileo was suppressed by religious/political authority, not scientists. Galileo was not suppressed or “outvoted” by other early scientists. Many scientific contemporaries agreed with his observations[2], and were appalled by his trial.[3] Galileo was persecuted by the religious-political establishment – the Catholic Church, which in 1616 ordered him to stop defending his view of the solar system, which contradicted church dogma. After Galileo published his famous Dialogue, the Roman Inquisition tried him in 1633 for defying Church authority, and found him guilty of suspected religious heresy, forced him to recant, banned his books and sentenced him to house arrest for life.[4] Galileo died eight years later.[5]\n2. Science is evidence-based; the most vocal skeptics are belief-based. The key difference between Galileo and the Church concerned Galileo’s “way of knowing,” or epistemology. How is knowledge attained?\nMedieval scholarship and Catholic Church dogma relied on the authority of Aristotle and a literal interpretation of the Bible to place earth at the center of the universe.\nIn contrast, Galileo’s views were not based on an infallible authority. His conclusions flowed from observations and logic. Galileo’s evidence- and logic-based method of inquiry later became known as the scientific method.\nThe vast majority of vocal skeptics are not engaged in climate research. The common bond uniting them, observers note, is an ideological belief system: Government regulation is bad, so problems that may require regulation must be resisted.[6] From there, they search for ways to cast doubt on the science.[7] Unlike Galileo and modern scientists, they do not change their view when presented with new evidence, because their position derives not from open-ended scientific inquiry, but from strongly-held ideological convictions.\nIn contrast, climate science applies the scientific method pioneered by Galileo. Scientists make observations, form logical hypotheses, then test their hypotheses through experiments and further observations. They follow the evidence wherever it leads.\nThe Church’s attack on Galileo and the skeptical assault on climate science are far from unique. History is full of examples where new scientific findings threatened powerful vested interests – whether religious, financial or ideological — and provoked a furious backlash.\n3. The discovery of global warming overturned an age-old belief; the skeptics seek to restore it. In arguing that the planets revolve around the sun, Galileo was challenging an idea that had dominated Western thought for over 1400 years. Ever since Ptolemy (90-168 AD) codified Aristotle’s “geocentrism,” most philosopher/scientists had accepted the common sense view that the earth is the center of the universe, with the sun and planets revolving around us.\nSimilarly, the prevailing view throughout history was that people, through our own actions, could not possibly alter earth’s climate on a global scale. Even into the 20th century, the overwhelming majority of scientists maintained, in science historian Spencer Weart’s words,\nthe widespread conviction that the atmosphere was a stable, automatically self-regulated system. The notion that humanity could permanently change global climate was implausible on the face of it, hardly worth a scientist's attention.[8]\nSome say climate science’s first “Galileo moment” came in 1896, when Swedish scientists Svante Arrhenius, after years of laborious hand calculations, predicted eventual global warming due to CO2 emissions.[9] Others point to 1938, when a British steam engineer named Guy Stewart Callendar, after poring over old CO2 and temperature records, stood alone before the Royal Meteorological Society to argue that global warming was already happening.[10]\nArrhenius and Callendar were ahead of their time, and failed to persuade others. In both cases, the scientific establishment found their calculations oversimplified and their evidence incomplete, certainly not convincing enough to overturn the ancient view that global climate was impervious to human acts.\nMainstream scientific opinion was slow to change. During the post-war science boom in the 1950’s, early computers and advanced methods allowed scientists to directly investigate objections to Arrhenius’ and Callendar’s view.[11] Using the new digital computers, Gilbert Plass found that more CO2 could indeed block more heat.[12] Hans Suess analyzed radioactive isotopes to detect ancient carbon in the air, presumably from fossil fuels.[13] Roger Revelle and Suess discovered that the oceans could not quickly take up additional CO2. David Keeling built the first sensor capable of accurately measuring atmospheric CO2 – just as Galileo had invented a more advanced telescope – and found that the CO2 level was indeed rising.\nFrom 1960 to 1990, the evidence kept accumulating, from areas of study as far afield as geology, astronomy and biology. As the gaps in knowledge were filled, one-by-one, most scientists changed their views and gradually formed a new consensus: significant anthropogenic (human caused) global warming was likely.[14]\nBy 2000, the evidence was overwhelming.\nThe hypothesis proposed by Arrhenius in 1896—denied by almost every expert through the first half of the twentieth century and steadily advancing through the second half—was now as well accepted as any scientific proposal of its nature could ever be.[15]\nThe climate pioneers were vindicated.\nCritics of climate science, backed by the alarmed fossil fuel industry,[16] sprang into action in the late 1980s, when the mounting evidence led to calls for international action to limit CO2 emissions. They did not argue, like Galileo, for a revolutionary hypothesis based on new evidence, because they could not agree on one among themselves.[17] They produced little new evidence. Instead, they searched for flaws in others’ research, and launched a public relations campaign to sow public doubt.\nUnlike Galileo, climate skeptics were not trying to overturn an ancient view. Their goal was the opposite: to restore the age-old conventional wisdom, that, by itself, “human activity was too feeble to sway natural systems”[18]. In clinging to this old view, the skeptics' stance more closely resembles that of the Catholic Church, which fought Galileo’s views for another 100 years after the scientific establishment had embraced him.\nImage created by Jarren Nylund\n4. Climate scientists, not skeptics, are being dragged into court Armed with ideological certainty, backed by powerful financial and political interests, skeptics have sought to not only discredit the science but impugn the researchers’ honesty. Unfounded accusations of deception and conspiracy fly freely,[19] and some climate scientists even receive death threats.[20] These attacks, according Dr. Naomi Oreskes, “have had a chilling effect... Intimidation works.”[21]\nIn April 2011, personal attacks on scientists took a more ominous turn, when Virginia’s Attorney General Ken Cuccinelli, a fierce climate skeptic, launched a criminal fraud investigation of a prominent climate scientist, Dr. Michael Mann.[22] Multiple investigations by independent scientific bodies have found no trace of wrongdoing in Mann’s work, and a Virginia judge dismissed Attorney General’s subpoena request for lack of evidence. Yet, as of September 2011, Cuccinellis’ crusade continues.[23]\nIf Galileo were alive today, watching climate scientists being dragged into court on baseless charges, is there any doubt whose side he would take?\n[1] On Sept 7, 2011, at the Republican presidential debate in Simi Valley, Texas Gov.. Rick Perry, became the highest level politician to invoke the Galileo comparison.\nWell, I do agree that there is — the science is — is not settled on this. The idea that we would put Americans' economy at — at — at jeopardy based on scientific theory that's not settled yet, to me, is just — is nonsense. I mean, it — I mean — and I tell somebody, I said, just because you have a group of scientists that have stood up and said here is the fact, Galileo got outvoted for a spell.http://www.nytimes.com/2011/09/08/us/politics/08republican-debate-text.html?pagewanted=all\nThe founders of Australia’s “Galileo Movement” claim that global warming is a “fabrication,” and\ncite as inspiration Galileo Galilei, the 17th century astronomer and father of modern science, who challenged the dogma of the Roman Catholic Church to report the Earth orbited around the sun. http://www.scientificamerican.com/article.cfm?id=galileo-movement-fuels-australia-climate-change-divide\n[2] http://www.nytimes.com/2011/09/09/science/earth/09galileo.html?_r=1&scp=3&sq=galileo&st=cse\n[3] personal communication, Spencer Weart, 9-17-2011.\n[4] Wooton, David. Galileo: Watcher of the Skies, Yale University Press, New Haven (2010), p. 224-5.\n[5] Galileo died on January 8, 1642 at age 77.\n[6] http://dotearth.blogs.nytimes.com/2008/03/04/the-never-ending-story/?hp\n[7] See Oreskes, Naomi and Erik M. Conway. Merchants of Doubt, Bloomsbury Press, New York (2010)\n[8] http://www.aip.org/history/climate/co2.htm\n[9] http://www.aip.org/history/climate/co2.htm\n[10] http://www.aip.org/history/climate/co2.htm\n[12] Dr. Spencer Weart’s excellent history of this period can be found in overview at http://www.aip.org/history/climate/summary.htm, with more details at http://www.aip.org/history/climate/co2.htm, the linked timeline and other articles.\n[13] Weart, Spencer. The Discovery of Global Warming, Harvard University Press, New York (2004), p. 26\n[14] Weart, p. 164.\n[15] Weart, p. 191.\n[16] http://www.nytimes.com/2009/04/24/science/earth/24deny.html?pagewanted=all\n[17] http://www.nytimes.com/2008/03/04/science/earth/04climate.html\n[18] http://www.aip.org/history/climate/summary.htm\n[19] Oreskes and Conway, page 4, 198-213. 264.\n[20] http://www.theaustralian.com.au/news/nation/climate-scientists-angered-by-deniers-death-threat-campaign/story-e6frg6nf-1226079058193\n[21] Oreskes and Conway, p. 264-5.\n[22] http://www.washingtonpost.com/wp-dyn/content/article/2010/08/30/AR2010083005004.html?sid=ST2010050303477\n[23] http://voices.washingtonpost.com/virginiapolitics/2010/07/the_university_of_virginia_hol.html Also see the Climate Science Legal Defense Fund: http://profmandia.wordpress.com/2011/09/09/donation/\nLast updated on 3 February 2014 by dana1981. View Archives"
  },
  {
   "title": "Climate change isn't increasing extreme weather damage costs",
   "paragraph": "Is climate change increasing extreme weather damage costs?\nLink to this page\nWhat the science says...\nThe data and research aren't conclusive as to whether climate change is increasing extreme weather damage costs. However, many types of extreme weather are becoming more intense and/or frequent, and disaster costs from extreme weather events are rising.\nClimate Myth...\nClimate change isn't increasing extreme weather damage costs\n\"Disasters Cost More Than Ever — But Not Because of Climate Change\" (Roger Pielke Jr.)\nReinsurance company Munich Re provides data about the number of annual disasters, and the frequency of these events is indeed rising.\nMunich Re has also concluded following a study of \"Severe Weather in North America\" that (emphasis added),\n\"Among many other risk insights the study now provides new evidence for the emerging impact of climate change. For thunderstorm-related losses the analysis reveals increasing volatility and a significant long-term upward trend in the normalized figures over the last 40 years. These figures have been adjusted to account for factors such as increasing values, population growth and inflation ... In all likelihood, we have to regard this finding as an initial climate-change footprint in our US loss data from the last four decades.\"\nMost arguments against a climate influence on extreme weather damages come from research by Roger Pielke Jr. His main argument is that disaster costs are only rising because we've become wealthier.\n\"In reality, the numbers reflect more damage from catastrophes because the world is getting wealthier. We’re seeing ever-larger losses simply because we have more to lose — when an earthquake or flood occurs, more stuff gets damaged.\"\nIt's true that some of the rising costs due to these disasters can be explained by increased wealth. Geophysical events like earthquakes aren't becoming more frequent, but they are costing more than they used to. However, the frequency of storms and floods is increasing rapidly, and their costs are rising faster than the costs of earthquakes. This suggests that something (i.e. climate change) is adding to the frequency and costs of these disaster events.\nPielke's research focuses on the 'normalized' costs of land-falling hurricanes in the USA, where he accounts for rising wealth and population density in regions prone to hurricanes. When accounting for a few such factors, Pielke finds no long-term trend in 'normalized' damages, and thus concludes that climate change isn't making these storms any more expensive. He argues we've just got more stuff in areas hit by hurricanes, and that increase in stuff is what's causing storm costs to rise.\nHowever, Pielke ignores some important factors in his normalization procedure. For example, our engineering has improved significantly over the past century to make buildings more resistant to hurricane damage. We now have instruments orbiting the Earth on satellites tracking storms, weather models predicting their paths, and communications technology to warn people well before they make landfall. This technology costs money, as evidenced by the fact that funding cuts may soon cripple our ability to predict hurricane tracks, for example. Yet Pielke fails to account for the costs of these improved technologies. This point was made by Judith Curry in 2007.\n\"The second problem with the analysis is that the paper does not account for major engineering improvements that rendered these regions in Florida less susceptible to damage.\"\nKevin Trenberth also made this point in Science in 2010.\n\"He completely ignores the benefits from improvements in hurricane warning times, changes in building codes, and other factors that have been important in reducing losses.\"\nMunich Re also disagrees with Pielke (emphasis added),\n\"Several of the events of 2013 illustrated how well warnings and loss minimisation measures can restrict the impact of natural catastrophes. In the case of the most recent winter storms in Europe, for example, the losses remained comparatively low”, said Torsten Jeworrek, Munich Re Board member responsible for global reinsurance business.\"\nCurry also points out that the lack of a long-term trend in hurricane damage losses in Pielke's analysis depends heavily on two large, expensive hurricane strikes in the 1920s (the Great Miami Hurricane of 1926 and the Lake Okeechobee Hurricane of 1928). However, property values were badly inflated leading up to the US stock market crash of 1929, and Pielke fails to account for this inflation factor. Curry concludes,\n\"If you omit the data prior to the 1930’s, and look for the decade early in the period with the largest total damage, it turns out to be 1936-1945 ... The period post 1929 with the greatest amount damage is 1996-2005, which is 84% greater than the period 1936-1945. Such a conclusion is counter to Pielke’s conclusion that found no trend in damage.\"\nPielke has dug himself even deeper into a hole in trying to support this myth by claiming that efforts and technologies to mitigate disaster damages don't make a difference in damage trends \"for floods, U.S. hurricanes or tornadoes.\" The problem is that those referenced papers he links don't support his claims.\nThe US hurricanes reference goes to one of Pielke's own papers, published in 2008. That paper specifically states,\n\"The normalization methodologies do not explicitly reflect two important factors driving losses: demand surge and loss mitigation. Adjustments for these factors are beyond the scope of this paper, but it is important for those using this study to consider their potential effect.\"\nThe paper includes a brief discussion of improved building codes in Florida potentially reducing disaster losses by up to 40%, but then dismisses their importance by claiming \"As strong codes have only been implemented in recent years and in some cases vary significantly on a county-by-county basis, their effect on overall losses is unlikely to be large.\" However, this speculation is not substantiated with any sort of analysis, does not include areas outside of Florida, and does not include mitigation measures other than improved building codes.\nIn fact, a study of Florida building performance found that homes built after 2002 sustained less average hurricane damage than those built between 1994 and 2001, which in turn sustained less damage than those built before 1994. This is strong evidence that improvements in building resistance to hurricane damage is making a marked difference in damage losses – a difference Pielke does not account for.\nElsewhere, Pielke referenced a 2011 paper by Barthel & Neumeyer to try and support his argument that mitigation makes no difference in cost trends. However, the same authors published a paper in 2012 that concluded (emphasis added),\n\"A trend analysis of normalized insured damage from natural disasters is not only of interest to the insurance industry, but can potentially be useful for attempts at detecting whether there has been an increase in the frequency and/or intensity of natural hazards, whether caused by natural climate variability or anthropogenic climate change...We find no significant trends at the global level, but we detect statistically significant upward trends in normalized insured losses from all non-geophysical disasters as well as from certain specific disaster types in the United States and West Germany.\"\nAnother paper, Schmidt et al. (2009) also looks at US hurricane losses and concludes,\n\"In the period 1971–2005, since the beginning of a trend towards increased intense cyclone activity, losses excluding socio-economic effects show an annual increase of 4% per annum. This increase must therefore be at least due to the impact of natural climate variability but, more likely than not, also due to anthropogenic forcings.\"\nIn short, Pielke's own paper does not remotely support Pielke's claim that mitigation doesn't make a difference in US hurricane disaster damages. Pielke has misrepresented his own research. The other referenced papers don't seem to support Pielke's claims regarding flood or tornado mitigation either, and research Pielke neglects does find rising trends in normalized disaster losses in some areas, like from US hurricanes and severe thunderstorms.\nIt's also not surprising that hurricanes would now be doing more damage, because research has shown that the most intense hurricanes are already occurring more often as a result of human-caused global warming. This, combined with rising sea levels, has also led to larger storm surges and the costs of the damage that goes with them. As Grinsted et al. (2013) concluded,\n\"we have probably crossed the threshold where Katrina magnitude hurricane surges are more likely caused by global warming than not.\"\nGlobal warming also adds moisture to the atmosphere, with the increase in precipitation also adding to the flooding associated with these storms, and the damages they cause.\nOddly, Pielke seems to contradict himself in his concluding statement, saying,\n\"As countries become richer, they are better able to deal with disasters — meaning more people are protected and fewer lose their lives. Increased property losses, it turns out, are a price worth paying.\"\nThis implies that Pielke understands that the costs of the technologies that he neglects, because those costs \"are a price worth paying.\" Apparently they're just not worth including in his calculations.\nIt's also worth noting that another of Pielke's claims, \"In fact, today’s climate models suggest that future changes in extremes that cause the most damage won’t be detectable in the statistics of weather (or damage) for many decades,\" is not remotely true, as illustrated in this post. To support this incorrect claim, Pielke references his own work, which is limited to the statistics of economic loss data for tropical cyclones only (and is contradicted by other research on that subject, as discussed above). However, trends in many extremes are already detectable, including in North Atlantic hurricane intensity (e.g. see Emanuel 2005, Elsner et al. 2008, Knutson et al. 2010, Emanuel 2012, Kunkel et al. 2013, Grinsted et al. 2013, Holland and Bruyère 2013). Pielke's own paper even acknowledges this point, so once again he's misrepresented his own research:\n\"This result confirms the general agreement that it is far more efficient to seek to detect anthropogenic signals in geophysical data directly rather than in loss data\"\nThe bottom line is that many types of extreme weather are being intensified by human-caused global warming, and that will continue in the future. And there is evidence that climate change is adding to the costs of extreme weather damage.\nLast updated on 1 April 2014 by dana1981. View Archives"
  },
  {
   "title": "Climate is chaotic and cannot be predicted",
   "paragraph": "Chaos theory and global warming: can climate be predicted?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nWeather is chaotic but climate is driven by Earth's energy imbalance, which is more predictable.\nClimate Myth...\nClimate is chaotic and cannot be predicted\n'Lorenz (1963), in the landmark paper that founded chaos theory, said that because the climate is a mathematically-chaotic object (a point which the UN's climate panel admits), accurate long-term prediction of the future evolution of the climate is not possible \"by any method\". At present, climate forecasts even as little as six weeks ahead can be diametrically the opposite of what actually occurs, even if the forecasts are limited to a small region of the planet.' (Christopher Monckton)\nOne of the defining traits of a chaotic system is 'sensitive dependence to initial conditions'. This means that even very small changes in the state of the system can quickly and radically change the way that the system develops over time. Edward Lorenz's landmark 1963 paper demonstrated this behavior in a simulation of fluid turbulence, and ended hopes for long-term weather forecasting.\nHowever, climate is not weather, and modeling is not forecasting.\nAlthough it is generally not possible to predict a specific future state of a chaotic system (there is no telling what temperature it will be in Oregon on December 21 2012), it is still possible to make statistical claims about the behavior of the system as a whole (it is very likely that Oregon's December 2012 temperatures will be colder than its July 2012 temperatures). There are chaotic components to the climate system, such as El Nino and fluid turbulence, but they all have much less long-term influence than the greenhouse effect. It's a little like an airplane flying through stormy weather: It may be buffeted around from moment to moment, but it can still move from one airport to another.\nNor do climate models generally produce weather forecasts. Models often run a simulation multiple times with different starting conditions, and the ensemble of results are examined for common properties (one example: Easterling 2009). This is, incidentally, a technique used by mathematicians to study the Lorenz functions.\nThe chaotic nature of turbulence is no real obstacle to climate modeling, and it does not negate the existence or attribution of climate change.\nLast updated on 8 September 2010 by chuckbot."
  },
  {
   "title": "Climate science peer review is pal review",
   "paragraph": "How contrarians used pal review to publish contrarian papers\nLink to this page\nWhat the science says...\nThe lone documented case of true 'pal review' was committed by climate contrarians in the journal Climate Research from 1997 to 2003, during which time editor Chris de Freitas accepted 14 papers from a select group of contrarians. The journal had not published any papers from that group of authors previously, and only published 2 more papers from the group of 'pals' after de Freitas left.\nClimate Myth...\nClimate science peer review is pal review\n\"Peer review has become ”pal review.” Send a paper to one of the very many journals published by the American Geophysical Union–the world’s largest publisher of academic climate science–and you can suggest five reviewers. The editor doesn’t have to take your advice, but he’s more likely to if you bought him dinner at the last AGU meeting, isn’t he? That is, of course, unless journal editors are somehow different than government officials, congressmen, or you.\" (Patrick Michaels)\nWe often hear claims from climate contrarians that climate scientists are guilty of what they describe as \"pal review.\" The conspiracy theory goes something like this - climate scientists conduct biased research with the goal of confirming the human-caused global warming theory. They then submit their biased results to a peer-reviewed journal with friendly editors (\"pals\") who pass their paper along to friendly reviewers (other \"pals\") who give their fraudulent work the green light for publication. Thus, the contrarians argue, the preponderance of peer-reviewed literature supporting human-caused global warming is really just a sign of corruption amongst climate scientists.\nHowever, while climate contrarians are never able to produce any evidence to support their conspiracy theory, John Mashey has thoroughly documented a real world example of true pal review. Contrary to the standard conspiracy theory, the pal review did not involve mainstream climate scientists, but instead the climate contrarians themselves.\nThe True Story of Climate Research Pal Review\nMashey has done an excellent job documenting a real life case of pal review, which happened at the journal Climate Research between 1997 and 2003. That particular journal was once again brought to the forefront in the recent second Climategate stolen email release.\nIn those emails, various climate scientists had expressed concern that Climate Research was publishing shoddy papers by a small group of climate contrarians, and discussed what they could do about it. The most infamous of these papers was one by Soon and Baliunas (2003) which concluded that current global temperatures are not anomalous compared the past 1,000 years. After publishing this paper, Soon was invited by Senator James Inhofe to testify before US Congress, and the Soon and Baliunas paper was used by Congressional Republicans to justify opposition to climate legislation.\nHowever, the paper contained numerous major fundamental flaws, such as equating dryness with hotness, and was subsequently roundly refuted by an article in the American Geophysical Union journal Eos written by a number of prominent climate scientists. This paper, and Climate Research's refusal to revise or retract it, led to the resignation of five of the journal's editors, including recently-appointed editor-in-chief Hans von Storch, who explained the reason for his resignation:\n\"..the reason was that I as newly appointed Editor-in-Chief wanted to make public that the publication of the Soon & Baliunas article was an error, and that the review process at Climate Research would be changed in order to avoid similar failures. The review process had utterly failed; important questions have not been asked....It was not the first time that the process had failed, but it was the most severe case....I withdrew also als editor because I learned during the conflict that [Climate Research] editors used different scales for judging the validity of an article. Some editors considered the problem of the Soon & Baliunas paper as merely a problem of \"opinion\", while it was really a problem of severe methodological flaws. Thus, I decided that I had to disconnect from that journal, which I had served proudly for about 10 years.\"\nIn short, the journal's chief editor voiced the exact same concerns as the climate scientists in the Climategate 2 emails - that certain Climate Research editors were systematically publishing methodologically flawed papers in their journal. Soon and Baliunas were far from the only climate contrarians to benefit from the journal's friendly editorial policy. In fact, the biggest pal review beneficiary bears a very familiar name.\nPatrick Michaels and Pals\nMashey has examined the publications in Climate Research in great detail, and has produced a spreadsheet of its publications and a report summarizing his findings.\nPrior to Hans von Storch's promotion to Climate Research editor-in-chief in 2003, the journal did not have a chief editor, and so authors sent their manuscripts to an Associate Editor of their choice. One particular Associate Editor, Chris de Freitas, published 14 separate papers from a select group of 14 climate contrarians during the 6 year period of 1997 to 2003:\nSallie Baliunas, Robert Balling, John Christy, Robert Davis (both Climate Research author and editor), David Douglass, Vincent Gray, Sherwood Idso, PJ \"Chip\" Knappenberger, Ross McKitrick, Pat Michaels, Eric Posmentier, Arthur Robinson, Willie Soon, and Gerd-Rainer Weber.\nAs Mashey shows, from 1990 to 1996, Climate Research published zero papers from this group. From 1997 to 2003, the journal published 17 papers from this group, 14 with de Freitas as the Associate Editor. Serial data deleter Patrick Michaels was an author on 7 of the 14 pal reviewed papers, which also accounted for half of his total peer-reviewed publications during this timeframe. During this period, 14 of the 24 (58%) papers accepted by de Freitas came from this group of contrarians. After von Storch's resignation in 2003, de Freitas published 3 more papers from authors outside this group before leaving the journal in 2006.\nAnother on the list of 'pals', Robert Davis, was another Associate Editor at Climate Research who accepted 36 papers during his tenure, two of which were co-authored by another pal, Robert Balling. The journal also published 5 other papers from this group by non-pal editors. However, in total, at least 16 of the 21 (76%) of the papers published by Climate Research which were authored by this group of climate contrarians had pal review editors, mostly de Freitas (67%) during this six year window.\nAfter von Storch's resignation, Mashey documents that the pals' Climate Research publications dried up. Davis accepted one of Balling's papers submitted in 2004, and papers co-authored by Balling and by de Freitas were published by the journal in 2008 (Table 1). 18 of the 21 (86%) of the 15 pals' Climate Research publications were submitted in the 1997 to 2003 timeframe.\nTable 1: Climate Research publications grouped by Associate Editor. Grey bars show approximate editor tenure as derived from received dates of papers. The \"pals\" papers are shown in red capitals, 14 accepted by de Freitas (bold), and 7 handled by others (red, underlined italics). De Freitas also accepted 13 seemingly normal papers from other authors (lowercase black).\nMashey also finds that the 15 'pals' were closely connected in climate contrarian activities outside of Climate Research as well, for example working for various anti-climate think tanks, most being connected with either Fred Singer or Patrick Michaels.\n\"all have shown persistent involvement with organizations that do climate anti-science, most of which also have tobacco connections.\"\nThere is also substantial overlap with the pals joining together to author these papers (Figure 1).\nFigure 1: Overlap between pal authors of the 14 de Freitas Climate Research pal review publications between 1997 and 2003. The node numbering represents the Climate Research volume and page number of the pal publications, while the node connections represent papers written by the same pal authors (i.e. 9.3p14 and 23.1p15 were both authored by Michaels and Knappenberger). Image by jg and Kevin C.\nThe Purpose of the Mainstream Pal Review Myth\nFor those who oppose the prudent path forward with regards to climate change, which involves major global greenhouse gas emissions reductions, the scientific consensus on human-caused global warming is a very inconvenient thing. Despite the public relations damage resulting from Climategate, people still trust climate scientists' opinions about climate science (although political conservatives' trust in scientists in general has declined). However, much of the public (at least the American public) doesn't realize that there is a scientific consensus on human-caused climate change. Polls in October 2010 and September 2011 found that 44% and 37% of the American public believes that scientists are divided regarding the cause of global warming, respectively.\nAccording to the March 2012 George Mason Center for Climate Change Communication (CCCC) national poll, climate scientists are the most trusted source for climate science information, with 74% of public trust (Figure 2). However, a large segment of the population believes there is a major scientific debate on the subject, no doubt thanks to the false media balance which gives the ~3% minority of experts who think humans aren't the dominant cause of the current climate change (and their non-expert surrogates) ~50% of the media attention. Therefore, many people don't believe that humans are the primary cause of global warming (approximately 41% of Americans).\nFigure 2: Responses to the George Mason CCCC poll question \"How much do you trust or distrust the following as a source of information about global warming?\"\nThe numbers reveal a stark picture: 76% of Americans trust climate scientists, but 41% think scientists are divided on the causes of the warming, and 41% think the observed warming is mostly natural.\nThus as Ding et al. (2011) concluded, if a larger percentage of people realized that there is a scientific consensus on the issue amongst the group they trust most on the subject (and rightly so), more people would believe that humans are causing global warming, and more people would demand that we do something about it. The lack of public awareness of the scientific consensus on human-caused climate change is one of the biggest obstacle to taking climate mitigation action.\nFor this reason, climate contrarians have attacked the scientific consensus from many different angles. Some have tried to attack the credibility of the many different surveys and studies documenting the consensus. Others simply ignore this documentation and deny the consensus exists at all.\nThe third group, discussed in this post, attacks the credibility of the consensus itself, claiming it's all part of a massive fraudulent conspiracy of thousands of corrupt climate scientists (note that conspiracy theories are one of the five characteristics of scientific denialism). Ironically, this conspiracy theory has been most recently voiced by pal review beneficiary Patrick Michaels.\n\"Peer review has become ”pal review.” Send a paper to one of the very many journals published by the American Geophysical Union–the world’s largest publisher of academic climate science–and you can suggest five reviewers. The editor doesn’t have to take your advice, but he’s more likely to if you bought him dinner at the last AGU meeting, isn’t he? That is, of course, unless journal editors are somehow different than government officials, congressmen, or you.\"\nMichaels of course provides no evidence whatsoever to support this conspiracy theory of peer-review corruption. He expects us to swallow his tale of \"pal review\" - the conspiracy theory that thousands of climate scientists are publishing thousands of biased papers every year in order to keep the human-caused global warming theory propped up - based on nothing more than his say-so.\nWhile Michaels is indeed something of an expert on the subject, his expertise comes from himself being one of the individuals most guilty of engaging in climate research pal review.\nPal Review Summary\nWhile Patrick Michaels has accused mainstream climate scientists of a vast conspiracy involving pal review (and exposed his own characteristic of scientific denialism in the process) without any substantiation or supporting evidence, in reality Patrick Michaels himself was the biggest beneficiary in the one actual demonstrated case of climate science pal review, as documented by Mashey.\nA group of 14 climate contrarians found a sympathetic journal editor who proceeded to publish a large number of papers from this group over a very short timeframe, many of which were scientifically flawed, some of which were subsequently used by politicians to oppose climate legislation.\nIronically, the climate scientists who tried to do something about this problem have themselves been accused of trying to \"hijack\" or \"subvert\" the peer-review process. And of course the guiltiest party of all, Patrick Michaels has accused thousands of climate scientists of the sort of pal review he himself engaged in.\nOur tale is one of irony, hypocrisy, and projection. The next time you see a complaint about the fairy tale of rampant climate science \"pal review\", direct the accuser to John Mashey's documentation of a pal review true story.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 19 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "Climate scientists are in it for the money",
   "paragraph": "Climate scientists would make more money in other careers\nLink to this page\nWhat the science says...\nClimate scientists could make far more money in other careers - most notably, working for the oil industry.\nClimate Myth...\nClimate scientists are in it for the money\nIn truth, the overwhelming majority of climate-research funding comes from the federal government and left-wing foundations. And while the energy industry funds both sides of the climate debate, the government/foundation monies go only toward research that advances the warming regulatory agenda. With a clear public-policy outcome in mind, the government/foundation gravy train is a much greater threat to scientific integrity.\n-Henry Payne, National Review\nIf you are reading the comments on basically any climate change related article, it won't take long to get to one (or more!) commenters boldly claiming that \"climate scientists are only in it for the money\". This will often be accompanied by outrageously high $ amounts to really get anybody's hackles up but without any real evidence for their statement.\nIn a video as part of her Global Weirding series with PBS, Katharine Hayhoe comprehensively debunks this myth.\nRichard Alley makes some relevant points in this interview snippet:\nMany of the scientists interviewed for Denial101x also explain why they do what they do and it doesn't have anything to do with money (big surprise!). All those expert interviews are available in the Wakelet-collection Denial101x Expert Interviews.\nJohn Timmer also tackled this myth at ArsTechnica in 2011 and 2012:\nSo, are there big bucks to be had in climate science? Since it doesn't have a lot of commercial appeal, most of the people working in the area, and the vast majority of those publishing the scientific literature, work in academic departments or at government agencies. Penn State, home of noted climatologists Richard Alley and Michael Mann, has a strong geosciences department and, conveniently, makes the department's salary information available. It's easy to check, and find that the average tenured professor earned about $120,000 last year, and a new hire a bit less than $70,000.\nAs did Scott Mandia on his blog:\nAre scientists getting rich from grant funding? I will use myself as a case study in this post and, in Part II, I will write about others’ experiences.\nI recall a lecture I gave on climate change back in April 2009. After I was finished, a gentleman told me that he though[sic] the whole thing was a hoax so that we scientists could get rich from funding. Before I even had a chance to reply, a voice from the crowd (my wife) yelled out, “Trust me, I can tell you, he isn’t making any money from this. Nada. Zip. Zilch. Nothing!” The truth hurts, doesn’t it?\nLast updated on 25 November 2017 by dana1981. View Archives"
  },
  {
   "title": "Climate sensitivity is low",
   "paragraph": "How sensitive is our climate?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nNet positive feedback is confirmed by many different lines of evidence.\nClimate Myth...\nClimate sensitivity is low\n\"His [Dr Spencer's] latest research demonstrates that – in the short term, at any rate – the temperature feedbacks that the IPCC imagines will greatly amplify any initial warming caused by CO2 are net-negative, attenuating the warming they are supposed to enhance. His best estimate is that the warming in response to a doubling of CO2 concentration, which may happen this century unless the usual suspects get away with shutting down the economies of the West, will be a harmless 1 Fahrenheit degree, not the 6 F predicted by the IPCC.\" (Christopher Monckton)\nClimate sensitivity is the estimate of how much the earth's climate will warm in response to the increased greenhouse effect if we double the amount of carbon dioxide in the atmosphere. This includes feedbacks which can either amplify or dampen that warming. This is very important because if it is low, as some climate 'skeptics' argue, then the planet will warm slowly and we will have more time to react and adapt. If sensitivity is high, then we could be in for a very bad time indeed.\nThere are two ways of working out what climate sensitivity is. The first method is by modelling:\nClimate models have predicted the least temperature rise would be on average 1.65°C (2.97°F) , but upper estimates vary a lot, averaging 5.2°C (9.36°F). Current best estimates are for a rise of around 3°C (5.4°F), with a likely maximum of 4.5°C (8.1°F).\nThe second method calculates climate sensitivity directly from physical evidence, by looking at climate changes in the distant past:\nVarious paleoclimate-based equilibrium climate sensitivity estimates from a range of geologic eras. Adapted from PALEOSENS (2012) Figure 3a by John Cook.\nThese calculations use data from sources like ice cores to work out how much additional heat the doubling of greenhouse gases will produce. These estimates are very consistent, finding between 2 and 4.5°C global surface warming in response to doubled carbon dioxide.\nIt’s all a matter of degree\nAll the models and evidence confirm a minimum warming close to 2°C for a doubling of atmospheric CO2 with a most likely value of 3°C and the potential to warm 4.5°C or even more. Even such a small rise would signal many damaging and highly disruptive changes to the environment. In this light, the arguments against reducing greenhouse gas emissions because of climate sensitivity are a form of gambling. A minority claim the climate is less sensitive than we think, the implication being we don’t need to do anything much about it. Others suggest that because we can't tell for sure, we should wait and see.\nIn truth, nobody knows for sure quite how much the temperature will rise, but rise it will. Inaction or complacency heightens risk, gambling with the entire ecology of the planet, and the welfare of everyone on it.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 July 2015 by skeptickev. View Archives"
  },
  {
   "title": "Climate's changed before",
   "paragraph": "What does past climate change tell us about global warming?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nGreenhouse gasses, principally CO2, have controlled most ancient climate changes. This time around humans are the cause, mainly by our CO2 emissions.\nClimate Myth...\nClimate's changed before\nClimate is always changing. We have had ice ages and warmer periods when alligators were found in Spitzbergen. Ice ages have occurred in a hundred thousand year cycle for the last 700 thousand years, and there have been previous periods that appear to have been warmer than the present despite CO2 levels being lower than they are now. More recently, we have had the medieval warm period and the little ice age. (Richard Lindzen)\nGreenhouse gasses – mainly CO2, but also methane – were involved in most of the climate changes in Earth’s past. When they were reduced, the global climate became colder. When they were increased, the global climate became warmer. When CO2 levels jumped rapidly, the global warming that resulted was highly disruptive and sometimes caused mass extinctions. Humans today are emitting prodigious quantities of CO2, at a rate faster than even the most destructive climate changes in earth's past.\nAbrupt vs slow change.\nLife flourished in the Eocene, the Cretaceous and other times of high CO2 in the atmosphere because the greenhouse gasses were in balance with the carbon in the oceans and the weathering of rocks. Life, ocean chemistry, and atmospheric gasses had millions of years to adjust to those levels.\nLush life in the Arctic during the Eocene, 50 million years ago (original art - Stephen C. Quinn, The American Museum of Natural History, N.Y.C)\nBut there have been several times in Earth’s past when Earth's temperature jumped abruptly, in much the same way as they are doing today. Those times were caused by large and rapid greenhouse gas emissions, just like humans are causing today.\nThose abrupt global warming events were almost always highly destructive for life, causing mass extinctions such as at the end of the Permian, Triassic, or even mid-Cambrian periods. The symptoms from those events (a big, rapid jump in global temperatures, rising sea levels, and ocean acidification) are all happening today with human-caused climate change.\nSo yes, the climate has changed before humans, and in most cases scientists know why. In all cases we see the same association between CO2 levels and global temperatures. And past examples of rapid carbon emissions (just like today) were generally highly destructive to life on Earth.\nBasic rebuttal written by howardlee\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 6 August 2015 by pattimer. View Archives"
  },
  {
   "title": "Climategate CRU emails suggest conspiracy",
   "paragraph": "What do the 'Climategate' hacked CRU emails tell us?\nLink to this page\nWhat the science says...\nSelect a level... Intermediate Advanced\nA number of independent investigations from different countries, universities and government bodies have investigated the stolen emails and found no evidence of wrong doing. Focusing on a few suggestive emails, taken out of context, merely serves to distract from the wealth of empirical evidence for man-made global warming.\nClimate Myth...\nClimategate CRU emails suggest conspiracy\n“[T]he 1079 emails and 72 documents seem indeed evidence of a scandal involving most of the most prominent scientists pushing the man-made warming theory - a scandal that is one of the greatest in modern science. […] emails suggesting conspiracy, collusion in exaggerating warming data, possibly illegal destruction of embarrassing information, organised resistance to disclosure, manipulation of data, private admissions of flaws in their public claims and much more.” (Andrew Bolt, Herald Sun)\nIn November 2009, the servers at the University of East Anglia in Britain were illegally hacked and emails were stolen. When a selection of emails between climate scientists were published on the internet, a few suggestive quotes were seized upon by many claiming global warming was all just a conspiracy. A number of independent enquiries have investigated the conduct of the scientists involved in the emails. All have cleared the scientists of any wrong doing:\nIn February 2010, the Pennsylvania State University released an Inquiry Report that investigated any 'Climategate' emails involving Dr Michael Mann, a Professor of Penn State's Department of Meteorology. They found that \"there exists no credible evidence that Dr. Mann had or has ever engaged in, or participated in, directly or indirectly, any actions with an intent to suppress or to falsify data\". On \"Mike's Nature trick\", they concluded \"The so-called “trick”1 was nothing more than a statistical method used to bring two or more different kinds of data sets together in a legitimate fashion by a technique that has been reviewed by a broad array of peers in the field.\"\nIn March 2010, the UK government's House of Commons Science and Technology Committee published a report finding that the criticisms of the Climate Research Unit (CRU) were misplaced and that CRU’s \"Professor Jones’s actions were in line with common practice in the climate science community\".\nIn April 2010, the University of East Anglia set up an international Scientific Assessment Panel, in consultation with the Royal Society and chaired by Professor Ron Oxburgh. The Report of the International Panel assessed the integrity of the research published by the CRU and found \"no evidence of any deliberate scientific malpractice in any of the work of the Climatic Research Unit\".\nIn June 2010, the Pennsylvania State University published their Final Investigation Report, determining \"there is no substance to the allegation against Dr. Michael E. Mann\".\nIn July 2010, the University of East Anglia published the Independent Climate Change Email Review report. They examined the emails to assess whether manipulation or suppression of data occurred and concluded that \"we find that their rigour and honesty as scientists are not in doubt.\"\nIn July 2010, the US Environmental Protection Agency investigated the emails and \"found this was simply a candid discussion of scientists working through issues that arise in compiling and presenting large complex data sets.\"\nIn September 2010, the UK Government responded to the House of Commons Science and Technology Committee report, chaired by Sir Muir Russell. On the issue of releasing data, they found \"In the instance of the CRU, the scientists were not legally allowed to give out the data\". On the issue of attempting to corrupt the peer-review process, they found \"The evidence that we have seen does not suggest that Professor Jones was trying to subvert the peer review process. Academics should not be criticised for making informal comments on academic papers\".\nIn February 2011, the Department of Commerce Inspector General conducted an independent review of the emails and found \"no evidence in the CRU emails that NOAA inappropriately manipulated data\".\nIn August 2011, the National Science Foundation concluded \"Finding no research misconduct or other matter raised by the various regulations and laws discussed above, this case is closed\".\nJust as there are many independent lines of evidence that humans are causing global warming, similarly a number of independent investigations have found no evidence of falsification or conspiracy by climate scientists.\n\"Mike's Nature trick\" and \"hide the decline\"\nThe most quoted email is from Phil Jones discussing paleo-data used to reconstruct past temperatures (emphasis mine):\n\"I've just completed Mike's Nature trick of adding in the real temps to each series for the last 20 years (ie from 1981 onwards) and from 1961 for Keith's to hide the decline.\"\n\"Mike's Nature trick\" refers to a technique (aka \"trick of the trade\") used in a paper published in Nature by lead author Michael Mann (Mann 1998). The \"trick\" is the technique of plotting recent instrumental data along with the reconstructed data. This places recent global warming trends in the context of temperature changes over longer time scales.\nThe most common misconception regarding this email is the assumption that \"decline\" refers to declining temperatures. It actually refers to a decline in the reliability of tree rings to reflect temperatures after 1960. This is known as the \"divergence problem\" where tree ring proxies diverge from modern instrumental temperature records after 1960. The divergence problem is discussed in the peer reviewed literature as early as 1995, suggesting a change in the sensitivity of tree growth to temperature in recent decades (Briffa 1998). It is also examined more recently in Wilmking 2008 which explores techniques in eliminating the divergence problem. So when you look at Phil Jone's email in the context of the science discussed, it is not the schemings of a climate conspiracy but technical discussions of data handling techniques available in the peer reviewed literature. More on the hockey stick divergence problem...\nTrenberth's \"travesty we can't account for the lack of warming\"\nThe second most cited email is from climate scientist and IPCC lead author Kevin Trenberth. The highlighted quote is this: \"The fact is that we can't account for the lack of warming at the moment and it is a travesty that we can't.\" This has been most commonly interpreted (among skeptics) as climate scientists secretly admitting amongst themselves that global warming really has stopped. Trenberth is actually discussing a paper he'd recently published that discusses the planet's energy budget - how much net energy is flowing into our climate and where it's going (Trenberth 2009).\nIn Trenberth's paper, he discusses how we know the planet is continually heating due to increasing carbon dioxide. Nevertheless, surface temperature sometimes shows short term cooling periods. This is due to internal variability and Trenberth was lamenting that our observation systems can't comprehensively track all the energy flow through the climate system. More on Trenberth's travesty...\nThe full body of evidence for man-made global warming\nAn important point to realise is that the emails involve a handful of scientists discussing a few pieces of climate data. Even without this data, there is still an overwhelming and consistent body of evidence, painstakingly compiled by independent scientific teams from institutions across the world.\nWhat do they find? The planet is steadily accumulating heat. When you add up all the heat building in the oceans, land and atmosphere plus the energy required to melt glaciers and ice sheets, the planet has been accumulating heat at a rate of 190,260 Gigawatts over the past 40 years (Murphy 2009). Considering a typical nuclear power plant has an output of 1 Gigawatt, imagine over 190,000 power plants pouring their energy output directly into heating our land and oceans, melting ice and warming the air.\nThis build-up of heat is causing ice loss across the globe, from the Arctic to the Antarctic. Both Greenland and Antarctica are losing ice at an accelerated rate (Velicogna 2009, ). Even East Antarctica, previously thought to be too cold and stable, is now losing ice mass (Chen 2009). Glacier shrinkage is accelerating. Arctic sea ice has fallen so sharply, observations exceed even the IPCC worst case scenario. The combination of warming oceans and melting ice has resulted in sea level rise tracking the upper limit of IPCC predictions.\nRising temperatures have impacted animal and plant species worldwide. The distribution of tree lines, plants and many species of animals are moving into cooler regions towards the poles. As the onset of spring is happening earlier each year, animal and plant species are responding to the shift in seasons. Scientists observe that frog breeding, bird nesting, flowering and migration patterns are all occurring earlier in the year (Parmeson 2003). There are many other physical signs of widespread warming. The height of the tropopause, a layer in our atmosphere, is rising (Santer 2003). Arctic permafrost, covering about 25% of Northern Hemisphere land, is warming and degrading (Walsh 2009). The tropical belt is widening (Seidel 2007). These results are all consistent with global warming.\nWhat’s causing this heat build-up? Humans are emitting huge amounts of carbon dioxide into the atmosphere - 29 billion tonnes in 2009 (CDIAC). Greenhouse theory predicts that more carbon dioxide in the atmosphere will trap heat energy as it escapes out to space. What do we observe? Carbon dioxide absorbs heat at certain wavelengths. Satellites over the past 40 years find less heat escaping to space at these wavelengths (Harries 2001, Griggs 2004, Chen 2007). Where does the heat go? Surface measurements find more heat returning back to the Earth's surface (Philipona 2004). Tellingly, the increase occurs at those same carbon dioxide absorption wavelengths (Evans 2006). This is the human fingerprint in global warming.\nThere are multiple lines of empirical evidence that global warming is happening and human activity is the cause. A few suggestive emails may serve as a useful distraction for those wishing to avoid the physical realities of climate change. But they change nothing about our scientific understanding of humanity’s role in global warming.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nExpert interview with Kevin Trenberth\nLast updated on 14 October 2016 by pattimer. View Archives"
  },
  {
   "title": "Clouds provide negative feedback",
   "paragraph": "What is the net feedback from clouds?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nEvidence is building that net cloud feedback is likely positive and unlikely to be strongly negative.\nClimate Myth...\nClouds provide negative feedback\n\"Climate models used by the International Panel on Climate Change (IPCC) assume that clouds provide a large positive feedback, greatly amplifying the small warming effect of increasing CO2 content in air. Clouds have made fools of climate modelers. A detailed analysis of cloud behavior from satellite data by Dr. Roy Spencer of the University of Alabama in Huntsville shows that clouds actually provide a strong negative feedback, the opposite of that assumed by the climate modelers. The modelers confused cause and effect, thereby getting the feedback in the wrong direction.\" (Ken Gregory)\nThe effect of clouds in a warming world is complicated. One challenge is that clouds cause both warming and cooling. Low-level clouds tend to cool by reflecting sunlight. High-level clouds tend to warm by trapping heat.\nAs the planet warms, clouds have a cooling effect if there are more low-level clouds or less high-level clouds. Clouds would cause more warming if the opposite is true. To work out the overall effect, scientists need to know which types of clouds are increasing or decreasing.\nSome climate scientists, such as Richard Lindzen and Roy Spencer, are skeptical that greenhouse gas emissions will cause dangerous warming. Their skepticism is based mainly on uncertainty related to clouds. They believe that when it warms, low-level cloud cover increases. This would mean the Earth's overall reflectiveness would increase. This causes cooling, which would cancel out some of the warming from an increased greenhouse effect.\nHowever, recent evidence indicates this is not the case. Two separate studies have looked at cloud changes in the tropics and subtropics using a combination of ship-based cloud observations, satellite observations and climate models. Both found that cloud feedback in this region appears to be positive, meaning more warming.\nAnother study used satellite measurements of cloud cover over the entire planet to measure cloud feedback. Although a very small negative feedback (cooling) could not be ruled out, the overall short-term global cloud feedback was probably positive (warming). It is very unlikely that the cloud feedback will cause enough cooling to offset much of human-caused global warming.\nOther studies have found that the climate models that best simulate cloud changes are the ones that find it to be a positive feedback, and thus have higher climate sensitivities. Steven Sherwood explains one such study:\nWhile clouds remain an uncertainty, the evidence is building that clouds will probably cause the planet to warm even further, and are very unlikely to cancel out much of human-caused global warming. It's also important to remember that there many other feedbacks besides clouds. There is a large amount of evidence that the net feedback is positive and will amplify global warming.\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 25 July 2017 by skeptickev. View Archives"
  },
  {
   "title": "CO2 effect is saturated",
   "paragraph": "Is the CO2 effect saturated?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nThe notion that the CO2 effect is 'saturated' is based on a misunderstanding of how the greenhouse effect works.\nClimate Myth...\nCO2 effect is saturated\n\"Each unit of CO2 you put into the atmosphere has less and less of a warming impact. Once the atmosphere reaches a saturation point, additional input of CO2 will not really have any major impact. It's like putting insulation in your attic. They give a recommended amount and after that you can stack the insulation up to the roof and it's going to have no impact.\" (Marc Morano, as quoted by Steve Eliot)\nThe mistaken idea that the Greenhouse Effect is 'saturated', that adding more CO2 will have virtually no effect, is based on a simple misunderstanding of how the Greenhouse Effect works.\nThe myth goes something like this:\nCO2 absorbs nearly all the Infrared (heat) radiation leaving the Earth's surface that it can absorb. True!\nTherefore adding more CO2 won't absorb much more IR radiation at the surface. True!\nTherefore adding more CO2 can't cause more warming. FALSE!!!\nHere's why; it ignores the very simplest arithmetic.\nIf the air is only absorbing heat from the surface then the air should just keep getting hotter and hotter. By now the Earth should be a cinder from all that absorbed heat. But not too surprisingly, it isn't! What are we missing?\nThe air doesn't just absorb heat, it also loses it as well! The atmosphere isn't just absorbing IR Radiation (heat) from the surface. It is also radiating IR Radiation (heat) to Space. If these two heat flows are in balance, the atmosphere doesn't warm or cool - it stays the same.\nLets think about a simple analogy:\nWe have a water tank. A pump is adding water to the tank at, perhaps, 100 litres per minute. And an outlet pipe is letting water drain out of the tank at 100 litres per minute. What is happening to the water level in the tank? It is remaining steady because the flows into and out of the tank are the same. In our analogy the pump adding water is the absorption of heat by the atmosphere; the water flowing from the outlet pipe is the heat being radiated out to space. And the volume of water inside the tank is the amount of heat in the atmosphere.\nWhat might we do to increase the water level in the tank?\nWe might increase the speed of the pump that is adding water to the tank. That would raise the water level. But if the pump is already running at nearly its top speed, I can't add water any faster. That would fit the 'It's Saturated' claim: the pump can't run much faster just as the atmosphere can't absorb the Sun's heat any faster\nBut what if we restricted the outlet, so that it was harder for water to get out of the tank? The same amount of water is flowing in but less is flowing out. So the water level in the tank will rise. We can change the water level in our tank without changing how much water is flowing in, by changing how much water is flowing out.\nSimilarly we can change how much heat there is in the atmosphere by restricting how much heat leaves the atmosphere rather than by increasing how much is being absorbed by the atmosphere.\nThis is how the Greenhouse Effect works. The Greenhouse gases such as carbon dioxide and water vapour absorb most of the heat radiation leaving the Earth's surface. Then their concentration determines how much heat escapes from the top of the atmosphere to space. It is the change in what happens at the top of the atmosphere that matters, not what happens down here near the surface.\nSo how does changing the concentration of a Greenhouse gas change how much heat escapes from the upper atmosphere? As we climb higher in the atmosphere the air gets thinner. There is less of all gases, including the greenhouse gases. Eventually the air becomes thin enough that any heat radiated by the air can escape all the way to Space. How much heat escapes to space from this altitude then depends on how cold the air is at that height. The colder the air, the less heat it radiates.\n(OK, I'm Australian so this image appeals to me)\nSo if we add more greenhouse gases the air needs to be thinner before heat radiation is able to escape to space. So this can only happen higher in the atmosphere. Where it is colder. So the amount of heat escaping is reduced.\nBy adding greenhouse gases, we force the radiation to space to come from higher, colder air, reducing the flow of radiation to space. And there is still a lot of scope for more greenhouse gases to push 'the action' higher and higher, into colder and colder air, restricting the rate of radiation to space even further.\nThe Greenhouse Effect isn't even remotely Saturated. Myth Busted!\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 July 2015 by pattimer. View Archives"
  },
  {
   "title": "CO2 increase is natural, not human-caused",
   "paragraph": "What is causing the increase in atmospheric CO2?\nLink to this page\nWhat the science says...\nThere are many lines of evidence which clearly show that the atmospheric CO2 increase is caused by humans. The clearest of these is simple accounting - humans are emitting CO2 at a rate twice as fast as the atmospheric increase (natural sinks are absorbing the other half). There is no question whatsoever that the CO2 increase is human-caused. This is settled science.\nClimate Myth...\nCO2 increase is natural, not human-caused\nthat atmospheric CO2 increase that we observe is a product of temperature increase, and not the other way around, meaning it is a product of natural variation...it may be the Emily Litella moment for climate science and CO2 – “Never mind…” (Anthony Watts)\nSimple Accounting\nThe easiest way to prove that the atmospheric CO2 increase is man-made is through a simple accounting approach (i.e. see Cawley 2011). The equation for the change in atmospheric CO2 (ΔCatm) is\nThis says that if we ‘emit’ a ton of carbon by, say, triggering a volcano then the atmosphere will gain a ton. If we ‘absorb’ a ton of carbon by growing a tree, then the atmosphere loses a ton. We can expand the equation by counting human emissions (HE) and absorption (HA) and natural emissions (NE) and absorption (NA) separately.\nThis works because carbon is additive. If a volcano emits a ton of carbon and a factory emits a ton then the atmosphere has gained two tons. This is a very simple balance sheet for the carbon cycle and fortunately there are ‘accountants’ who have measured some of these values for us.\nRecently the amount of CO2 in the atmosphere has been rising at 2.4 parts per million per year, or around 19 billion tons/year. Meanwhile human emissions excluding land use change (like clearing or planting forests) are 37 billion tons per year. In billions of tons per year we have:\n​\nWe can rearrange this:\nHumans are also clearing rainforests and changing land use, but here we'll assume that human effects on absorption (HA) are not much different from zero, i.e.\nSo Natural Absorption (NA) must be bigger than Natural Emissions (NE). Nature is absorbing more CO2 than it is emitting. It is not causing atmospheric CO2 to rise at all - in fact it is acting to try and reduce atmospheric CO2, and thus the long term rise is entirely because of humans.\nOcean Acidification\nThe oceans are the Earth's largest carbon storage medium, so if the atmospheric CO2 increase were \"natural\", it would likely be coming from the oceans. But we know the CO2 increase is not coming from the oceans, because the pH of the oceans is dropping (a.k.a. ocean acidification).\nWhen CO2 is absorbed into a solution, it binds with a water molecule to form a molecule of carbonic acid:\nCO2 + H2O = H2CO3\nH2CO3 has a rather strong acidifying effect in that 95% of it turns into HCO3-. This loss of an H+ ion causes the ocean pH to decrease (for more details on ocean acidification, see the OA no OK series).\nIn short, the fact that the pH of the oceans is decreasing tell us that they are absorbing more carbon than they are releasing, not vice-versa.\nOceanic CO2 Rising Fastest at the Surface\nIf CO2 were being driven into the ocean from the air, the oceanic concentration would rise fastest at the surface. If CO2 were being expelled from the oceans, we would expect to see the opposite - decreasing concentrations at the surface.\nThe World Ocean Circulation Experiment (WOCE) and the Joint Global Ocean Flux Study (JGOFS) has observed that as we expect for CO2 being driven into the oceans, concentrations of CO2 in the oceans are rising fastest at the surface.\nAtmospheric O2 is Decreasing\nBurning carbon requires oxygen (O2), and when we burn an atom of carbon, the required oxygen becomes part of the CO2 molecule. So if the CO2 increase is caused by burning carbon (fossil fuels), we would expect atmospheric O2 levels to decrease at the same rate. And that's indeed what we observe (Figure 1).\nFigure 1: Atmospheric Oxygen Concentration observed from Cape Grim, Tasmania\nThere's no reason to expect that a natural release of CO2 would have any effect on atmospheric O2 levels. On the other hand, the O2 concentration is changing exactly as we would expect from a fossil-fuel driven CO2 increase.\nCO2 Rise is Smoother than Temperature\nSome, most recently Murry Salby, have argued that the CO2 rise is in reponse to the temperature rise. However, the temperature rise has been quite erratic (because there are many factors which impact the average global temperature, especially in the short-term). If atmospheric CO2 changes were in response to temperature changes, then we would expect to see an erratic rise in CO2 as well. Instead, the atmospheric CO2 increase is very smooth, similar to the increase in human CO2 emissions.\nFigure 2: Human CO2 emissions (blue, left y-axis, Source: IEA) vs. atmospheric CO2 concentration (red, right y-axis, Source: Mauna Loa record)\nIsotopic Signature\nCarbon is composed of three different isotopes: carbon-12, 13, and 14. Carbon-12 is by far the most common, while carbon-13 is about 1% of the total, and carbon-14 accounts for only about 1 in 1 trillion carbon atoms in the atmosphere.\nCO2 produced from burning fossil fuels or burning forests has a different isotopic composition from CO2 in the atmosphere, because plants have a preference for the lighter isotopes (carbon-12 and 13); thus they have lower carbon-13 to 12 ratios. Since fossil fuels are ultimately derived from ancient plants, plants and fossil fuels all have roughly the same carbon-13 to 12 ratio – about 2% lower than that of the atmosphere. As CO2 from these materials is released into, and mixes with, the atmosphere, the average carbon-13 to 12 ratio of the atmosphere decreases.\nReconstructions of atmospheric carbon isotope ratios from various proxy sources have determined that at no time in the last 10,000 years are the carbon-13 to 12 ratios in the atmosphere as low as they are today. Furthermore, the carbon-13 to 12 ratios begin to decline dramatically just as the CO2 starts to increase — around 1850 AD. This is exactly what we expect if the increased CO2 is in fact due to fossil fuel burning beginning in the Industrial Revolution.\nFigure 3: Atmospheric carbon-13 ratio observations from Cape Grim, Tasmania\nThese isotopic observations confirm that the increase in atmospheric CO2 comes from biogenic carbon, not from the oceans or volcanoes. Some \"skeptics\" like Murry Salby argue that the carbon-13 ratio isn't unique to fossil fuels. However, because the carbon-14 ratio has also decreased significantly (Figure 4), we know it's from old (fossil fuel) sources, not modern sources. This is not new science either, it's something we've known for over half a century (Revelle and Suess 1957), and there have been many studies confirming these results. For example, Levin & Hesshaimer (2000):\n\"It has been erroneously argued that the observed atmospheric CO2 increase since the middle of the 19th century may be due to an ongoing natural perturbation of gross fluxes between the atmosphere, biosphere, and oceans. That the increase is in fact a predominantly anthropogenic disturbance, caused by accelerated release of CO2 from burning of fossil fuels, has been elegantly demonstrated through 14C analyses of tree rings from the last two centuries (Stuiver and Quay 1981; Suess 1955; Tans et al. 1979).\"\nFigure 4: Temporal change of carbon-14 ratio in tree rings grown at the Pacific coast (Levin & Hesshaimer 2000)\nSettled Science\nAs you can see, there are many lines of evidence showing that the increase in atmospheric CO2 is due to human fossil fuel combustion. Each one of these lines of evidence is very conclusive on its own, and when all put together, it's abundantly clear that the science is settled on this issue.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nAdditional videos from the MOOC\nAndy Skuce: The CO2 rise is man man-made\nInterviews with various experts\nExpert interview with Corinne Le Quéré\nLast updated on 8 July 2015 by pattimer. View Archives"
  },
  {
   "title": "CO2 is coming from the ocean",
   "paragraph": "Oceans are acidifying from absorbed CO2\nLink to this page\nWhat the science says...\nMeasurements of carbon isotopes and falling oxygen in the atmosphere show that rising carbon dioxide is due to the burning of fossil fuels and cannot be coming from the ocean.\nClimate Myth...\nCO2 is coming from the ocean\n\"The solubility of carbon dioxide in water is listed in the Handbook of Chemistry and Physics as a declining function of temperature. ... The rising values of atmospheric carbon dioxide during the time of the Mouna Loa measurements could clearly be a function of reduced solubility of CO2 in the oceans of the Planet.\" (Watts Up With That)\nWe can be confident the extra CO2 in the atmosphere has come from the oxidation of fossil fuels and not from outgassing from the ocean or from soil/land sources by using two key observations.\n1. Oxygen decrease\nAtmospheric oxygen is going down by the same amount as atmospheric CO2 is going up. Oxygen is so abundant at about 21% (209,500 ppm) that we are in no danger of running out; the change in oxygen simply shows that whatever the source of CO2 in the atmosphere, the carbon part of it has come from the oxidation of reduced carbon compounds and the oxygen has come from oxygen gas in the atmosphere. That is, the extra CO2 was not released in the form of CO2 from an unknown source but instead some reduced carbon compound was burnt in the atmosphere to produce CO2. See: AR3WG1 Section 3.5.1, especially Figure 3.4.\n2. Known fossil fuel CO2 emissions\nMost obviously, any alternative explanation for the source of the CO2 in the atmosphere has to also come up with where the 30 billion tonnes of CO2 known to be released by fossil fuel burning each year goes.\nAtmospheric CO2 is currently increasing at about 2 ppmv per year (or 16 billion tonnes). That is, only around half of the CO2 we release remains in the atmosphere. The pH decrease in the oceans corresponds to most of the “missing” CO2, so we can also be confident that land use changes etc are not a major source/sink. Caveat: Land use and biomass changes certainly soak up a lot of CO2, some it simply regrowth of forests etc, but the point is that the increasing CO2 in the atmosphere clearly demonstrates that they do not soak up enough.\nIn summary:\nAmount of increased CO2\nin the atmosphere + Amount of increased CO2\nin the oceans = Amount of known fossil\nfuel emissions of CO2\nAcknowledgements: this post was written by New Zealand chemical oceanographer, Doug Mackie.\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 10 July 2015 by pattimer. View Archives"
  },
  {
   "title": "CO2 is just a trace gas",
   "paragraph": "How substances in trace amounts can cause large effects\nLink to this page\nWhat the science says...\nSmall amounts of very active substances can cause large effects.\nClimate Myth...\nCO2 is just a trace gas\n\"We have been grossly misled to think there is tens of thousands of times as much CO2 as there is! Why has such important information been withheld from the public? If the public were aware that man-made CO2 is so incredibly small there would be very little belief in a climate disaster ...\" (Gregg Thompson)\nCO2 makes up 390 ppm (0.039%)* of the atmosphere, how can such a small amount be important? Saying that CO2 is \"only a trace gas\" is like saying that arsenic is \"only\" a trace water contaminant. Small amounts of very active substances can cause large effects.\nSome Examples of Important Small Amounts:\nHe wasn't driving drunk, he just had a trace of blood alcohol; 800 ppm (0.08%) is the limit in all 50 US states, and limits are lower in most other countries).\nDon't worry about your iron deficiency, iron is only 4.4 ppm of your body's atoms (Sterner and Eiser, 2002).\nIreland isn't important; it's only 660 ppm (0.066%) of the world population.\nThat ibuprofen pill can't do you any good; it's only 3 ppm of your body weight (200 mg in 60 kg person).\nThe Earth is insignificant, it's only 3 ppm of the mass of the solar system.\nYour children can drink that water, it only contains a trace of arsenic (0.01 ppm is the WHO and US EPA limit).\nOzone is only a trace gas: 0.1 ppm is the exposure limit established by the US National Institute for Occupational Safety and Health. The World Health Organization (WHO) recommends an ozone limit of 0.051 ppm.\nA few parts per million of ink can turn a bucket of water blue. The color is caused by the absorption of the yellow/red colors from sunlight, leaving the blue. Twice as much ink causes a much stronger color, even though the total amount is still only a trace relative to water.\n\"Traces\" of CO2\nAlthough percentage is a convenient way to talk about the amount of gas in the atmosphere, it only tells how much is there relative to everything else; percentage doesn’t give an absolute amount.\nFor example, you have trouble breathing on top of Mount Everest even though the atmosphere still contains 21% oxygen just like at sea level. The percentage isn't important, you need a certain number of oxygen molecules with each breath, regardless of how much or little they are diluted by inert gases. At an altitude of 8000 m the whole atmosphere is diluted.\nThe total number of CO2 molecules above our heads in the atmosphere is more important than their percentage in the atmosphere. If the amount of inert nitrogen gas (N2) in the atmosphere were to be cut in half then the percentage of CO2 would jump (to about 600 ppm; 0.06%) without a change in the absolute amount of CO2 and no substantial change in the energy balance of the Earth. Adding a huge number of energy-absorbing CO2 molecules to the atmosphere doesn’t change its percent number very much, only because it's being added to a vast inert N2 background.\nWe know the amount of CO2 in the atmosphere has increased because we have measured it. We know the climate has warmed from current and historical data. The link between increasing greenhouse gases and increasing temperature is clear: just as ink makes water more colored, CO2 makes the atmosphere more absorbing. The extra CO2 in our atmosphere is trapping energy that would otherwise escape to space. The measured global warming matches closely with the amount of energy trapped from the greenhouse gases added to the atmosphere.\nA doubling of the trace molecule CO2 from 280 ppm to 560 ppm is still a trace, but just like with arsenic, the difference between a small trace and a larger trace is fatal.\n* To convert ppm to percentage divide by 10,000.\nPhoto credit: http://www.photographyblogger.net/15-cool-pictures-of-ink-in-water/\nBasic rebuttal written by Sarah\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "CO2 is not a pollutant",
   "paragraph": "Is CO2 a pollutant?\nLink to this page\nWhat the science says...\nSelect a level... Intermediate Advanced\nWhile there are direct ways in which CO2 is a pollutant (acidification of the ocean), its primary impact is its greenhouse warming effect. While the greenhouse effect is a natural occurence, too much warming has severe negative impacts on agriculture, health and environment.\nClimate Myth...\nCO2 is not a pollutant\n'To suddenly label CO2 as a \"pollutant\" is a disservice to a gas that has played an enormous role in the development and sustainability of all life on this wonderful Earth. Mother Earth has clearly ruled that CO2 is not a pollutant.' (Robert Balling, as quoted by Popular Technology)\nWe commonly think of pollutants as contaminants that make the environment dirty or impure. A vivid example is sulphur dioxide, a by-product of industrial activity. High levels of sulphur dioxide cause breathing problems. Too much causes acid rain. Sulphur dioxide has a direct effect on health and the environment. Carbon dioxide, on the other hand, is a naturally occuring gas that existed in the atmosphere long before humans. Plants need it to survive. The CO2 greenhouse effect keeps our climate from freezing over. How can CO2 be considered a pollutant?\nA broader definition of pollutant is a substance that causes instability or discomfort to an ecosystem. Over the past 10,000 years, the level of atmospheric carbon dioxide in the atmosphere has remained at relatively stable levels. However, human CO2 emissions over the past few centuries have upset this balance. The increase in CO2 has some direct effects on the environment. For example, as the oceans absorb CO2 from the atmosphere, it leads to acidification that affects many marine ecosystems. However, the chief impact from rising CO2 is warmer temperatures.\nFigure 1: CO2 levels (parts per million) over the past 10,000 years. Blue line from Taylor Dome ice cores (NOAA). Green line from Law Dome ice core (CDIAC). Red line from direct measurements at Mauna Loa, Hawaii (NOAA).\nRising CO2 levels causes an enhanced greenhouse effect. This leads to warmer temperatures which has many consequences. Some effects are beneficial such as improved agriculture at high latitudes and increased vegetation growth in some circumstances. However, the negatives far outweigh the positives. Coast-bound communities are threatened by rising sea levels. Melting glaciers threaten the water supplies of hundreds of millions. Species are becoming extinct at the fastest rate in history.\nHow we choose to define the word 'pollutant' is a play in semantics. To focus on a few positive effects of carbon dioxide is to ignore the broader picture of its full impacts. The net result from increasing CO2 are severe negative impacts on our environment and the living conditions of future humanity.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 July 2015 by pattimer. View Archives"
  },
  {
   "title": "CO2 is not increasing",
   "paragraph": "Are CO2 levels increasing?\nLink to this page\nWhat the science says...\nCurrently, humans are emitting around 29 billion tonnes of carbon dioxide into the atmosphere per year. Around 43% remains in the atmosphere - this is called the 'airborne fraction'. The rest is absorbed by vegetation and the oceans. While there are questions over how much the airborne fraction is increasing, it is clear that the total amount of CO2 in the atmosphere is increasing dramatically. Current CO2 levels are the highest in 15 million years.\nClimate Myth...\nCO2 is not increasing\n\"...there is the contention by Wolfgang Knorr of the Department of Earth Sciences at the University of Bristol in England that carbon dioxide levels in the atmosphere are about where they were 160 years ago.\" (as quoted by Ken Ward Jr.)\nThe 'airborne fraction' refers to the amount of human CO2 emissions remaining in the atmosphere. Approximately 43% of our CO2 emissions stay in the atmosphere with the rest being absorbed by carbon sinks. But is the airborne fraction increasing? A paper published in November 2009 found no statistically significant trend (Knorr 2009). Anthony Watts labeled this result the \"Bombshell from Bristol\" - A potentially devastating result for anthropogenic global warming. Was it such a shock? The 2007 IPCC verdict on the airborne fraction was \"There is yet no statistically significant trend in the CO2 growth rate since 1958 .... This 'airborne fraction' has shown little variation over this period.\" (IPCC AR4) I'm not sure the move from \"not much happening\" \"to \"still not much happening\" warrants the label \"bombshell\".\nThe airborne fraction is calculated from the rate of human CO2 emissions and changes in atmospheric CO2 concentration. The global increase in atmospheric CO2 has been directly measured since 1959 and can be calculated from ice cores for earlier periods. Primarily, CO2 emissions come from fossil fuel combustion with a lesser contribution from land use changes. Fossil fuel combustion is calculated from international energy statistics. CO2 emissions from land-use changes are more difficult to estimate and come with greater uncertainty. Land use emissions are estimated using deforestation and other land-use data, fire observations from space and carbon cycle modeling.\nThere have been several recent studies determining the airborne fraction. Trends in the sources and sinks of carbon dioxide (Le Quere 2009) examines the airborne fraction from 1959 to 2008. This period was chosen as we have directly measured atmospheric CO2 levels over this time. Fossil fuel emissions rose steadily in recent decades, contributing 8.7 ± 0.5 gigatonnes of carbon in 2008. This is 41% greater than fossil fuel emissions in 1990. CO2 emissions from land use was estimated at 1.2 ± 0.4 gigatonnes of carbon in 2008. Note the proportionally higher uncertainty compared to fossil fuel emissions.\nOver this period, an average of 43% of each year's CO2 emissions remained in the atmosphere although there is much year-to-year variability. The noise in the airborne fraction was reduced by removing the variability associated with El Nino Southern Oscillation (ENSO) and volcanic activity. They found the airborne fraction increased by 3 ± 2% per decade. This is a slightly increasing trend although only barely statistically significant .\nKnorr 2009 extends this analysis back to 1850 by combining direct CO2 measurements from Mauna Loa and the South Pole with CO2 data derived from Antarctic ice cores. This enabled Knorr to compare CO2 emissions to atmospheric CO2 levels for the past 150 years.\nFigure 1: Observed increase atmospheric CO2 derived from direct measurements, taking the average of Mauna Loa (Hawaii) and the South Pole (thin solid line) and two ice cores: Law Dome (dashed thin line) and Siple (thin dotted line). This is compared to total anthropogenic emissions (thick solid line) and 46% of total emissions (thick dashed line). (Knorr 2009)\nKnorr finds that since 1850, the airborne fraction has eemained relatively constant. When CO2emissions were low, the amount of CO2absorbed by natural carbon sinks was correspondingly low. As human CO2 emissions sharply increased in the 20th Century, the amount absorbed by nature increased correspondingly. The airborne fraction remained level at around 43%. The trend since 1850 is found to be 0.7 ± 1.4% per decade.\nThere are several differences in methodology between Knorr 2009 and Le Quere 2009. Knorr's result does not include the filtering for ENSO and volcanic activity employed by Le Quéré. However, when Knorr does include this filtering in his analysis, he finds a trend of 1.2 ± 0.9% per decade. This is smaller than Le Quere's result but is statistically significant.\nKnorr also finds the 150 year trend while Le Quéré looks at the last 50 years. This may be significant. If the airborne fraction is increasing, it is possibly a recent phenomenon due to natural carbon sinks losing their absorption ability after becoming saturated. Several studies have found recent drops in the uptake of CO2 by oceans (Le Quere 2007, Schuster 2007, Park 2008). However, with such a noisy signal, this is one question that will require more data before being more fully resolved.\nLastly, some perspective. There are still areas of uncertainty associated with the carbon cycle. Because of this uncertainty, scientists are currently debating whether the airborne fraction is steady at 43% or slightly Increasing from 43%. Unfortunately, some skeptics use this uncertainty to hold the position that the airborne fraction is closer to 0%.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nAdditional videos from the MOOC\nAndy Skuce: The CO2 rise is man man-made\nInterviews with various experts\nExpert interview with Corinne Le Quéré\nLast updated on 8 July 2015 by pattimer. View Archives"
  },
  {
   "title": "CO2 is not the only driver of climate",
   "paragraph": "CO2 is main driver of climate change\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nTheory, models and direct measurement confirm CO2 is currently the main driver of climate change.\nClimate Myth...\nCO2 is not the only driver of climate\nCO2 is not the only driver of climate. There are a myriad of other radiative forcings that affect the planet's energy imbalance. Volcanoes, solar variations, clouds, methane, aerosols - these all change the way energy enters and/or leaves our climate.\nNatural processes have determined Earth’s climatic history, but human industrial activities have introduced a new mechanism that is driving Earth’s climate future.\nAt any given time, the Earth’s climate is subjected to a myriad of natural influences. The impact of each influence varies based on the magnitude of the natural change, the duration over which the change occurs, and whether or not that change is part of an overall repeated cycle.\nProcesses that have historically altered the face of the planet, like cycles in the Earth’s orbit around the Sun or shifts in continental tectonic plates, occur over tens of thousands to millions of years. While not nearly as dramatic, the influence of solar, ocean, and wind patterns is much more immediate, but these effects generally alternate between warming and cooling over the course of months to decades in relation to their respective cycles. Volcanic eruptions and impacts from celestial bodies, like asteroids, have a near instantaneous effect, but very few of these one-time events are of sufficient size to impact the global climate for more than a few years.\nThe industrial contribution of CO2 and other greenhouse gases to the atmosphere differs from its natural counterparts in fundamental ways. This human influence is happening very rapidly, is not cyclical, and pushes the climate continually and relentlessly in the single direction of warming.\nAll of these influences, along with additional factors like land use changes, carbon soot and halocarbon emissions, and albedo variations, must be considered cumulatively to determine the net impact.\nOver the last 30 years of direct satellite observation of the Earth’s climate, many natural influences including orbital variations, solar and volcanic activity, and oceanic conditions like El Nino (ENSO) and the Pacific Decadal Oscillation (PDO) have either had no effect or promoted cooling conditions.\nDespite these natural oppositions, global temperatures have steadily risen throughout that time.\nWhile natural processes continue to introduce short term variability, the unremitting rise of CO2 from industrial activities has become the dominant factor in determining our planet’s climate now and in the years to come.\nBasic rebuttal written by Michael Searcy\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 15 July 2015 by pattimer. View Archives"
  },
  {
   "title": "CO2 is plant food",
   "paragraph": "Plants cannot live on CO2 alone\nLink to this page\nWhat the science says...\nSelect a level... Basic Advanced\nMore Carbon Dioxide in the atmosphere is not necessarily good for plants.\nClimate Myth...\nCO2 is plant food\nEarth's current atmospheric CO2 concentration is almost 390 parts per million (ppm). Adding another 300 ppm of CO2 to the air has been shown by literally thousands of experiments to greatly increase the growth or biomass production of nearly all plants. This growth stimulation occurs because CO2 is one of the two raw materials (the other being water) that are required for photosynthesis. Hence, CO2 is actually the \"food\" that sustains essentially all plants on the face of the earth, as well as those in the sea. And the more CO2 they \"eat\" (absorb from the air or water), the bigger and better they grow. (source: Plants Need CO2)\nAn argument made by those who prefer to see a bright side to climate change is that carbon dioxide (CO2) being released by the burning of fossil fuels is actually good for the environment. This conjecture is based on simple and appealing logic: if plants need CO2 for their growth, then more of it should be better. We should expect our crops to become more abundant and our flowers to grow taller and bloom brighter.\nHowever, this \"more is better\" philosophy is not the way things work in the real world. There is an old saying, \"Too much of a good thing can be a bad thing.\" For example, if a doctor tells you to take one pill of a certain medicine, it does not follow that taking four is likely to heal you four times faster or make you four times better. It's more likely to make you sick.\nIt is possible to boost growth of some plants with extra CO2, under controlled conditions inside of greenhouses. Based on this, 'skeptics' make their claims of benefical botanical effects in the world at large. Such claims fail to take into account that increasing the availability of one substance that plants need requires other supply changes for benefits to accrue. It also fails to take into account that a warmer earth will see an increase in deserts and other arid lands, reducing the area available for crops.\nPlants cannot live on CO2 alone; a complete plant metabolism depends on a number of elements. It is a simple task to increase water and fertilizer and protect against insects in an enclosed greenhouse but what about doing it in the open air, throughout the entire Earth? Just as increasing the amount of starch alone in a person's diet won't lead to a more robust and healthier person, for plants additional CO2 by itself cannot make up for deficiencies of other compounds and elements.\nWhat would be the effects of an increase of CO2 on agriculture and plant growth in general?\n1. CO2 enhanced plants will need extra water both to maintain their larger growth as well as to compensate for greater moisture evaporation as the heat increases. Where will it come from? In many places rainwater is not sufficient for current agriculture and the aquifers they rely on are running dry throughout the Earth (1, 2).\nOn the other hand, as predicted by climate research, we are experiencing more intense storms with increased rainfall rates throughout much of the world. One would think that this should be good for agriculture. Unfortunately when rain falls in short, intense bursts it does not have time to soak into the ground. Instead, it quickly floods into creeks, then rivers, and finally out into the ocean, often carrying away large amounts of soil and fertilizer.\n2. Unlike Nature, our way of agriculture does not self-fertilize by recycling all dead plants, animals and their waste. Instead we have to constantly add artificial fertilizers produced by energy-intensive processes mostly fed by hydrocarbons, particularly from natural gas which will eventually be depleted. Increasing the need for such fertilizer competes for supplies of natural gas and oil, creating competition between other needs and the manufacture of fertilizer. This ultimately drives up the price of food.\n3. Too high a concentration of CO2 causes a reduction of photosynthesis in certain of plants. There is also evidence from the past of major damage to a wide variety of plants species from a sudden rise in CO2 (See illustrations below). Higher concentrations of CO2 also reduce the nutritional quality of some staples, such as wheat.\n4. As is confirmed by long-term experiments, plants with exhorbitant supplies of CO2 run up against limited availability of other nutrients. These long term projects show that while some plants exhibit a brief and promising burst of growth upon initial exposure to C02, effects such as the \"nitrogen plateau\" soon truncate this benefit\n5. Plants raised with enhanced CO2 supplies and strictly isolated from insects behave differently than if the same approach is tried in an otherwise natural setting. For example, when the growth of soybeans is boosted out in the open this creates changes in plant chemistry that makes these specimens more vulnerable to insects, as the illustration below shows.\nFigure 1: Plant defenses go down as carbon dioxide levels go up, the researchers found. Soybeans grown at elevated CO2 levels attract many more adult Japanese beetles than plants grown at current atmospheric carbon dioxide levels. Science Daily; March 25, 2008. (Credit: Photo courtesy of Evan Delucia)\nFigure 2: More than 55 million years ago, the Earth experienced a rapid jump in global Carbon Dioxide levels that raised temperatures across the planet. Now, researchers studying plants from that time have found that the rising temperatures may have boosted the foraging of insects. As modern temperatures continue to rise, the researchers believe the planet could see increasing crop damage and forest devastation. Science Daily; Feb. 15, 2008.\nFigure 3: Global Warming reduces plant productivity. As Carbon Dioxide increases, vegetation in Northern Latitudes also increases. However, this does not compensate for decreases of vegetation in Southern Latitudes. The overall amount of vegetation worldwide declines\n6. Likely the worst problem is that increasing CO2 will increase temperatures throughout the Earth. This will make deserts and other types of dry land grow. While deserts increase in size, other eco-zones, whether tropical, forest or grassland will try to migrate towards the poles. Unfortunately it does not follow that soil conditions will necessarily favor their growth even at optimum temperatures.\nIn conclusion, it would be reckless to keep adding CO2 to the atmosphere. Assuming there are any positive impacts on agriculture in the short term, they will be overwhelmed by the negative impacts of climate change.\nAdded CO2 will likely shrink the range available to plants while increasing the size of deserts. It will also increase the requirements for water and soil fertility as well as plant damage from insects.\nIncreasing CO2 levels would only be beneficial inside of highly controlled, enclosed spaces like greenhouses.\nBasic rebuttal written by doug_bostrom\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "CO2 lags temperature",
   "paragraph": "CO2 lags temperature - what does it mean?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nCO2 didn't initiate warming from past ice ages but it did amplify the warming. In fact, about 90% of the global warming followed the CO2 increase.\nClimate Myth...\nCO2 lags temperature\n\"An article in Science magazine illustrated that a rise in carbon dioxide did not precede a rise in temperatures, but actually lagged behind temperature rises by 200 to 1000 years. A rise in carbon dioxide levels could not have caused a rise in temperature if it followed the temperature.\" (Joe Barton, US House of Representatives (Texas) 1985-2019) - Full Statement\nEarth’s climate has varied widely over its history, from ice ages characterised by large ice sheets covering many land areas, to warm periods with no ice at the poles. Several factors have affected past climate change, including solar variability, volcanic activity and changes in the composition of the atmosphere. Data from Antarctic ice cores reveals an interesting story for the past 400,000 years. During this period, CO2 and temperatures are closely correlated, which means they rise and fall together. However, based on Antarctic ice core data, changes in CO2 follow changes in temperatures by about 600 to 1000 years, as illustrated in Figure 1 below. This has led some to conclude that CO2 simply cannot be responsible for current global warming.\nFigure 1: Vostok ice core records for carbon dioxide concentration and temperature change.\nThis statement does not tell the whole story. The initial changes in temperature during this period are explained by changes in the Earth’s orbit around the sun, which affects the amount of seasonal sunlight reaching the Earth’s surface. In the case of warming, the lag between temperature and CO2 is explained as follows: as ocean temperatures rise, oceans release CO2 into the atmosphere. In turn, this release amplifies the warming trend, leading to yet more CO2 being released. In other words, increasing CO2 levels become both the cause and effect of further warming. This positive feedback is necessary to trigger the shifts between glacials and interglacials as the effect of orbital changes is too weak to cause such variation. Additional positive feedbacks which play an important role in this process include other greenhouse gases, and changes in ice sheet cover and vegetation patterns.\nA 2012 study by Shakun et al. looked at temperature changes 20,000 years ago (the last glacial-interglacial transition) from around the world and added more detail to our understanding of the CO2-temperature change relationship. They found that:\nThe Earth's orbital cycles triggered warming in the Arctic approximately 19,000 years ago, causing large amounts of ice to melt, flooding the oceans with fresh water.\nThis influx of fresh water then disrupted ocean current circulation, in turn causing a seesawing of heat between the hemispheres.\nThe Southern Hemisphere and its oceans warmed first, starting about 18,000 years ago. As the Southern Ocean warms, the solubility of CO2 in water falls. This causes the oceans to give up more CO2, releasing it into the atmosphere.\nWhile the orbital cycles triggered the initial warming, overall, more than 90% of the glacial-interglacial warming occured after that atmospheric CO2 increase (Figure 2).\nFigure 2: Average global temperature (blue), Antarctic temperature (red), and atmospheric CO2 concentration (yellow dots). Source.\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 January 2016 by pattimer. View Archives"
  },
  {
   "title": "CO2 limits will harm the economy",
   "paragraph": "The economic impacts of carbon pricing\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nThe costs of inaction far outweigh the costs of mitigation.\nClimate Myth...\nCO2 limits will harm the economy\n\"Legally mandated measures for reducing greenhouse gas emissions are likely to have significant adverse impacts on GDP growth of developing countries [...] This in turn will have serious implications for our poverty alleviation programs.\" (Pradipto Ghosh)\nIf climate change proceeds without any efforts to reduce it, we can expect to incur serious economic costs. In fact, it's not unreasonable to expect that the effects of climate change will create greater economic instability worldwide.The solution is, of course, to reduce fossil fuel use. One way to do this is to shift away from fossil fuels towards renewable energy sources. The other way is to reduce energy demands through increased efficiency.\nBoth mechanisms have economic implications. In order to stimulate the private sector’s investment in renewables, governments can put a levy on fuels, which may be used to fund or subsidise new initiatives.\nTo reduce demand, there are a number of solutions available, but most seek to raise the cost of carbon through taxes. Such increased costs give rise to concerns that change underwritten by taxes or levies will damage economic prospects, particularly in developing countries. However, there is a consensus among economists with expertise in climate that we should put a price on carbon emissions.\n2015 New York University survey results of economists with climate expertise when asked under what circumstances the USA should reduce its emissions\nThe Representative Picture\nIn the Fifth IPCC Assessment Report (AR5), a new set of scenarios called Representative Concentration Pathways (RCP) will be used. The four RCPs replace the previous scenarios from the \"Special Report on Emissions Scenarios\" (SRES). Each RCP represents a set of initial conditions and projections to year 2100, based on a synthesis of the peer-reviewed literature.\nThe graphs below show the predicted RCP trajectories for economic performance:\nGDP projections of the four scenarios underlying the RCPs (van Vuuren et.al. 2011). Grey area for income indicates the 98th and 90th percentiles (light/dark grey) of the IPCC AR4 database (Hanaoka et al. 2006). The dotted lines indicate four of the SRES marker scenarios.\nThe number of each RCP is the forcing (in watts per square metre) associated with a specific amount of emissions for each scenario, up to the year 2100. The graph of GDP clearly shows that the pathways that reduce emissions the most in that time frame (2.6 - green, and 4.5 - red) are those with the best long-term economic performance. In other words, the investment required to reduce emissions is repaid by increased economic performance. Business as usual strategies (high-emission scenarios RCP 6 and 8.5) are the least profitable; the money saved early on is dwarfed by the costs of damage and disruption done in the longer term.\nPutting a Price on Carbon\nThere are a number of schemes under consideration, and a number already implemented. According to the article Pollution Economics in the New York Times, more than 20 percent of global greenhouse gas emissions are now subject to carbon pricing systems. About 60 other states, provinces or countries are considering similar approaches, according to a recent World Bank report.\nIt’s too early to judge long-term economic performance of the early adopters, but Canada’s province of British Columbia serves as a good example of how carbon pricing can reduce fuel use - in their case through a revenue-neutral scheme. A recent study found that since 1st July 2008, when the tax was introduced:\nBC’s fuel consumption has fallen by 17.4% per capita (and fallen by 18.8% relative to the rest of Canada).\nThese reductions have occurred across all the fuel types covered by the tax (not just vehicle fuel)\nBC’s GDP kept pace with the rest of Canada’s over that time\nThe tax shift has enabled BC to have Canada’s lowest income tax rates (as of 2012).\nThe tax shift has benefited taxpayers; cuts to income and other taxes have exceeded carbon tax revenues by $500 million from 2008-12.\nSource: BC’s Carbon Tax Shift After Five Years: Results, Elgie & McClay 2013\nIn a separate report, the British Columbia Department of Finance found that in 2012, BC's taxes were among the lowest corporate tax rates in North America and the G7 nations.\nConclusions\nThere is a consensus among expert climate economists that carbon pollution limits are needed to prevent climate change from badly damaging the global economy.\nA number of economic incentives are being tried with varying degrees of success. Regional schemes are already proving effective, flexible and popular. An important ingredient seems to be an accompanying tax reduction that makes the carbon tax revenue-neutral.\nIn the long term, unless we drastically reduce the rate at which we are still emitting greenhouse gases, we are very likely to incur huge costs as a result of climate change. Part of these costs will be in adaptation, and the inevitable disruption. In part costs will escalate due to turmoil and uncertainty throughout the economic world. There will also be costs that cannot be quantified, particularly when we try to value a human life and its loss.\nWe have to reduce our emissions. If we are to avoid draconian government intervention, carbon pricing schemes are a viable method of encouraging us to reduce fossil fuel use. Coupled with other measures to stimulate renewable energy development, putting a price on carbon may help us make the transition away from fossil fuels. And from our experience to date, it seems likely that carbon taxes, instead of bringing an economy to its knees, may well help transform an outdated system into one fitting for a sustainable century.\nBasic Rebuttal written by GPWayne and dana1981\nFurther Reading: The Intermediate and Advanced rebuttals contain detailed information about carbon pricing and tax schemes. Skeptical Science contributor Andy Skuce has also written an article about British Columbia’s experience here, with an update here describing the findings of the Elgie & McClay paper.\nLast updated on 2 January 2016 by dana1981. View Archives"
  },
  {
   "title": "CO2 limits will hurt the poor",
   "paragraph": "How are the poor impacted by climate change?\nLink to this page\nWhat the science says...\nThose who contribute the least greenhouse gases will be most impacted by climate change.\nClimate Myth...\nCO2 limits will hurt the poor\n\"Legally mandated measures for reducing greenhouse gas emissions are likely to have significant adverse impacts on GDP growth of developing countries, including India.\" (Pradipto Ghosh, as quoted by Associated Press)\nThe central question of climate change is, How will it affect humanity? This question can be examined by estimating which regions are most vulnerable to future climate change (Samson et al 2011). The researchers then compared the global map of climate vulnerability to a global map of carbon dioxide emissions. The disturbing finding was that the countries that have contributed the least to carbon dioxide emissions are the same regions that will be most affected by the impacts of climate change.\nTo estimate the impact of climate change on people, James Samson and his co-authors developed a new metric called Climate Demography Vulnerability Index (CDVI). This takes into account how regional climate will change as well as how much local population is expected to grow. They incorporated this index into a global map and found highly vulnerable regions included central South America, the Middle East and both eastern and southern Africa. Less vulnerable regions were largely in the northern part of the Northern Hemisphere.\nFigure 1: Global Climate Demography Vulnerability Index. Red corresponds to more vulnerable regions, blue to less vulnerable regions. White areas corresponds to regions with little or no population (Samson et al 2011).\nNext, they created a map of national carbon dioxide emissions per capita. They found the countries most severely impacted by climate change contributed the least to greenhouse gas emissions. It is quite striking that blue, less-polluting regions in the CO2 emissions map correspond to the red, highly vulnerable areas in the vulnerability map.\nFigure 2: National average per capita CO2 emissions based on OECD/IEA 2006 national CO2 emissions (OECD/IEA, 2008) and UNPD 2006 national population size (UNPD, 2007).\nThe study didn't delve into the question of which countries are least able to adapt to the impacts of climate change. But it doesn't take a great leap of the imagination to surmise that the poor, developing countries that emit the least pollution are also those with the least amount of infrastructure to deal with climate impacts. So we are left with a double irony - the countries that contribute least to global warming are both the most impacted and the least able to adapt.\nThis research put into perspective those who try to delay climate action, arguing that \"CO2 limits will hurt the poor\". This argument is usually code for \"rich, developed countries should be able to pollute as much as they like\". This presents us with a moral hazard. If those who are emitting the most greenhouse gas are the least affected by direct global warming impacts, how shall we motivate them to change?\nBasic rebuttal written by John Cook\nUpdate August 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 August 2015 by MichaelK. View Archives"
  },
  {
   "title": "CO2 limits will make little difference",
   "paragraph": "Meaningful climate action needs global cooperation\nLink to this page\nWhat the science says...\nWhile it's true that any single country's CO2 emissions reductions will make little difference, only if every nation agrees to limit CO2 emissions can we achieve significant cuts on a global scale.\nClimate Myth...\nCO2 limits will make little difference\n\"Cap and tax is as pointless as it is cruel. Australia accounts for 1.5 per cent of global carbon emissions. So if it cut its emissions, the warming forestalled would be infinitesimal.\" (Christopher Monckton)\nSome skeptics have claimed that anything more than a modest greenhouse gas emissions reduction would shut a nation's economy down. However, as we previously explored, this claim is not even remotely true. In fact, we showed that the benefits of carbon pricing would outweigh the costs several times over, even in the legislation proposed in the USA which would have cut the country's emissions 80% by 2050.\nAustralian Example\nIf Australia were to cut its emissions at a constant rate to get to 80% lower emissions by 2050, then it would have emitted 40% less CO2 by 2050 than it would have done at today's rates. In a business-as-usual scenario, the atmospheric CO2 concentration in 2050 will be approximately 550 parts per million by volume (ppmv). Australian CO2 emissions are approximately 1.5% of global emissions, so if the country were to maintain this percentage until 2050, Australia would be responsible for 1.5% of the 160 ppmv increase during that period, or 2.4 ppmv. If Australia were to cut its emissions by an average of 40% over that period, the difference in atmospheric CO2 concentration would be approximately 1 ppmv.\nSo the skeptics seemingly have a point here. CO2 emissions cuts from Australia, by itself, would have an insignificant effect on global CO2 concentrations and temperature. However, this is a perfect example of what's known as the Tragedy of the Commons.\nTragedy of the Commons\nThe Tragedy of the Commons was first described by Hardin (1968). It's \"a dilemma arising from the situation in which multiple individuals, acting independently and rationally consulting their own self-interest, will ultimately deplete a shared limited resource even when it is clear that it is not in anyone's long-term interest for this to happen.\"\nThe global climate is effectively a shared natural resource. If every nation decides to continue emitting CO2 unabated in their own self-interest, the consequent climate change will be bad for almost everyone.\nGame Theory\nThe concept of Nash equilibrium in game theory provides an analogous scenario. In our example we'll consider the USA and Australia, each with $10. Reducing carbon emissions will cost either country $3. The consequences of global warming will cost each country $7 if no action is taken, and $4 if only one takes action. The potential resulting outcomes look like this (remaining funds for USA in blue, and for Australia in red):\nAustralia\nEmissions Reduced?\nYes\nNo\nUSA\nYes\n7, 7\n3, 6\nNo\n6, 3\n3, 3\nEither side can only tie or win if they don't reduce emissions, and they can only tie or lose if they do reduce emissions. Thus it seems to be in each country's best interest not to reduce emissions. But the best overall outcome is if both sides reduce their emissions, in which case the net economic impact is smallest. If each country looks out only for its own best interest, the overall economic impact is largest.\nIt's quite a good analogy to carbon pricing. A frequent argument used by politicians in most countries is \"if our country introduces carbon pricing, businesses will just move to another country where they can emit carbon for free\".\nSo how do you get both sides to reduce their emissions even though it seems to be in the best interest of neither? Collusion.\nInternational Climate Conferences\nThis is the purpose of international climate conferences such as those held at Kyoto and Copenhagen. Every nation can make the argument that their emissions cuts alone will have an insignificant impact on global temperatures. We've heard the exact same argument in the USA, despite our much larger overall emissions than Australia.\nBut if all nations can come together and agree to reduce CO2 emissions in their own best interests, then the combined emissions reductions and impact on global temperatures can be significant. But to achieve the necessary global emissions reductions to avoid dangerous global warming, we need all countries on board.\nLast updated on 1 March 2011 by dana1981."
  },
  {
   "title": "CO2 limits won't cool the planet",
   "paragraph": "Climate choice: stable or disrupted climate\nLink to this page\nWhat the science says...\nContinued greenhouse gas emissions at or above current rates would cause further warming and induce many changes in the global climate system during the 21st century that would very likely be larger than those observed during the 20th century. (source: IPCC 2007)\nClimate Myth...\nCO2 limits won't cool the planet\n\"[CO2 limits] will not make a difference for 1000 years. So this is a government which is proposing to put at risk our manufacturing industry, to penalise struggling families, to make a tough situation worse for millions of households right around Australia. And for what? To make not a scrap of difference to the environment any time in the next 1000 years.\" (Tony Abbott)\nSkeptics have argued that if reductions in CO2 will not cool the planet for hundreds of years, then it is not prudent to cut emissions and put any burden on a fossil-fuel-driven economy. But does this make sense?\nA choice between sustained temperature increase and doing nothing is not, unfortunately, the choice we are facing. The real choice we face is between decreasing CO2 emissions (in which case temperatures will still warm a bit more and then stabilize), and letting CO2 emissions go and and on (in which case temperatures will continue to rise and rise). In the future, when technologically and economically feasible, it may be possible to withdraw carbon from the atmosphere, perhaps with increasing tree growth or chemical \"scrubbing\". But this is just a goal for the future.\nAccording the IPCC, from the Summary for Policy Makers:\nContinued greenhouse gas emissions at or above current rates would cause further warming and induce many changes in the global climate system during the 21st century that would very likely be larger than those observed during the 20th century.\nAnthropogenic warming and sea level rise would continue for centuries due to the time scales associated with climate processes and feedbacks, even if greenhouse gas concentrations were to be stabilised.\nFigure: Global surface temperature projections for IPCC Scenarios. Shading denotes the ±1 standard deviation range of individual model annual averages. The orange line is constant CO2 concentrations at year 2000 values. The grey bars at right indicate the best estimate (solid line within each bar) and the likely range. (Source: IPCC). Emissions scenarios\nThis is one skeptic argument where one must wonder if some of those who wish to prevent action on climate change really understand what the argument is about.\nBasic rebuttal written by grypo\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 19 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "CO2 only causes 35% of global warming",
   "paragraph": "How CO2 warming is driving climate\nLink to this page\nWhat the science says...\nThe Nature commentary by Penner et al. on which this argument is based actually says that on top of the global warming caused by carbon dioxide, other short-lived pollutants (such as methane and black carbon) cause an additional warming approximately 65% as much as CO2, and other short-lived pollutants (such as aerosols) also cause some cooling. However, claiming that CO2 has only caused 35% of global warming is a gross misinterpretation and misunderstanding of the paper.\nClimate Myth...\nCO2 only causes 35% of global warming\nCO2 does not account for even a majority of the warming seen over the past century. If other species accounted for 65% of historical warming that leaves only 35% for carbon dioxide. (Doug Hoffman)\nIn August 2010, Nature published a commentary by Penner et al. which mainly focused on the uncertainty regarding the effect short-lived pollutants (such as aerosols and black carbon) have on the climate. As is often the case, many in the blogosphere misinterpreted and misunderstood the statements and conclusions in the commentary. Not surprisingly, the biggest misinterpretation related to the contribution of anthropogenic greenhouse gases to global warming. Below is the most misunderstood quote, with emphasis on the key word.\n\"Of the short-lived species, methane, tropospheric ozone and black carbon are key contributors to global warming, augmenting the radiative forcing of carbon dioxide by 65%. Others — such as sulphate, nitrate and organic aerosols — cause a negative radiative forcing, offsetting a fraction of the warming owing to carbon dioxide.\"\nNumerous blogs have (mis)interpreted this statement to mean that carbon dioxide is only causing 35% as much global warming as previously believed. A more accurate reading of the quote is that certain short-lived pollutants cause warming in addition to carbon dioxide - quantitatively, approximately 65% as much warming as CO2. And certain other short-lived species cause a cooling effect which offsets some of this warming.\nThis is not a new conclusion. The IPCC puts the radiative forcing from CO2 at 1.66 W/m2, compared to the forcing from other greenhouse gases, black carbon, and tropospheric ozone at approximately 1.4 W/m2. Similarly, the negative forcing from aerosols is approximately -1.2 W/m2.\nFigure 1: Radiative forcing estimates from the IPCC FAR\nThus if anything, the 65% figure is an underestimate of the contributions of short-lived pollutants to global warming, but this contribution does not change the 1.66 W/m2 radiative forcing from CO2 or the amount of global warming it has caused.\nMuch ado has also been made about another quote from the commentary:\n\"Warming over the past 100 years is consistent with high climate sensitivity to atmospheric carbon dioxide combined with a large cooling effect from short-lived aerosol pollutants, but it could equally be attributed to a low climate sensitivity coupled with a small effect from aerosols. These two possibilities lead to very different projections for future climate change.\"\nThis statement gets to the main point of the commentary - that there remains significant uncertainty regarding the effect of these short-lived pollutants on the global climate. However, estimates of the planetary climate sensitivity to increasing atmospheric CO2 and other radiative forcings are not solely based on the change in the mean global temperature over the past 100 years. In fact, the climate sensitivity parameter has been estimated through many different methods, including:\nclimate models\nrecent responses to large volcanic eruptions\nrecent responses to solar cycles\npaleoclimate data\ndata from the last Glacial Maximum\nand yes, data from the instrumental period\nAll of these different methods show strong agreement, overlapping in the IPCC climate sensitivity range of 2 to 4.5°C for a doubling of atmospheric CO2 (2xCO2).\nFigure 2: Distributions and ranges for climate sensitivity from different lines of evidence. The circle indicates the most likely value. The thin colored bars indicate very likely value (more than 90% probability). The thicker colored bars indicate likely values (more than 66% probability). Dashed lines indicate no robust constraint on an upper bound. The IPCC likely range (2 to 4.5°C) and most likely value (3°C) are indicated by the vertical grey bar and black line, respectively (Knutti and Hegerl 2008)\nInterestingly, Penner et al. find that whether the climate sensitivity parameter is on the low or high end, reducing anthropogenic emissions of the short-lived warming pollutants would achieve a significant reduction in global warming over the next 50-100 years. In the red lines in the Figure 3, they employ a climate model with a sensitivity of 5°C for 2xCO2, slightly outside the IPCC likely range. The blue line is a climate model with a sensitivity of 2°C for 2xCO2, on the lower end of the IPCC range. Note that even with the lower climate sensitivity, the model shows the planet warming 3°C by 2100 in this emissions scenario (see the figure caption for further details).\nFigure 3: Global mean temperature measurements (black) and projections based on an IPCC scenario with high emissions (A2) for a climate sensitivity parameter of 5°C (upper red) and 2°C (upper blue). Linearly decreasing the total anthropogenic radiative forcing owing to methane, tropospheric ozone and black carbon — starting in 2010 and achieving pre-industrial levels by 2050 — results in significant near-term climate mitigation (lower blue and red curves) (Penner 2010)\nUnfortunately, reducing the short-lived cooling pollutants such as aerosols would cause a warming effect of similar magnitude, and so CO2 remains the primary pollutant of concern. Coincidentally, a group of scientists from NASA GISS just published a paper in Science entitled Atmospheric CO2: Principle Control Knob Governing Earth's Temperature.\nAlthough it is important to reduce the remaining climate uncertainties, such as the magnitude of the impacts of short-lived pollutants, it does not change the fact that CO2 is very likely the driving force behind the current global warming, or that if we double the amount of CO2 in the atmosphere from pre-industrial levels, the planet will likely warm in the range of 2 to 4.5°C.\nLast updated on 13 August 2015 by dana1981. View Archives"
  },
  {
   "title": "CO2 was higher in the late Ordovician",
   "paragraph": "Low solar levels during Ordovician Period\nLink to this page\nWhat the science says...\nDuring the Ordovician, solar output was 4% lower than current levels, and there was a large continent over the South Pole. Consequently, CO2 levels at around 1,000 to 2,300 ppm were actually low enough to promote glaciation in the southern continent of Gondwana. Ample geological and geochemical evidence points to strong weathering in parallel with the cooling of the Ordovician climate. Since rock weathering reduces atmospheric CO2, this again reinforces the scientific fact that CO2 is a strong driver of climate.\nClimate Myth...\nCO2 was higher in the late Ordovician\n\"To the consternation of global warming proponents, the Late Ordovician Period was also an Ice Age while at the same time CO2 concentrations then were nearly 12 times higher than today - 4400 ppm. According to greenhouse theory, Earth should have been exceedingly hot. Instead, global temperatures were no warmer than today. Clearly, other factors besides atmospheric carbon influence earth temperatures and global warming.\" (Monte Hieb)\nOlder scientific papers inferred very high CO2 levels in the Ordovician, generating a paradox of a cold climate during a time of high greenhouse gas levels. But recent work has shown that atmospheric CO2 was much lower than the myth claims, and it kept falling through the Ordovician. It was less than 8 times preindustrial values towards the end (see the graph below), which may sound very high, but with a 4% fainter sun back then and with a large continent over the South Pole, it was low enough to trigger a major continental ice sheet.\nThe Ordovician was a time of mountain building (the Taconic/Caledonian orogeny) and violent ashy volcanic eruptions as the continents of Laurentia, Baltica and Avalonia began to collide. Mountain building, lots of fresh volcanic ash and erosion tend to accelerate the weathering of silicate rocks, which draws down CO2 from the atmosphere, cooling the planet on a timeframe of hundreds of thousands to millions of years. And indeed, strontium isotopes confirm a large increase in the contribution of weathered volcanic rocks into ocean waters between about 470 and 450 million years ago. Neodymium isotopes (a proxy for ancient sea level change) show that ice sheets were in place in the late Ordovician.\nThe latter half of the Ordovician also saw the development of Earth's earliest plant-dominated terrestrial biosphere. Those early moss-like plants accelerated rock weathering rates, simultaneously drawing down CO2 and supplying nutrients like phosphorous to the oceans, which fertilized plankton activity, which further reduced CO2 as their carbon-rich remains sank to the sea bed. The climate cooled so much that it crossed a \"tipping point\" 444 million years ago, triggering the Hirnantian Glaciation, which was so severe it resulted in one of the biggest mass extinctions since animals first evolved. For more on that see this article.\nSo, far from presenting a paradox, late Ordovician CO2 levels are entirely consistent with a cool climate and glaciation. Moreover the geological, geochemical and fossil evidence all consistently show that a big drawdown of CO2 drove that cooling, proving again that CO2 is the principle control knob on climates both ancient and modern.\nCooling climate before the Hirnantian Mass Extinction. Cyan horizontal band is the Hirnantian Stage. Redrawn from Armstrong & Harper 2014. 13C-derived CO2 range from Pancost et al 2013, Plant spore first appearance simplified from Edwards et al 2014 and Rubinstein et al 2010.\nIntermediate rebuttal written by howardlee\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 23 October 2015 by pattimer. View Archives"
  },
  {
   "title": "CO2 was higher in the past",
   "paragraph": "Do high levels of CO2 in the past contradict the warming effect of CO2?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe Ordovician glaciation was a brief excursion to coldness during an otherwise warm era, due to a coincidence of conditions. It is completely consistent with climate science.\nClimate Myth...\nCO2 was higher in the past\n\"The killer proof that CO2 does not drive climate is to be found during the Ordovician- Silurian and the Jurassic-Cretaceous periods when CO2 levels were greater than 4000 ppmv (parts per million by volume) and about 2000 ppmv respectively. If the IPCC theory is correct there should have been runaway greenhouse induced global warming during these periods but instead there was glaciation.\"\n(The Lavoisier Group)\nGeologists refer to ancient ice-cap formations and ice-ages as \"glaciations.\" One such glaciation that occurred during the Late Ordovician era, some 444 million years ago has captured the attention of climate scientists and skeptics alike. To get some perspective on timing, that's just over 200 million years before dinosaurs began to roam the Earth.\nUnlike other glaciations in the last 500 million years, this one was exceptionally brief (lasting perhaps only a million years or so) but the main reason for generating so much interest recently is because it took place when CO2 levels were apparently sky-high. As Ian Plimer notes in his book, \"Heaven and Earth\", pp165:\n\"The proof that CO2 does not drive climate is shown by previous glaciations...If the popular catastrophist view is accepted, then there should have been a runaway greenhouse when CO2 was more than 4000 ppmv. Instead there was glaciation. Clearly a high atmospheric CO2 does not drive global warming and there is no correlation between global temperature and atmospheric CO2.\"\nOn the surface, Plimer does seem to have a point: if ice-caps managed to exist back then in an ultra-high CO2 environment, why are the vast majority of climate scientists worrying so much about keeping CO2 levels piddlingly low?\nTo answer this, we have to fill in some parts of the puzzle that are missing. Let's start with the CO2.\nPlimer's stated value of 4000 ppmv or greater is taken from Robert Berner's GEOCARB, a well-known geochemical model of ancient CO2. As the Ordovician was so long ago, there are huge uncertainties for that time period (according to the model, CO2 was between an incredible 2400 and 9000 ppmv.) Crucially, GEOCARB has a 10 million year timestep, leading Berner to explicitly advise against using his model to estimate Late Ordovician CO2 levels due its inability to account for short-term CO2 fluctuations. He noted that \"exact values of CO2... should not be taken literally.\"\nWhat about evidence for any of these short-term CO2 fluctuations? Recent research has uncovered evidence for lower ocean temperatures during the Ordovician than previously thought, creating ideal conditions for a huge spurt in marine biodiversity and correspondingly large drawdown of CO2 from the atmosphere through carbon burial in the ocean. A period of mountain-building was also underway (the so-called Taconic orogeny) increasing the amount of rock weathering taking place and subsequently lowering CO2 levels even further. The evidence is definitely there for a short-term disruption of the carbon cycle.\nAnother important factor is the sun. During the Ordovician, it would have been several percent dimmer according to established nuclear models of main sequence stars. Surprisingly, this raises the CO2 threshold for glaciation to a staggering 3000 ppmv or so. This also explains (along with the logarithmic forcing effect of CO2) why a runaway greenhouse didn't occur: with a dimmer sun, high CO2 is necessary to stop the Earth freezing over.\nIn summary, we know CO2 was probably very high coming into the Late Ordovician period, however the subsequent dip in CO2 was brief enough not to register in the GEOCARB model, yet low enough (with the help of a dimmer sun) to trigger permanent ice-formation. Effectively it was a brief excursion to coldness during an otherwise warm era, due to a coincidence of conditions.\nThe following (somewhat simplified) diagram may make this easier to understand:\nWhen looking at events such as these from the deep geological past, it is vital to keep in mind that there are many uncertainties, and generally speaking, the further back we look, the more there are. As our paleo techniques improve and other discoveries emerge this story will no doubt be refined. Also, although CO2 is a key factor in controlling the climate, it would be a mistake to think it's the only factor; ignore the other elements and you'll most likely get the story wrong.\nBasic rebuttal written by steve.oconnor\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 6 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Corals are resilient to bleaching",
   "paragraph": "How global warming is driving mass coral bleaching\nLink to this page\nWhat the science says...\nOn a world scale coral reefs are in decline. Over the last 30-40 years 80% of coral in the Caribbean have been destroyed and 50% in Indonesia and the Pacific. Bleaching associated with the 1982 -1983 El-Nino killed over 95% of coral in the Galapagos Islands and the 1997-1998 El-Nino alone wiped out 16% of all coral on the planet. Globally about 1% of coral is dying out each year.\nClimate Myth...\nCorals are resilient to bleaching\n\"Three recent articles give us reason to question the alarmists’ claims that coral reefs are in deep trouble due to the buildup of greenhouse gases.\" (World Climate Report)\nDespite what you may read or see in the mainstream media, out in the real world, massive and rapid changes are taking place in many ecological systems as a result of global warming. The Earth seems to be already convinced of global warming and is responding quickly.\nPerhaps the most significant, and likely most enduring, are the shifts taking place in the Earth's oceans. Whilst many readers may have read or heard about Ocean Acidification, there are numerous other changes taking place in the oceans which should be equally as concerning. One such phenomena to appear in the last few decades is mass coral bleaching, a consequence of the continued warming of the oceans. Once vast stretches of colourful reefs teeming with marine life are being reduced to lifeless rubble covered in seaweed or slime. Many areas are not recovering, and the scale and frequency of bleaching worldwide is getting worse. In fact, early reports suggest 2010 may have witnessed the largest single bleaching event ever recorded.\nThe lowdown on coral bleaching\nReef-coral are actually a symbiosis (a mutually beneficial relationship) between the coral polyp, an anemone-like creature, and tiny algae called zooxanthellae. The coral provide shelter and nutrients for the algae , and in exchange the algae provide carbohydrates (food) to the polyp, using energy from the sun (photosynthesis) and the nutrients provided by the coral. These algae live in the skin tissue of the polyp and produce the coloured pigments which make coral reefs so visually spectacular. When this partnership breaks down the polyps expel the algae, which leads to the \"bleached\" effect. Although the polyp does feed using its tentacles to snare food, the bulk of its nutrition (90%+) comes from the algae, and they are a critical component of coral skeleton formation and therefore reef maintenance and growth. Without symbiotic algae, the coral can die from starvation, or become so weakened by a lack of food, that it succumbs to harmful bacteria (Mao-Jones 2010), and/or seaweeds which can poison and kill coral on contact.\nBecause reef-coral have adapted tolerance to a narrow band of environmental conditions, bleaching can occur for a number of reasons, such as ocean acidification, pollution, excess nutrients from run-off, high UV radiation levels, exposure at extremely low tides and cooling or warming of the waters in which the coral reside. Typically these events are very localized in scale and if bleaching is mild, the coral can survive long enough to re-acquire new algal partners. So bleaching in itself is not something new, but mass coral bleaching on the huge scale being observed certainly appears to be, and represents a whole new level of coral reef decline.\nOcean warming is driving mass coral bleaching\nAs coral reefs operate very near to their upper limit of heat tolerance (Glynn & D'Croz 1990), bleaching en masse happens when the surface waters get too warm above their normal summer temperature, and are sustained at this warmer level for too long. The intensity of bleaching corresponds with how high, and how long temperatures are elevated and, as one might expect, the intensity of bleaching affects the rate of survival. Small rises of 1 -2 degree C, for weeks at a time, usually induce bleaching.\nThis episodic ocean warming has been most pronounced worldwide during El-Nino events, when the Pacific Ocean exchanges heat to the atmosphere and surface waters. In recent years though, severe mass bleaching is happening outside of El-Nino because of the \"background\" ocean warming. The huge mass bleaching in the Caribbean in 2005, a non El-Nino year, and again this year is a prime example of this (Eakin 2010) . Evidence connecting warm surface waters and mass coral bleaching has strengthened to the extent that the National Oceanic and Atmospheric Administration (NOAA) has a coral bleaching alert system in place. This alert system accurately forecasts mass coral bleaching based on satellite data of sea surface temperatures.\nHot water + Coral = Dead coral\nSo how does hot water kill coral? It requires both high water temperatures and sunlight. Oxygen is released as waste during photosynthesis and like all chemical processes this is affected by temperature, speeding up as more energy (warmth) is applied. When water temperatures rise too high the protective mechanisms to prevent heat damage, employed by the coral and the algae, are overwhelmed. The zooxanthellae algae produce high levels of oxygen waste which begin to poison the coral polyp. In acts of self-preservation the coral kick out the algae, and in doing so become susceptible to starvation, opportunistic diseases, competitive seaweeds and macroalgae (slime to you and me) . Coral can succumb to the effects of bleaching years later, and for those coral that survive, growth effectively ceases and full recovery can take anything up to a decade.\nCoral resilience is futile\nOn a world scale coral reefs are in decline, and it makes for rather depressing reading for an avid diver like myself. Over the last 30-40 years 80% of coral in the Caribbean have been destroyed (Gardner 2003) and 50% in Indonesia and the Pacific (Bruno & Selig 2007). Bleaching associated with the 1982 -1983 El-Nino killed over 95% of coral in the Galapagos Islands (Glynn 1990), and the 1997-1998 El-Nino alone wiped out 16% of all coral on the planet. Globally about 1% of coral is dying out each year. Not all of this continual decline is solely down to bleaching of course, pollution and other human activities are also contributing, but bleaching is speeding up the loss of coral.\nLooking only at bleaching though, we find that the incidence of mass coral bleaching increases dramatically in the last few decades. Despite modern records being biased by better monitoring and reporting in recent times, there seem to be little evidence of mass coral bleaching further back in time when examining long-lived coral communities. Studies from around the world show no signs of bleaching dating back many thousands of years, until recent decades (Abram 2003), (Aronson 2003). In the Caribbean there are no signs of previous mass bleaching dating back 220,000 years (Pandolfini & Jackson 2006)\nSo where does this resilience claim originate you may ask?. Perhaps from studies that have shown some coral, in secondary bleaching events, have lower rates of death. A few coral are in more fact tolerant to bleaching, some algae for instance manufacture their own \"organic sunscreen\". However this a only small proportion, major reef-building coral species seem incapable of forming long-lasting partnerships with these heat tolerant algae (Coffroth 2010), and the coral polyp themselves have a very poor genetic ability to adapt to warming (Csaszar 2010). However the \"resilience\" fallacy arose, there's no evidence a few hardy individuals will somehow prevent the loss of most coral worldwide.\nThe importance of coral reefs - the oasis in a marine desert\nSo what does this all have to do with the average man or woman in the street?, well, as far as humans are concerned, there is a rather large dollar value attached to coral reefs. Goods and services derived from coral reefs are very roughly estimated to be between $172 to $375 billion dollars per year (Martinez 2007). Not only that, but reefs directly provide food and income to over half a billion people worldwide. The decline of coral reefs is going to not only impact those that directly depend on them for a living and sustenance, but eventually have dramatic effects on economies worldwide, and will likely drastically drive up world food prices as fish populations nosedive.\nEcologically speaking the value of coral reefs is even greater because they are integral to the well being of the oceans as we know them. It might serve to picture them as the undersea equivalent of rainforest trees. Tropical waters are naturally low in nutrients because the warm water limits nutrients essential for life from welling up from the deep, which is why they are sometimes called a \"marine desert\". Through the photosynthesis carried out by their algae, coral serve as a vital input of food into the tropical/sub-tropical marine food-chain, and assist in recycling the nutrients too. The reefs provide home and shelter to over 25% of fish in the ocean and up to two million marine species. They are also a nursery for the juvenile forms of many marine creatures .\nI could go on, but the similarity with the rainforest should now be clear. Eliminate the undersea \"trees\", which mass coral bleaching is in the process of doing, and you'll eliminate everything that depends on it for survival, a point best exemplified in the following sequence of photos. (sequence of healthy coral-bleached coral-rubble & slime)\nA grim outlook for coral\nThe critical issue with global warming induced coral bleaching, as it is for many eco-systems, is the speed of warming. They are simply not being given sufficient time to evolve tolerance. The coral's algal partners have short lifetimes and possess genetic traits which may enable successful adaptation to warming. Coral themselves aren't so lucky, somewhat in contrast to their algae, they possess a poor genetic ability to combat warming stress and have decadal lifetimes. It's likely therefore that many coral will die because the speed of warming is too great within an individual communities lifetime.\nPerhaps a useful way of looking at it, is that the \"bar\" is continually being set higher and higher, and the recovery time between bleaching events becoming smaller and smaller. Gradually this continual ocean warming will start to impact areas which have so far escaped unscathed, and these coral will succumb too. Of course coral reefs aren't just under fire from bleaching, as mentioned earlier, humans are hurting them in many other ways. Ocean Acidification in particular is a large looming threat (Veron 2009). The increasing frequency and severity of bleaching, coupled with the persistent decline in coral around the world, should however immediately dispel any myths about coral resilience.\nIntermediate rebuttal written by Rob Painting\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 26 October 2016 by pattimer. View Archives"
  },
  {
   "title": "CRU tampered with temperature data",
   "paragraph": "Did CRU tamper with temperature data?\nLink to this page\nWhat the science says...\nThe Independent Climate Change Email Review went back to primary data sources and were able to replicate CRU's results. This means not only was CRU not hiding anything, but it had nothing to hide. Though CRU neglected to provide an exact list of temperature stations, it could not have hid or tampered with data.\nClimate Myth...\nCRU tampered with temperature data\n\"So recently the media picked up on the fact that CRU deleted the raw data for this important global temperature set long ago. We have known this for some time now. The interesting point is that it also seems each time they come across a new dataset it is simply replaced. So what we have is a process which allows the systematic choosing of ever warmer records over time which is so convoluted nobody can figure out what really happened.\" (The Air Vent)\nExhibit No. 1 of the climate conspiracy theory is a collection of emails stolen from the Climatic Research Unit (CRU) of the University of East Anglia (UEA), which appeared on the internet in November 2009. Though some of these \"Climategate\" emails can sound damning when quoted out of context, several inquiries have cleared the scientists. The most comprehensive inquiry, the Independent Climate Change Email Review, did something the media completely failed to do: it put the emails into context by investigating the main allegations. Its general findings (summarised here) were that the scientists' rigour and honesty are not in doubt, and their behaviour did not prejudice the advice given to policymakers, though they did fail to display the proper degree of openness.\nOne set of allegations against CRU concern its main area of research, the instrumental temperature record CRUTEM. The CRUTEM analysis is very similar to those produced by independent groups such as NASA’s Goddard Institute for Space Studies (GISS) and NOAA’s National Climatic Data Center (NCDC). Nevertheless, the contrarians allege that CRU manipulated data to fabricate a global warming trend; that CRU prevented critics from accessing the raw data and other information required to check its conclusions; and that CRU director Phil Jones failed to admit having cited fraudulent data twenty years ago. Thus they claim CRUTEM cannot be trusted.\nTo create the CRUTEM surface temperature analysis, CRU scientists take temperature data from 4,138 stations, and for each station they calculate the mean temperature for 1961-1990 and temperature anomalies relative to that period. They then arrange all this data into a 5x5 degree grid. This process requires that adjustments be made to account for sources of error such as changing station locations or urban heat island effect.\nFollowing Climategate, several amateur climate bloggers have attempted their own analyses of global temperature trends, and arrived at very similar results to CRU, GISS, and NCDC. The Review took a similar approach, going back to primary sources and obtaining raw station data to see if it was possible for critics to replicate CRU’s results. They were able to acquire as much data as necessary from both the Global Historical Climatology Network (GHCN) and the National Centre for Atmospheric Research (NCAR). They proceeded to write the computer code needed to analyse the data in the space of two days, without requiring any information from CRU.\nThus the Review demonstrated that CRU was not hiding anything: sufficient data was available to replicate CRU’s results, and any competent researcher would be able to analyse it. Furthermore, they had nothing to hide: both adjusted and unadjusted data yielded very similar results to CRUTEM, and CRU’s homogenisation adjustments make no significant difference to the global average. Although the Review stopped short of drawing scientific conclusions, it appears that CRU’s conclusions are robust.\nBased on this, the Review concluded (its emphasis):\nCRU was not in a position to withhold access to [temperature] data or tamper with it. We demonstrated that any independent researcher can download station data directly from primary sources and undertake their own temperature trend analysis.\nOn the allegation of biased station selection and analysis, we find no evidence of bias. Our work indicates that analysis of global land temperature trends is robust to a range of station selections and to the use of adjusted or unadjusted data. The level of agreement between independent analyses is such that it is highly unlikely that CRU could have acted improperly to reach a predetermined outcome. [1.3.1]\nThis is stated more explicitly in Chapter 6:\nIt is impossible for a third party to tamper improperly with the data unless they have also been able to corrupt the GHCN and NCAR sources. We do not consider this to be a credible possibility, and in any case this would be easily detectable by comparison to the original NMO records [6.4]\nThe Review also considered the availability of metadata; that is, whether there was enough information available to identically replicate CRUTEM. As noted above, the computer code was no problem. Getting an exact list of temperature stations included in CRUTEM was more of an issue. Such a list was provided with the first version of CRUTEM in 1986, but CRU neglected to update it in the latest version, CRUTEM3, published in 2006.\nAn up-to-date list was not released until October 2007, in response to an FoI request. Even then, the Review Team found it was not straightforward to identify all the stations, due to a lack of standardisation. However, 90% could be matched with stations in the GHCN database, and CRU informed them that the remaining 10% could be obtained from other sources such as the relevant National Meteorological Office. As a “test case”, the Review did obtain data directly from the Japanese NMO.\nThe Review makes the following criticism of CRU:\nCRU should have made available an unambiguous list of the stations used in each of the versions of [CRUTEM] at the time of publication. We find that CRU’s responses to reasonable requests for information were unhelpful and defensive. [1.3.1]\nThe inquiry also briefly dealt with the allegation “that Jones was complicit in malpractice in failing to respond appropriately to allegations of fraud made against […] Professor Wei-Chyung Wang”, whose data Jones cited in a 1990 paper on the urban heat island effect. The allegedly “fabricated” claim was that few if any of a certain selection of Chinese weather stations had moved over time. Wang’s university investigated and rejected the accusation of fraud. Meanwhile, Jones responded within one year with a peer-reviewed analysis confirming the original conclusions. In any case, this was only one paper and does not change anything we know about the urban heat island effect.\nThe overall implication of the allegations was to cast doubt on the extent to which CRU’s work in this area could be trusted and should be relied upon and we find no evidence to support that implication. [1.3.1]\nDespite being heralded as “the final nail in the coffin of anthropogenic global warming”, Climategate has not even invalidated CRU's results, let alone the conclusions of the climate science community. In any case, the entire work of CRU comprises only a small part of the large body of evidence for anthropogenic global warming. That mountain of evidence cannot be explained away by the behaviour of a few individuals.\nLast updated on 24 December 2010 by James Wight."
  },
  {
   "title": "Deniers are part of the 97%",
   "paragraph": "Contrarian opinions on global warming match the 2–3% fringe minority of peer-reviewed papers\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nIf anyone claims to be part of the 97%, it means they disagree with the contrarian argument that humans are having a minimal impact on global warming. Moreover, in order to be part of the 96% expert consensus, they must explicitly agree that humans are responsible for the majority of the global warming over the past half-century.\nClimate Myth...\nDeniers are part of the 97%\n\"All [contrarians] are part of that 97% because that 97% includes those who think humans have some influence on climate. Well, that's a fairly innocuous statement.\" (Roy Spencer)\nIn May 2013, several Skeptical Science contributors published a paper showing that of peer-reviewed climate publications over the past 20 years that take a position on the cause of global warming, 97 percent agree that humans are responsible. Since that paper was published, it's been met with extensive denialism.\nOne of the most common contrarian reactions to the results of our paper has been to claim that 'skeptics' are included in the 97 percent as well. These arguments are based on one of the categories used in our study regarding \"implicit endorsements\" of human-caused global warming. A paper that was included in this category:\n\"Implies humans are causing global warming. E.g., research assumes greenhouse gas emissions cause warming without explicitly stating humans are the cause\"\nThis particular category doesn't state how much global warming humans are causing, and hence climate contrarians claim that because they admit humans are causing some global warming, they should be included in the 97 percent.\nHowever, this argument only considers one of the seven categories used in our study. Another critical category, the \"implicit rejections\" included any paper that (emphasis added):\n\"implies humans have had a minimal impact on global warming without saying so explicitly E.g., proposing a natural mechanism is the main cause of global warming\"\nFor those desiring papers with more explicit positions on the cause of global warming, we also used categories that only included papers that explicitly quantified the human contribution to global warming. We asked the scientific authors to rate their own papers, and of the papers in those categories (237 total), 96 percent agreed that humans are responsible for the majority of the current global warming.\nTherefore, if anyone claims to be part of the 97 percent, it means they disagree with the contrarian argument that humans are having a minimal impact on global warming. Moreover, in order to be part of the 96 percent expert consensus, they must explicitly agree that humans are responsible for the majority of the global warming over the past half-century (a position the latest IPCC report took with 95 percent confidence). Those who believe the human influence on the climate is minimal hold fringe views that are consistent with just 2 to 3 percent of the peer-reviewed climate science literature.\nLast updated on 13 February 2014 by dana1981. View Archives"
  },
  {
   "title": "DMI show cooling Arctic",
   "paragraph": "DMI data on Arctic temperatures: Hide the Increase?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nWhile summer maximums have showed little trend, the annual average Arctic temperature has risen sharply in recent decades.\nClimate Myth...\nDMI show cooling Arctic\nFrom DMI we learn, that Arctic 80N-90N temperatures in the melt season this year is colder than average. This was the case last year too, while earlier years in the DMI analysis period (1958-2010) hardly ever shows Arctic melt season temperatures this cold (Frank Lansner)\nA recent WUWT article by Frank Lansner, August 5th 2010 has the heading “DMI polar data shows cooler Arctic temperature since 1958”. Peter Berenyi also posted a similar chart here on SkS.\nIf we look at the Danish Meteorological Institute (DMI) Arctic temperature records, we see that every year the Arctic Surface temperatures rise to a maximum of just above 0 degrees C in summer, before falling again to extremely cold temperatures in the long dark winters. As each summer advances, the ice surface temperature rises, until the melting point of ice is reached. At this point any further thermal energy is used to melt the ice, and the surface temperature (and the air just above it) can not increase while any significant amount of ice remains. At present there are still significant amounts of sea ice above 80 degrees North through the Summer melt season.\nIt is well documented that annual average Arctic air temperatures have increased over the past few decades, at a rate around twice the global average. How are these temperature records reconciled with the DMI summer data?\nIf we look at the entire official daily DMI data set, we can quickly see that the Summer temperatures do not vary much over the entire record (much as in the Lansner chart), but the Winter temperatures have significantly increased, visible even on this zoomed out vertical scale which includes the large seasonal variations.\nFigure 1: DMI daily temperature values, annual average and linear trend over the entire record period\nFitting a simple linear trend to the data, we see that the DMI trend over the recorded period is positive at 0.37 degrees C/decade, roughly twice the reported global average (from surface measurement data sets and satellite data), entirely in keeping with other reported Arctic temperature data from multiple sources (such as Bekryaev 2010, which gives 0.364 degrees C/decade over a similar 1958 to 2008 period).\nIf we look at the DMI 12 month anomaly trend (pink, offset for clarity) compared with the reported “fall” in “Melt season” temperatures (green), we see the Lansner chart in context. This chart is therefore highly misleading.\nFigure 2: DMI summer melt season temperatures and annual DMI temperature anomaly as well as five year running averages\nLast updated on 31 October 2010 by Peter Hogarth."
  },
  {
   "title": "Dropped stations introduce warming bias",
   "paragraph": "Why are there fewer weather stations and what's the effect?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nDropped stations show more warming than kept stations.\nClimate Myth...\nDropped stations introduce warming bias\n'Two American researchers allege that U.S. government scientists have skewed global temperature trends by ignoring readings from thousands of local weather stations around the world, particularly those in colder altitudes and more northerly latitudes, such as Canada.' (Vancouver Sun)\nAn oft-cited skeptic argument is that the decrease in available temperature measuring stations during the 1990s introduces an increased bias towards warming. The argument is based upon the premise put forward by some climate change skeptics that stations which show more warming were kept and ones that show less warming were dropped. The reason this assumption is made is because during the 1990s there was a large reduction in the number of meteorological stations being used for global temperature analyses.\nIn order to test this theory, several independent researchers (Tamino, Ron Broberg, Zeke Hausfather, Joseph at Residual Analysis and others at the Clear Climate Code Project) and have calculated whether the stations dropped showed less warming than the ones kept. The results? Several find no difference and several find that dropped stations show more warming.\nFurthermore, it is also important to note that the methods used in global temperature analyses make them robust to the loss of stations because they use techniques which incorporate multiple nearby stations into analysis of any individual region.\nSo to conclude: Independent researchers have shown that there is no truth to the claim that cooling stations were removed, in fact evidence suggests that if these stations were included, warming would be shown to be slightly greater.\nLast updated on 23 October 2016 by robert way. View Archives"
  },
  {
   "title": "Earth hasn't warmed as much as expected",
   "paragraph": "Has Earth warmed as much as expected?\nLink to this page\nWhat the science says...\nThe argument that \"Earth hasn't warmed as much as expected\" generally relies on ignoring the factors which have a cooling effect on the Earth's temperatures, and the planet's thermal inertia, which delays the full amount of global warming. When we do the calculations and include all radiative forcings and the amount of heat being absorbed by the oceans, it shows that the Earth has warmed almost exactly as much as we would expect.\nClimate Myth...\nEarth hasn't warmed as much as expected\n\"According to the UN’s Intergovernmental Panel on Climate Change, the greenhouse forcing from man made greenhouse gases is already about 86% of what one expects from a doubling of CO2 (with about half coming from methane, nitrous oxide, freons and ozone), and alarming predictions depend on models for which the sensitivity to a doubling for CO2 is greater than 2C which implies that we should already have seen much more warming than we have seen thus far, even if all the warming we have seen so far were due to man.\" (Richard Lindzen)\nThe argument that \"Earth hasn't warmed as much as expected\" is a favorite of Dr. Richard Lindzen. Lindzen seems to have first made this argument in a 2002 letter to his local mayor in Newton, Massachusetts.\n\"the impact on the heat budget of the Earth due to the increases in CO2 and other man influenced greenhouse substances has already reached about 75% of what one expects from a doubling of CO2, and the temperature rise seen so far is much less (by a factor of 2-3) than models predict\"\nIn 2005, Lindzen made the same argument in testimony to the UK Parliament House of Lords Economic Affairs Committee. He later repeated the argument on National Public Radio (NPR) in 2006, again on NPR in 2007 in a public debate which included Gavin Schmidt and Michael Crichton, in an Energy&Environment-published paper in 2007, in an article in 2008, another article in 2009, and of course the 2011 article examined in the Case Study and re-published uncritically at WattsUpWithThat and many other \"skeptic\" media sources. Suffice it to say, Lindzen makes this argument frequently.\nLindzen's argument has also been rebutted several times, including by Coby Beck in 2006 and Stefan Rahmstorf in 2008. Let's examine the errors that these rebuttals have uncovered in Lindzen's arguments.\nThermal Inertia\nDue to the fact that much of the Earth is covered in oceans, and it takes a long time to heat water, there is a lag before we see the full warming effects of an increase in atmospheric greenhouse gases (this is also known as \"thermal inertia\"). In fact, we know there remains unrealized warming from the greenhouse gases we've already emitted because there is a global energy imbalance. The amount of unrealized warming is dependent upon the amount of CO2 in the atmosphere (or other radiative forcing causing the energy imbalance) and the thermal inertia of the oceans (which causes a lag before the warming is realized). Lindzen does briefly acknowledge thermal inertia in his UK Parliament testimony:\n\"the observed warming is too small compared to what models suggest. Even the fact that the oceans' heat capacity leads to a delay in the response of the surface does not alter this conclusion.\"\nUnfortunately, Lindzen does not substantiate this claim, or provide any references to support it. However, Stefan Rahmstorf does attempt to quantify the thermal inertia effect in his rebuttal:\n\"Data from about 1 million ocean temperature profiles show that the ocean has been taking up heat at a rate of 0.6 W/m2 (averaged over the full surface of the Earth) for the period 1993–2003 [21]. This rate must be subtracted from the greenhouse gas forcing of 2.6 W/m2, as actual warming must reflect the net change in heat balance, including the heat flow into the ocean.\"\nRahmstorf references Willis et al. (2004), which found an oceanic warming rate of 0.86 ± 0.12 watts per square meter of ocean. Given that approximately 70% of the Earth's surface is ocean, this becomes approximately 0.6 ± 0.07 watts per square meter (W/m2) of overall ocean heat uptake. Schwartz et al. (2010) put the value at 0.37 ± 0.12 W/m2. For our purposes, we'll put the figure at 0.25 to 0.67 with a most likely value of 0.4 W/m2. Let's keep these numbers in our back pocket and move on to the second neglected factor.\nAerosols and Other Cooling Effects\nLindzen briefly addresses aerosols in his most recent article:\n\"Modelers defend this situation...by arguing that aerosols have cancelled [sic] much of the warming (viz Schwartz et al, 2010)...However, a recent paper (Ramanathan, 2007) points out that aerosols can warm as well as cool\"\nIn short, Lindzen's argument is that the radiative forcing from aerosols is highly uncertain with large error bars, and that they have both cooling (mainly by scattering sunlight and seeding clouds) and warming (mainly by black carbon darkening the Earth's surface and reducing its reflectivity) effects. These points are both accurate.\nHowever, neglecting aerosols in calculating how much the planet should have warmed does not account for their uncertainty. On the contrary, this is treating aerosols as if they have zero forcing with zero uncertainty. It's true that aerosols have both cooling and warming effects, but which is larger?\nIn his argument, Lindzen refers us to Ramanathan et al. (2007). This study examined the warming effects of the Asian Brown Cloud and concluded that \"atmospheric brown clouds enhanced lower atmospheric solar heating by about 50 per cent.\" The study also noted that, consistent with Lindzen's claims about the aerosol forcing uncertainty, there is \"at least a fourfold uncertainty in the aerosol forcing effect.\" However, this study focused on the warming effects of black carbon, and did not compare them to the cooling effects of atmospheric aerosols.\nRamanathan and Carmichael (2008), on the other hand, examined both the warming and cooling effects of aerosols. This study found that black carbon has a warming effect of approximately 0.9 W/m2, while aerosol cooling effects account for approximately -2.3 W/m2. Thus Ramanathan and Carmichael find that the net radiative forcing from aerosols + black carbon is approximately -1.4 W/m2. This is broadly consistent with the IPCC net aerosol + black carbon forcing most likely value of -1.1 W/m2:\nFigure 1: Global average radiative forcing in 2005 (best estimates and 5 to 95% uncertainty ranges) with respect to 1750. Source (IPCC AR4).\nNote that Lindzen's assumed zero net aerosol + black carbon forcing is outside of this confidence range; therefore, neglecting its effect cannot be justified. However, since the IPCC provides us with the 95% confidence range of the total net anthropogenic forcing in Figure 1, we can account for the uncertainties which concern Lindzen, and evaluate how much warming we \"should have seen\" by now.\nExpected Forcing Effects on Temperature Thus Far\nIn fact, this is a simple calculation. The IPCC 95% confidence range puts the total net anthropogenic forcing at 0.6 to 2.4 W/m2 (Figure 1). On top of that, as discussed above, ocean heat uptake accounts for between 0.25 and 0.67 W/m2. Therefore, subtracting the ocean heat uptake, the total net anthropogenic forcing over this period is somewhere between -0.07 and 2.15 W/m2, with a most likely value of 1.1 W/m2.\nA doubling of atmospheric CO2 corresponds to a radiative forcing of 3.7 W/m2, according to the IPCC. Therefore, the net anthropogenic radiative forcing thus far is between approximately 0% and 58% of the forcing associated with a doubling of atmospheric CO2, with a most likely value of 30%.\nIn order to be thorough, we can also include the natural radiative forcings. Most have had approximately zero net effect since 1750, with the exception of the Sun, which has had a forcing of 0.06 to 0.30 W/m2 with a most likely value of 0.12 W/m2 over this period (Figure 1). Therefore, net forcing since 1750 is approximately 0 to 2.45 W/m2, with a most likely value of 1.25 W/m2. Thus the total net forcing thus far is between 0% and 66% of the forcing associated with a doubling of atmospheric CO2, with a most likely value of 34%.\nWhat Does This Tell Us About Climate Sensitivity?\nSo far, global surface air temperatures have increased approximately 0.8°C in response to these radiative forcings. Since we're 0% to 66% of the way to the radiative forcing associated with a doubling of atmospheric CO2 (most likely value of 34%), the amount we should expect the planet to warm if CO2 doubles (also known as \"climate sensitivity\") has a most likely value of 2.4°C, with a minimum of 1.2°C (because of the large aerosol cooling effect uncertainty and the fact that we may only be 0% of the way to the doubled CO2 forcing, we can't place an upper limit on the climate sensivity parameter with this calculation). Using a much wider range of evidence, the IPCC puts the likely climate sensitivity range to a doubling of CO2 at 2 to 4.5°C with a most likely value of 3°C. Our calculation is consistent with IPCC the most likely value.\nHow Much Warming Should We Have Seen?\nWe can also flip the calculation backwards, assuming the IPCC most likely climate sensitivity of 3°C for a doubling of atmospheric CO2 and using the numbers above. In this case, we should have seen from 0% to 66% of 3°C, or about 0 to 2.0°C. Clearly the amount of warming we have seen so far is well within this range. Additionally, the most likely amount of warming is 34% of 3°C, which is 1.0°C. In other words, we have seen very close to the amount of warming that we \"should have\" seen, according to the IPCC.\nWarming is Consistent with What We Expect\nIn short, contrary to Lindzen's claims, the amount of surface warming thus far (0.8°C) is consistent with what we \"should have seen\" based on the IPCC numbers. Moreover, this calculation puts the most likely climate sensitivity parameter value within the IPCC's stated range, whereas the much lower value claimed in Lindzen and Choi (2009) (less than 1°C for CO2 doubling) is inconsistent even with our calculated climate sensitivity lower bound (1.2°C). For additional discussion of the errors with Lindzen and Choi (2009), see here.\nWhen we actually account for thermal inertia and negative forcings, we find that the amount of warming we have seen is consistent with what the IPCC would expect, but inconsistent with Lindzen and Choi 2009. Thus the correct conclusion is that if Lindzen is correct about low climate sensitivity, we should already have seen much less warming than we have seen thus far.\nLast updated on 5 February 2011 by dana1981."
  },
  {
   "title": "Extreme weather isn't caused by global warming",
   "paragraph": "Is extreme weather caused by global warming?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nGlobal warming amplifies the risk factors for extreme weather events - and that is all that Climate Science claims.\nClimate Myth...\nExtreme weather isn't caused by global warming\n\"The 30 major droughts of the 20th century were likely natural in all respects; and, hence, they are \"indicative of what could also happen in the future,\" as Narisma et al. state in their concluding paragraph. And happen they will. Consequently, the next time a serious drought takes hold of some part of the world and the likes of Al Gore blame it on the \"carbon footprints\" of you and your family, ask them why just the opposite of what their hypothesis suggests actually occurred over the course of the 20th century, i.e., why, when the earth warmed - and at a rate and to a degree that they claim was unprecedented overthousands of years - the rate-of-occurrence of severe regional droughts actually declined.\" (source: CO2 Science)\nWhenever there is an extreme weather event, such as a flood or drought, people ask whether that event was caused by global warming. Unfortunately, there is no straightforward answer to this question. Weather is highly variable and extreme weather events have always happened. Detecting trends takes time, particularly when observational records are rare or even missing in certain regions. An increase in extreme weather is expected with global warming because rising temperatures affect weather parameters in several ways. Changes in the frequency of extreme events coinciding with global warming have already been observed, and there is increasing evidence that some of these changes are caused by the impacts of human activities on the climate.\nHow global warming affects weather parameters\nRising temperatures can have several effects on the factors involved in weather. For example:\nThey increase the rate of evapotranspiration, which is the total evaporation of water from soil, plants and water bodies. This can have a direct effect on the fequency and intensity of droughts.\nA warmer atmosphere can hold more water vapour. The atmosphere now holds 4% more water vapour than it did 40 years ago as a result of increasing temperatures. This increases the risk of extreme rainfall events.\nChanges in sea-surface temperatures (SSTs) also have an effect by bringing about associated changes in atmospheric circulation and precipitation. This has been implicated in some droughts, particularly in the tropics.\nThese changes don't automatically generate extreme weather events but they change the odds that such events will take place. It is equivalent to the loading of dice, leading to one side being heavier, so that a certain outcome becomes more likely. In the context of global warming, this means that rising temperatures increase the odds of extreme events occurring.\nChanges in extreme weather events are already being observed\nIn the US, the Global Changes Research Program published a report in 2009 entitled Global Climate Change Impacts in the US. The National Climate Change chapter reports the following findings for recent decades:\nHeavy rainfall events have increased both in frequency and in intensity by 20%, and are the main cause behind the increase in overall precipitation in the US. The Northeast and Midwest have seen the greatest increase in such events.\nThe frequency of drought has increased in areas such as the Southeast and the West, and decreased in other areas. Rising temperatures make droughts more severe and/or widespread, and also lead to the earlier melting of snowpacks, which can exacerbate problems in vulnerable areas.\nAtlantic hurricanes have increased both in power and frequency, coinciding with warming oceans that provide energy to these storms. In the Eastern Pacific, there have been fewer but stronger hurricanes recently. More research is needed to better understand the extent to which other factors, such as atmospheric stability and circulation, affect hurricane development.\nSimilarly, Australia has seen the odds of both heavy rainfalls and droughts increase, and similar patterns are being observed worldwide, coinciding with rising temperatures over the past 50 years. Heat waves are also occurring more frequently as temperatures shift upwards:\nSource: NASA/Goddard Space Flight Center GISS and Scientific Visualization Studio\nIn conclusion, although it isn't possible to state that global warming is causing a particular extreme event, it is wrong to say that global warming has no effect on the weather. Rising air and sea temperatures have a number of effects on the water cycle, and this increases the odds for more extreme weather events.\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Freedom of Information (FOI) requests were ignored",
   "paragraph": "Climategate and the Freedom of Information (FOI) requests\nLink to this page\nWhat the science says...\nThe Independent Climate Change Email Review found the CRU scientists were unhelpful and unsympathetic to information requesters and at times broke FoI laws. However, CRU is a small research unit with limited resources, and they perceived the requesters were not acting in good faith. The same inquiry found the rigour and honesty of the scientists are not in doubt, and their behaviour did not prejudice the advice given to policymakers.\nClimate Myth...\nFreedom of Information (FOI) requests were ignored\n\"The emails suggest that the authors co-operated (perhaps the word is “conspired”) to prevent data from being made available to other researchers through either data archiving requests or through the Freedom of Information Acts of both the U.S. and the UK.\" (Pajamas Media)\nExhibit No. 1 of the climate conspiracy theory is a collection of emails stolen from the Climatic Research Unit (CRU) of the University of East Anglia (UEA), which appeared on the internet in November 2009. Though some of these \"Climategate\" emails can sound damning when quoted out of context, several inquiries have cleared the scientists. The most comprehensive inquiry was the Independent Climate Change Email Review.\nOne allegation arising from the emails (and arguably the only instance where there is actually a case to be answered) is that Freedom of Information requests received by CRU were wrongly denied. Meanwhile, defenders of CRU “have suggested that a number of these FoIA requests were inappropriate or frivolous.” [10.2]\nBelow I have reproduced some of the emails often quoted in support of these allegations (all were written by Phil Jones):\n7/5/2004: Many of us in the paleo field get requests from skeptics (mainly a guy called Steve McIntyre in Canada) asking us for series. Mike and I are not sending anything, partly because we don’t have some of the series he wants, also partly as we’ve got the data through contacts like you, but mostly because he’ll distort and misuse them. Despite this, Mike and I would like to make as many of the series we’ve used in the [Reviews of Geophysics] plots available from the CRU web page.\n2/2/2005: [D]on’t leave stuff lying around on ftp sites — you never know who is trawling them. The two MMs have been after the CRU station data for years. If they ever hear there is a Freedom of Information Act now in the UK, I think I’ll delete the file rather than send to anyone. Does your similar act in the US force you to respond to enquiries within 20 days? - our does! […] Tom Wigley has sent me a worried email when he heard about it—thought people could ask him for his model code. He has retired officially from UEA so he can hide behind that.\n21/2/2005: I’m getting hassled by a couple of people to release the CRU station temperature data. Don’t any of you three tell anybody that the UK has a Freedom of Information Act!\n27/4/2005: I got this email from McIntyre a few days ago. As far as I’m concerned he has the data — sent ages ago. I’ll tell him this, but that’s all — no code. If I can find it, it is likely to be hundreds of lines of uncommented fortran ! I recall the program did a lot more than just average the series. I know why he can’t replicate the results early on — it is because there was a variance correction for fewer series.\n29/5/2008: Can you delete any emails you may have had with Keith re AR4? Keith will do likewise. […] Can you email Gene and get him to do the same? […] We will be getting Caspar to do likewise.\n3/12/2008: When the FOI requests began here, the FOI person said we had to abide by the requests. It took a couple of half hour sessions — one at a screen, to convince them otherwise showing them what CA was all about. Once they became aware of the types of people we were dealing with, everyone at UEA […] became very supportive. […] The inadvertent email I sent last month has led to a Data Protection Act request sent by a certain Canadian, saying that the email maligned his scientific credibility with his peers! If he pays 10 pounds (which he hasn’t yet) I am supposed to go through my emails and he can get anything I’ve written about him. About 2 months ago I deleted loads of emails, so have very little — if anything at all.\n10/12/2008: Haven’t got a reply from the FOI person here at UEA. So I’m not entirely confident the numbers are correct. One way of checking would be to look on CA, but I’m not doing that. I did get an email from the FOI person here early yesterday to tell me I shouldn’t be deleting emails — unless this was ‘normal’ deleting to keep emails manageable! […] According to the FOI Commissioner’s Office, IPCC is an international organisation, so is above any national FOI. Even if UEA holds anything about IPCC, we are not obliged to pass it on, unless it has anything to do with our core business — and it doesn’t. I’m sounding like Sir Humphrey here! McIntyre often gets others to do the requesting, but requests and responses all get posted up on CA regardless of who sends them.\nThe general allegation is that CRU incorrectly denied FoI requests. In particular, the Review focused on the question of whether UEA’s formal processes for dealing with FoI requests were “fair and impartial”.\nThe Review Team interviewed the relevant UEA and CRU staff, as well as representatives of the Information Commissioner’s Office (ICO). UEA’s FoI process is centred around their Information Policy & Compliance Manager (IPCM). In the two years after current laws came into effect at the start of 2005, no requests for information were logged with the IPCM, though we know from the emails that there were such requests. We know from the IPCM log that CRU received four requests in 2007, two in 2008, and one in the first half of 2009 (four were fully granted and three rejected).\nThen came the storm. Between 24 July and 28 July, CRU received no less than 60 FoI requests, and 10 more between 31 July and 14 August. The requesters demanded access to both raw temperature station data and any related confidentiality agreements. The Review found evidence that this was an organized campaign (one request asked for information “involving the following countries: [insert 5 or so countries that are different from ones already requested]”). The Review says “such orchestrated campaigns [have] literally overwhelming impacts on small research units.”\nThe Review found there was “insufficient priority given from the UEA centre to motivating staff and to prompting continuing education” about their legal requirements under FoI law. Similarly, they found “a lack of engagement by core CRU team”, as well as “a tendency to assume that no action was required until precedents had been set”. Some of the emails suggest a “lack of sympathy with the requesters” and “a tendency to answer the wrong question or to give a partial answer.” [10.5]\n“There seems clear incitement to delete e-mails, although we have seen no evidence of any attempt to delete information in respect of a request already made.” (The former is legal but not the latter.) The email dated 3/12/2008 included “a clear statement that e-mails had been deleted […] It seems likely that many of these ‘deleted’ e-mails subsequently became public following the unauthorized release from the backup server.” [10.5]\nThe Review found that the IPCM “may have lacked […] the authority to challenge the assertions of senior professors” and “the UEA senior staff need to take more explicit responsibility for these processes”. He told the Review he felt “very much the bull’s eye at the centre of the target”. He explicitly denied that he “became very supportive” as suggested by Jones. The 10/12/2008 email provides “evidence that the IPCM did try to warn Prof. Jones about deliberate deletion of information”. [10.5]\nIn general, “[t]he Review found an ethos of minimal compliance (and at times non-compliance) by the CRU with both the letter and the spirit of the FoIA and EIR. We believe that this must change”. The Review also made it clear that CRU did not receive enough support from UEA management, and made recommendations to the university on how it should handle future information requests. It also recommended to the ICO that it engage more with universities and clarify how FoI law applies to research.\nHowever, as Steve Easterbrook commented, the Review “never really acknowledges the problems a small research unit (varying between 3.5 to 5 FTE staff over the last decade) would have in finding the resources and funding to be an early adopter in open data and public communication, while somehow managing to do cutting edge research in its area of expertise too.” The Review does point out that in the years since CRU was founded climate science has developed from “a relatively obscure area of science […] into an area of great political and public concern.”\nThe Review concluded:\n[W]e find that a fundamental lack of engagement by the CRU team with their obligations under FoIA/EIR, both prior to 2005 and subsequently, led to an overly defensive approach that set the stage for the subsequent mass of FoIA/EIR requests in July and August 2009. We recognize that there was deep suspicion within CRU, as to the motives of those making detailed requests. Nevertheless, the requirements of the legislation for release of information are clear and early action would likely have prevented much subsequent grief. [10.6]\nAs Phil Jones has admitted, CRU did the wrong thing with regard to Freedom of Information requests. However, they clearly perceived that the requests were not being made in good faith. The Review apparently made no attempt to investigate the motivations of the requesters.\nBut all this must be considered in the context of the Review's general findings (summarised here): although the scientists failed to display the proper degree of openness, their rigour and honesty are not in doubt, and their behaviour did not prejudice the advice given to policymakers. Despite being heralded as “the final nail in the coffin of anthropogenic global warming”, Climategate has not even invalidated CRU's results, let alone the conclusions of the climate science community. In any case, the entire work of CRU comprises only a small part of the large body of evidence for anthropogenic global warming. That mountain of evidence cannot be explained away by the behaviour of a few individuals.\nLast updated on 4 November 2016 by James Wight. View Archives"
  },
  {
   "title": "Glaciers are growing",
   "paragraph": "Are glaciers growing or retreating?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nMost glaciers are retreating, posing a serious problem for millions who rely on glaciers for water.\nClimate Myth...\nGlaciers are growing\n“[R]eports are coming in from all over the world: for the first time in over 250 years, glaciers in Alaska, Canada, New Zealand, Greenland, and now Norway are growing.”(JamulBlog)\nAlthough Glaciologists measure year-to-year changes in glacier activity, it is the long term changes which provide the basis for statements such as \"Global Glacier Recession Continues\". Some Skeptics confuse these issues by cherry picking individual glaciers or by ignoring long term trends. Diversions such as these do not address the most important question of what is the real state of glaciers globally?\nThe answer is not only clear but it is definitive and based on the scientific literature. Globally glaciers are losing ice at an extensive rate (Figure 1). There are still situations in which glaciers gain or lose ice more than typical for one region or another but the long term trends are all the same, and about 90% of glaciers are shrinking worldwide (Figure 2).\nFigure 1: Long-term changes in glacier volume adapted from Cogley 2009.\nFigure 2: Percentage of shrinking and growing glaciers in 2008–2009, from the 2011 WGMS report\nIt is also very important to understand that glacier changes are not only dictated by air temperature changes but also by precipitation. Therefore, there are scenarios in which warming can lead to increases in precipitation (and thus glacier ice accumulation) such as displayed in part of southwestern Norway during the 1990s (Nesje et al 2008).\nThe bottom line is that glacier variations can be dependent on localized conditions but that these variations are superimposed on a clear and evident long term global reduction in glacier volume which has accelerated rapidly since the 1970s.\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 6 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Global warming stopped in 1998, 1995, 2002, 2007, 2010, ????",
   "paragraph": "Did global warming stop in 1998, 1995, 2002, 2007, 2010?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nGlobal temperatures continue to rise steadily beneath the short-term noise.\nClimate Myth...\nGlobal warming stopped in 1998, 1995, 2002, 2007, 2010, ????\n\"January 2008 capped a 12 month period of global temperature drops on all of the major well respected indicators. HadCRUT, RSS, UAH, and GISS global temperature sets all show sharp drops in the last year\" (source: Watts Up With That).\nA common claim amongst climate skeptics is that the Earth has been cooling recently. 1998 was the first year claimed by skeptics for 'Global Cooling'. Then 1995 followed by 2002. Skeptics have also emphasized the year 2007-2008 and most recently the last half of 2010.\nNASA and climate scientists throughout the world have said, however, that the years starting since 1998 have been the hottest in all recorded temperature history. Do these claims sound confusing and contradictory? Has the Earth been cooling, lately?\nTo find out whether there is actually a 'cooling trend,' it is important to consider all of these claims as a whole, since they follow the same pattern. In making these claims, skeptics cherrypick short periods of time, usually about 20 years or less.\nThe temperature chart below is based on information acquired from NASA heat sensing satellites. It covers a 30 year period from January 1979 to November 2010. The red curve indicates the average temperature throughout the entire Earth.\nThe red line represents the average temperature. The top of the curves are warmer years caused by El Niño; a weather phenomenon where the Pacific Ocean gives out heat thus warming the Earth. The bottoms of the curves are usually La Niña years which cool the Earth. Volcanic eruptions, like Mount Pinatubo in 1991 will also cool the Earth over short time frames of 2-3 years.\nFigure 1: University of Alabama, Huntsville (UAH) temperature chart from January 1979 to November 2010. This chart is shown with no trend lines so the viewer may make his own judgment.\nBelow is the same temperature chart, showing how skeptics manipulate the data to give the impression of 'Global Cooling'. First they choose the warmest most recent year they can find. Then, in this case, they exclude 20 years of previous temperature records. Next they draw a line from the warmest year (the high peak) to the lowest La Niña they can find. In doing this they falsely give the impression that an ordinary La Niña is actually a cooling trend.\nFigure 2: Representation of how skeptics distort the temperature chart. Even though the chart clearly indicates increased warming, skeptics take small portions of out of context to claim the opposite.\nWhat do the past 30 years of temperature data really show? Below is the answer.\nFigure 3: Trend lines showing the sudden jump in temperatures in the 1995 La Niña (Green lines) and the 1998 (Pink lines) El Niño events. Brown line indicates overall increase in temperatures.\nThe chart above clearly shows that temperatures have gone up. When temperatures for the warm El Niño years (pink lines) during 1980-1995 are compared to 1998-2010, there is a sudden increase of at least 0.2o Centigrade (0.36o Fahrenheit). Temperatures also jumped up by about 0.15oC (0.27oF) between the cool La Niña years (Green lines) of 1979-1989 and those of 1996-2008 (the eruption of Mount Pinatubo in 1991 lowered the Earth's temperatures in the midst of an El Niño cycle). The overall trend from 1979 through November 2010 (Brown line) shows an unmistakable rise.\nThis is particularly clear when we statistically remove the short-term influences from the temperature record, as Kevin C did here:\nIn spite of these facts, skeptics simply keep changing their dates for 'Global Cooling', constantly confusing short-term noise and long-term trends (Figure 4).\nFigure 4: Average of NASA GISS, NOAA NCDC, and HadCRUT4 monthly global surface temperature anomalies from January 1970 through November 2012 (green) with linear trends applied to the timeframes Jan '70 - Oct '77, Apr '77 - Dec '86, Sep '87 - Nov '96, Jun '97 - Dec '02, and Nov '02 - Nov '12.\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 September 2017 by MichaelK. View Archives"
  },
  {
   "title": "Great Barrier Reef is in good shape",
   "paragraph": "The decline of the Great Barrier Reef\nLink to this page\nWhat the science says...\nThe evidence clearly shows that ocean warming and acidification due to human CO2 emissions are adversely impacting the Great Barrier Reef.\nClimate Myth...\nGreat Barrier Reef is in good shape\nthe Great Barrier Reef is in fine fettle (Bob Carter)\nWhat's the current state of the GBR (i.e. is it really \"in fine fettle\")?\nDespite being one of the best managed marine ecosystems worldwide, there is evidence that the ecological 'health' of the Great Barrier Reef has declined since the arrival of European settlers into the Queensland region. This evidence comes from a number of key sources. This area is not without its controversy, which is discussed elsewhere at Skeptical Science by Professor John Bruno (University of North Carolina) and others.\nHistoric photographic analysis (> late 19th century)\nOne of the first and most direct way has been via 'before and after' photographs assembled by Dr. David Wachenfeld of the Great Barrier Reef Marine Park Authority. Dr. Wachenfeld searched photographic archives of pictures of intertidal coral reef systems going back over 100 years. He then organised photographs to be taken in exactly the same spot of the same reef systems. The results revealed no change in reefs systems at 6 out of the 14 sites where it was possible to compare photographs from 100 or more years ago. At four sites, there was evidence of change in the health of intertidal reef communities, while at the 4 remaining sites, coral had more or less disappeared from the intertidal reef areas (Wachenfeld 1997). Photographic analysis is a very direct way of assessing whether or not coral reefs have declined at particular locations. However, it is important to acknowledge that photographers probably went out to photograph 'beautiful' areas of the reef as opposed to areas without coral cover. The criticism that arises is that had that happen, we might have seen an equal and opposite set of photographs in which coral returns to coastal reef flats. Several more recent studies have been exploring sediments using push coring and radioisotope dating techniques, which is allowing a reconstruction of past communities based on the sediments that have built-up. These studies are revealing that major changes have been occurring in coastal coral communities, with the general trend that coral communities have converted into seaweed communities that are very different to the coral communities of old.\nFigure 1. Matched photographs of the same section of reef taken around 1890 by William Saville- Kent and again in 1994 by Andrew Elliott. Full details can be obtained in the study reported by Wachenfeld (1997).\nMeta-studies: long-term trends (> 1960)\nSeveral research groups have assembled datasets from the growing number of studies that have been undertaken on the Great Barrier Reef. These studies are referred to as meta-studies given that they depend on drawing numbers from published papers. The idea is that if these sources are carefully vetted for a certain level of quality and rigor, then the dataset of numbers on things like the abundance of coral on coral reefs at a certain time in recent history can be established. One of the first of these was (Bellwood et al. 2004) who found a strong negative trend in coral abundance on Great Barrier Reef sites going back to 1960. This analysis supported a related study by (Pandolfi et al. 2003) who used historic and paleontological evidence to compare the Great Barrier Reef to other reef systems worldwide. They also concluded that the Great Barrier Reef, although still a beautiful reef system had declined over the past century. The third study by (Bruno and Selig 2007) undertook a similar analysis, and compared coral cover on the Great Barrier Reef in the early 1980s to recent survey data. This study revealed a major drop in coral cover (40-50%).\nFigure 2. Three independent studies reporting changes in the percentage of coral cover on reefs within the Great Barrier Reef Marine Park.\nAIMS long-term ecological surveys (>1986)\nThe third category of information comes from long-term survey data. The Australian Institute of Marine Science began annual ecological survey of key aspects of coral and fish abundance on the Great Barrier Reef in 1986. While these surveys biased toward offshore reef areas and hence have probably not caught the full picture of human impacts on the Great Barrier Reef, they are empirically sound in terms of the methodology needed to examine ecological parameters such as coral abundance over time. As indicated above, there has been some scientific debate (which always goes on and is part of the sceptical process of science) and some of the details have been discussed by Professor John Bruno in separate postings to SkS and Climate Shifts.\nDrawing together these three types of studies, there is fairly compelling evidence that the Great Barrier Reef has undergone significant ecological change over the past 50 to 100 years. Even the declined reported by the AIMS Long-Term Monitoring Project (Figure 2) from 28% to 22% coral cover (a decrease of 22%) from 1986 to 2004 (Sweatman et al. 2011) is of great concern given the massive size of the Great Barrier Reef and the speed of this change (10% decrease per decade).\nHow do we know the GBR decline is due to human impacts?\nEvidence for humans being responsible for the recent changes in the Great Barrier Reef comes from a number of different disciplines and sources.\nFirstly, there is a considerable body of information within the literature that has examined the sensitivity of reef building corals, a core component of coral reefs, to stressors such as high temperatures and light levels, reduced salinity and alkalinity, elevated nutrient loads, sedimentation, toxins, and pollutants. This information has been assembled in a number of different review articles and can be accessed through scientific journals such as Coral Reefs. The peer-reviewed science in these journal articles deal with the core sensitivity of reef building corals to particular factors.\nSecondly, there is abundant field evidence of these types of factors having a big impact on reef ecosystems. In this regard, there are numerous examples of sudden changes to environmental parameters (e.g. flood damage through reduced salinity to restructures, mass coral bleaching events triggered by elevated sea temperatures) which are consistent with the information from more laboratory and aquarium-based studies. Taken together, the two are constantly interacting. Observations in the laboratory that are not supported by field evidence generally trigger reinvestigation in the laboratory, and vice verse.\nLastly, there are modelling studies which summarise our understanding of how coral reefs might react to changes in the physical, chemical, and biological changes within the environment. These studies have provided projections or predictions which can be tested. In cases where a projection or prediction fails to be supported by laboratory and field evidence, models are then re-examined and reconfigured. The most important step, however, is to ensure that models that have been reconfigured are subsequently tested. Without this final step, one can fall into the trap of models that are adjusted to fit the results as opposed to being truly predictive in nature.\nWith these datasets in place, how are we able to link human activities to the changes that we are seeing? This comes down to the link between the stressors (elevated temperatures and light levels, reduce salinity and alkalinity, elevated nutrient loads, sedimentation, toxins and pollutants) and human activities. At local levels, there are extremely strong links between changes to land-use along the Queensland coast and elevated levels of sediments, nutrients, toxins and pollutants, and low salinity events. As trees were cut down in the major river catchments, for example, soil was destabilised along the banks of the rivers and creeks that feed into the coastal areas of the Great Barrier Reef. This led to much higher levels of these known stressors. Interestingly, there have been some studies (McCulloch et al. 2003) that have used isotopic information to show that the levels of these stressors have increased by 5-10 fold since the modification by European farmers of river catchments within the Great Barrier Reef basin.\nFrom McCulloch et al. (2003)\nThese types of data have built up a comprehensive set of linkages between human activities such as deforestation, intensive agriculture, and fishing. There are a number of resources that are available at the Great Barrier Reef Marine Park Authority and at the Australian Research Council Centre for Excellence in Coral Reef Studies and explore the evidence and background to our concern about these local factors.\nIs climate change a big threat to the Great Barrier Reef and how do we know?\nCoral reefs like the Great Barrier Reef depend on a narrow set of environmental conditions within which they prosper. At the heart of their biology, is a symbiosis that they form with tiny plant-like organisms known as dinoflagellates (commonly called zooxanthellae). This symbiosis is critical to the survival of corals and coral reefs, making possible the efficient trapping of sunlight by reef-building corals. This allows them a cheap source of energy with which to grow in the sunlit waters along tropical coastlines, and where they often deposit of vast quantities of calcium carbonate (a limestone-like substance), creating the three-dimensional structures that we know of as coral reefs. These magnificent structures build up over time, attracting over one million species which are known to live in and around coral reefs (Reaka-Kudla 1997). This mega-diversity is unrivalled anywhere else in the ocean, and supports industries like tourism and fisheries, and provides food for at least 100 million people worldwide (Hoegh-Guldberg 1999).\nCoral reefs have come under increasing pressure from the local effects discussed in Part 1 which arise from the rapidly growing coastal populations worldwide. In the early 1980s, coral reefs all over the world began to exhibit a phenomenon known as mass coral bleaching. Mass coral bleaching occurs when the symbiosis between corals and tiny plant-like organisms known as dinoflagellates breaks down. As a result, corals are deprived of their energy source, leaving them prone to disease and death. Over the past 25 years, mass coral bleaching and mortality events have grown in frequency and scale.\nFigure 4. Background image shows and extensively bleached reef near Great Keppel Island on the southern Great Barrier Reef in early 2006. The insert shows a normal looking coral on the left-hand side and a bleached coral on the right hand side. The dinoflagellates symbionts which leave the coral is it bleaches are also shown.\nThe observation that there are no reports of mass coral bleaching prior to 1980 in the scientific literature has led many to conclude that mass coral bleaching is a new phenomenon associated with the activities of humans. The flipside to this argument is that mass coral bleaching events may have occurred in the past but we've only just begun to notice them. This argument falls down for a number of reasons. Firstly, mass coral bleaching events are hard not to notice. When a reef undergoes mass coral bleaching, there is a brilliant white colour that can be seen from passing satellites. Secondly, these events cover hundreds of square kilometres of territory when they occur, and would be as hard not to notice as a bushfire would be on land. Given that scientists have been intensely studying coral reefs since the 1950s (as verified by the scientific literature which contains thousands of articles on coral reefs prior to 1980), it is inconceivable that observers would have failed to note this massive visual change in the ecosystem that they were studying. Lastly, filmmakers such as Ron and Valerie Taylor who have been filming coral reefs since the 1960s do not have mass coral bleaching images on film.\nThe Great Barrier Reef has experienced six major bleaching events since they began occurring. These events have been growing in scale and size, culminating in 1998 and 2002 in mass bleaching events which affected over 50% of reefs within the Great Barrier Reef Marine Park (Berkelmans; Oliver 1999; Berkelmans et al. 2004).\nFigure 5. Geographic patterns associated with mass coral bleaching in 1998 (Berkelmans; Oliver 1999).\nThere is little doubt within the scientific community that elevated sea temperatures are driving the increase in mass coral bleaching and mortality seen on the Great Barrier Reef over the past 25 years. The evidence comes down to 3 key components:\nFirstly, experimental studies within aquaria and laboratory settings revealed that corals will bleach when exposed to increases in sea temperature of around 2 to 3°C above their normal maximum temperatures (Glynn; Dcroz 1990; Hoegh-Guldberg; Smith 1989; Jones et al. 1998).\nSecondly field study measurements have revealed a close association between small changes in sea temperature above long-term summer maxima and mass coral bleaching events in the field (Berkelmans 2002; Bruno et al. 2001; Dove; Hoegh-Guldberg 2006; Glynn 1983, 1993, 1996; Oliver et al. 2009).\nLastly, satellite detection algorithms based on the laboratory and field evidence have been highly successful in predicting the incidence of mass coral bleaching based on small changes in sea surface temperature. In this case, satellites flown by the National Oceanic and Atmospheric Administration as part of their Coral Reef Watch Program are highly accurate in terms of predicting when and where bleaching will occur.\nFigure 6. Exposure to elevated heat stress during the lead up to the 1998 bleaching event. Degree heating weeks are calculated by multiplying the size of the sea surface temperature anomaly (relative to the long-term summer average) by the time of exposure. Anything over 4 results in bleaching, while anything above 8 is likely to result in large-scale coral mortality.\nMass coral bleaching represents a threat to coral reefs everywhere. Seawater temperatures are increasing in tropical waters at the rate of 0.1-0.4 degrees Celsius per decade, and will soon be high enough to cause mass coral bleaching every year. Several analyses have been done for coral reefs across the planet (Done et al. 2003; Donner et al. 2005; Hoegh-Guldberg 1999) which have used the known sensitivity of coral reefs and projections of how sea temperature will change over the coming decades. These studies also revealed that coral reefs will bleaching conditions on an annual basis as early as 2030. Given the mortality rates of corals following mass coral bleaching events can be extremely high (over 90% of all corals on a reef dying within a single mass bleaching event) and the fact that it takes at least 15-30 years for a coral reef to recover from a mass mortality event, these trends strongly suggest that coral-dominated ecosystems are very likely to disappear by the middle of the century.\nFigure 7. Projected sea temperature projected for the southern, central and northern Great Barrier Reef using a global circulation model (ECHAM4/OPYC3). The horizontal lines in each figure represents the temperature thresholds above which significant bleaching and mortality occur.\nFigure 8. Changes in sea surface temperature over the past 100 years (Australian Bureau of Meteorology).\nThere has been some debate about whether or not reef-building corals are able to modify their sensitivity to temperature and thereby beat the climate change signal over the time. Evidence cited in favour of this hypothesis has been the observation that corals and some oceans are already experiencing very high sea temperatures. One example are corals growing in the Arabian Gulf, where sea temperatures reach 36°C. The problem with this example is that corals have adapted to their local temperature conditions, but over hundreds if not thousands of years. The ability of an Arabian Gulf coral to migrate and successfully establish a reef in the Indian Ocean over 10 or even 100 years is not feasible given what we know about the migration rate corals.\nAnother point of discussion has centred on the ability of corals to change their zooxanthellae for varieties that are more tolerant to temperature. This idea that led to the adaptive beaching hypothesis (Buddemeier; Fautin 1993) which proposed that corals bleached deliberately to swap one set of zooxanthellae for more tolerant zooxanthellae. So far, there has not been any compelling evidence that this hypothesis is true. Furthermore, as we have begun to further understand the intricate intracellular symbiosis that corals form with their dinoflagellate zooxanthellae, the more it has become clear that this symbiosis takes an enormous length of time to evolve. That is, to live inside another cell, you need to modify self recognition, integrate biochemically, and undergo a whole series of different changes to ensure that a harmonious endosymbiosis results. The idea that corals could form and use a basis in ecological timeframes of a few years is very unlikely given their long generation times.\nAdditionally, it is extremely unlikely that the thermal tolerance of reef-building corals comes down to that of its zooxanthellae. Heat stress is also likely to be a problem with the host coral, so changing its zooxanthellae is unlikely to solve the problem of rising sea temperatures (Hoegh-Guldberg 1999; Hoegh-Guldberg et al. 2002; Hoegh-Guldberg et al. 2007). This said, some corals do have multiple genetic varieties of zooxanthellae in their tissues. These can be shuffled such that the more thermally tolerant ones can dominate as sea temperatures rise. These separate symbiosis have all evolved with their coral hosts and consequently, provide only limited change in the thermal tolerance of the association as it is challenged by rising sea temperatures. That is, the changes are phenotypic not genetic, and hence do not allow corals to adapt optimally to the novel conditions that are presented by rising sea temperatures.\nThe conclusion after considering all of the science surrounding the response of coral reefs to changes in sea temperature suggest that coral-dominated reef systems are very likely to disappear from a tropical oceans if we do not reduce carbon dioxide emissions.\nHow does ocean acidification interact with warming sea temperature?\nCoral reefs are susceptible to the impacts of ocean acidification (Raven et al. 2005). Ocean acidification arises from increased amounts of CO2 entering the world's oceans. CO2 reacts with water to create a dilute acid (carbonic acid), which interacts with other chemical species present in sea water such as carbonate ions. In the latter case, carbonate ions are increasingly turned into bicarbonate ions. The two changes brought about by ocean acidification, decreasing pH, and carbonate ion concentrations, impact a wide range of marine organisms (Hendriks et al. 2009; Kleypas and Langdon 2006; Kleypas et al. 2006). There is now a large amount of information showing that reef building corals are susceptible to relatively small changes in ocean acidification (Kleypas and Langdon 2006), probably as a result of declining carbonate ion concentrations in the first instance. An important point is that warming sea temperatures and steadily acidifying oceans interact in terms of their impact on reef building corals. That is, the thermal sensitivity of corals appears to increase as oceans become increasingly acidic (Anthony et al. 2008). The full implications of this interaction between climate change stressors has not been fully explored to date.\nWhat evidence do we have that warming and acidification are affecting the Great Barrier Reef?\nThere is now very strong evidence that the Great Barrier Reef is being affected by the combination of these two stress factors. In a study of 328 long-lived corals from a full range of habitats on the Great Barrier Reef has revealed that calcification rates have fallen by 15% since 1990, which is unprecedented in the 400 years of coral core records examined (De'ath et al. 2009). Similar results have also been found in a number of sites around the world (Tanzil et al. 2009). While it is not possible to attribute this change to either increasing tropical sea temperatures or ocean acidification on their own, this study does reveal that changes to the conditions within the Coral Sea (which is steadily rising in sea temperature and acidity) are driving some exceptional changes to fundamental reef processes like calcification.\nIs there a threshold above which coral-dominated ecosystems will disappear?\nOther evidence of the importance of these two factors comes from our current understanding of what limits the growth and distribution of coral reefs worldwide. The current distribution of carbonate coral reefs around the world today is associated with concentrations of carbonate ions of 200 µmol per kilogram water or more. In this case, there is a natural gradient towards reduced carbonate ion concentrations and more acidic oceans at higher latitudes. This is primarily due to the fact that cold water can contain a lot more carbon dioxide and water.\nThe significance of this threshold for carbonate ions is that these are the concentrations that you get in tropical oceans when carbon dioxide increases above 450 ppm. Given that these levels of carbon dioxide in the atmosphere are likely to be associated with at least a 2°C increase in sea temperature, it appears that coral reefs will largely disappeared if atmospheric concentrations of carbon dioxide exceed 450 ppm. The full background to this argument can be found in the following review paper in Science magazine: (Hoegh-Guldberg et al. 2007). In terms of the distribution of waters that will have the right chemistry for carbonate coral reef ecosystems, the following set of diagrams from this paper outlines how these change as atmospheric levels of CO2 increase (number in top left hand side of each panel). The blue water in these diagrams is essentially that required for carbonate reef ecosystems (which are represented by the pink dots).\nFigure 9. Changes in ocean chemistry is a function of atmospheric CO2 concentration. See Hoegh-Guldberg et al. (2007) for further details.\nCorals survived in past climates with warmer temps and higher CO2 than now, so shouldn't GBR survive now?\nThe Great Barrier Reef has undergone major transformations in the geological past. During the glacial periods when large amounts of water have been locked up in glaciers and landlocked ice masses, sea levels were over 100 m lower than they are today. This meant that the Great Barrier Reef lagoon was completely exposed and was a wooded grassland not too dissimilar to those found in Queensland today. All of the islands and reef structures that remained after the water retreated were covered with soil and were unrecognisable as coral reefs.\nThis has prompted some to ask: Why are we so worried about climate change today if the Great Barrier Reef has undergone these types of changes in the past?\nThe answer to this question is straightforward and comes down to the issue of the rate of change. The changes in temperature and sea level associated with glacial cycles occurred over 10-20,000 years. In this period of time, global temperatures changed by 5-8 degrees Celsius and atmospheric concentrations of carbon dioxide by 100 ppm. If we compare these sets of changes to that going on today, we see that we are undergoing similar set of changes only we are undertaking them in less than 100 years. That essentially means that the rate of change today is about 100-200 times faster than in the past.\nFigure 10. Changes in the Great Barrier Reef and Queensland coastline between glacial and interglacial periods. During last glacial period, sea levels were 100 m lower than they are today, exposing a large part of the Great Barrier Reef which subsequently covered by grasslands and forests.\nWhile some changes in the past were relatively rapid, animals and plants like those associated with the Great Barrier Reef had time to shift their geographic distribution as the environments changed. Today, there are clear signs that the speed of change is exceeding the ability of organisms and ecosystems to shift. The Great Barrier Reef provides an instructive example of how fast reef ecosystems need to migrate if they were to keep pace with changing climate. Corals at the northern end are adapted to warmer temperatures (about 3°C) than those in the southern part of the Great Barrier Reef. If corals were to move southward at a rate which matched an increase in sea temperature of 3°C by 2100, they would necessarily have to travel the entire length of the Great Barrier Reef (2500 km) within approximately 100 years. That means that reef ecosystems would have to travel at the rate of 25 km per year. There is very little evidence that complex ecosystems such as the Great Barrier Reef would be able to travel at this impossibly high rate of movement of the environmental conditions necessary for Coral Reef development.\nThe final point that needs to be made with respect to the question of why we are worried today if coral reefs of undergone big changes in the past is that of timescale. When coral reefs have experience calamities in the past, they have been absent for very long periods (i.e. millions of years). These timescales are well beyond those that make any sense with respect to humans and their dependence on the ecological services provided by coral reefs.\nPutting it bluntly, it is irrelevant that reefs have bounced back in geological time when the timescale of importance for the people depend on coral reefs (such as our vibrant tourist industry here in Australia, or the millions of people around the world need them for their food) is on a month to year basis.\nReferences:\nAnthony, K. R., D. I. Kline, G. Diaz-Pulido, S. Dove, and O. Hoegh-Guldberg, 2008: Ocean acidification causes bleaching and productivity loss in coral reef builders. Proc Natl Acad Sci U S A, 105, 17442-17446.\nBellwood, D. R., T. P. Hughes, C. Folke, and M. Nyström, 2004: Confronting the coral reef crisis. Nature, 429, 827-833.\nBerkelmans, R., 2002: Time-integrated thermal bleaching thresholds of reefs and their variation on the Great Barrier Reef. Marine ecology progress series, 229, 73-82.\nBerkelmans, R., and J. Oliver, 1999: Large-scale bleaching of corals on the Great Barrier Reef. Coral Reefs, 18, 55-60.\nBerkelmans, R., G. De’ath, S. Kininmonth, and W. J. Skirving, 2004: A comparison of the 1998 and 2002 coral bleaching events on the Great Barrier Reef: spatial correlation, patterns, and predictions. Coral Reefs, 23, 74-83.\nBruno, J., C. Siddon, J. Witman, P. Colin, and M. Toscano, 2001: El Nino related coral bleaching in Palau, western Caroline Islands. Coral Reefs, 20, 127-136.\nBruno, J. F., and E. R. Selig, 2007: Regional decline of coral cover in the Indo-Pacific: timing, extent, and subregional comparisons. PLoS ONE. , 2, e711.\nBuddemeier, R., and D. Fautin, 1993: Coral bleaching as an adaptive mechanism. Bioscience, 43, 320-326.\nDe'ath, G., J. M. Lough, and K. E. Fabricius, 2009: Declining Coral Calcification on the Great Barrier Reef. Science, 323, 116-119.\nDone, T., P. Whetton, R. Jones, R. Berkelmans, J. Lough, W. Skirving, and S. Wooldridge, 2003: Global Climate Change and Coral Bleaching on the Great Barrier Reef. Final Report to the State of Queensland Greenhouse Taskforce through the Department of Natural Resources and Minings, QDNRM, Brisbane. http://www.nrm.qld.gov.au/science/pdf/barrier_reef_report_1.pdf and http://www.nrm.qld.gov.au/science/pdf/barrier_reef_report_2.pdf, Q. G. Department of Natural Resources, Ed.\nDonner, S. D., W. J. Skirving, C. M. Little, M. Oppenheimer, and O. Hoegh-Guldberg, 2005: Global assessment of coral bleaching and required rates of adaptation under climate change. Glob Change Biol, 11, 2251-2265.\nDove, S. G., and O. Hoegh-Guldberg, 2006: The cell physiology of coral bleaching. Coral Reefs & Climate Change: Science and Management, 1-18.\nGlynn, P. W., 1983: Extensive Bleaching and Death of Reef Corals on the Pacific Coast of Panama. Environmental Conservation, 10, 149-154.\nGlynn, P. W., 1993: Coral reef bleaching: ecological perspectives. Coral Reefs, 12, 1-17.\nGlynn, P. W., 1996: Coral reef bleaching: facts, hypotheses and implications.\nGlynn, P. W., and L. Dcroz, 1990: Experimental-Evidence for High-Temperature Stress as the Cause of El-Nino-Coincident Coral Mortality. Coral Reefs, 8, 181-191.\nHendriks, I., C. Duarte, and M. Álvarez, 2009: Vulnerability of marine biodiversity to ocean acidification: A meta-analysis. Estuarine, Coastal and Shelf Science.\nHoegh-Guldberg, O., 1999: Climate change, coral bleaching and the future of the world's coral reefs. Mar Freshw Res, 50, 839-866.\nHoegh-Guldberg, O., and G. J. Smith, 1989: The effect of sudden changes in temperature, light and salinity on the population density and export of zooxanthellae from the reef corals Stylophora pistillata and Seriatopra hystrix. Journal of Experimental Marine Biology and Ecology, 129, 279-303.\nHoegh-Guldberg, O., R. J. Jones, S. Ward, and W. K. Loh, 2002: Communication arising. Is coral bleaching really adaptive? Nature, 415, 601-602.\nHoegh-Guldberg, O., and Coauthors, 2007: Coral reefs under rapid climate change and ocean acidification. Science, 318, 1737-1742.\nJones, R., O. Hoegh-Guldberg, A. Larkum, and U. Schreiber, 1998: Temperature induced bleaching of corals begins with impairment to the carbon dioxide fixation mechanism of zooxanthellae. Plant, Cell and Environment, 21, 1219-1230.\nJones, R. J., and O. Hoegh-Guldberg, 1999: Effects of cyanide on coral photosynthesis: implications for identifying the cause of coral bleaching and for assessing the environmental impacts of cyanide fishing. Marine Ecology Progress Series, 177, 83-91.\nKerswell, a., and R. Jones, 2003: Effects of hypo-osmosis on the coral Stylophora pistillata: nature and cause of'low-salinity. Marine Ecology Progress Series, 2003 ;0(0):0.\nKleypas, J. A., and C. Langdon, 2006: Coral reefs and changing seawater chemistry, Chapter 5 Coral Reefs and Climate Change: Science and Management. AGU Monograph Series, Coastal and Estuarine Studies, J. Phinney, O. Hoegh-Guldberg, J. Kleypas, W. Skirving, and A. E. Strong, Eds., Geophys. Union, 73-110.\nKleypas, J. A., R. A. Feely, V. J. Fabry, C. Langdon, C. L. Sabine, and R. R. Robbins, 2006: Impacts of ocean acidification on coral reefs and other marine calcifiers: A guide for future research, Report of a workshop held 18-20 April 2005, St Petersburg, FL, sponsored by NSF, NOAA and the US Geological Survey, 88pp.\nKoop, K., and Coauthors, 2001: ENCORE: The Effect of Nutrient Enrichment on Coral Reefs. Synthesis of Results and Conclusions. Marine Pollution Bulletin, 42, 91-120.\nMcCulloch, M., S. Fallon, T. Wyndham, E. Hendy, J. Lough, and D. Barnes, 2003: Coral record of increased sediment flux to the inner Great Barrier Reef since European settlement. Nature, 421, 727-730.\nOliver, J. K., R. Berkelmans, and C. M. Eakin, 2009: Coral Bleaching in Space and Time. Coral Bleaching, 21-39.\nPandolfi, J. M., and Coauthors, 2003: Global trajectories of the long-term decline of coral reef ecosystems. Science, 301, 955.\nRaven, J., and Coauthors, 2005: Ocean acidification due to increasing atmospheric carbon dioxide.\nReaka-Kudla, M. L., 1997: Global biodiversity of coral reefs: a comparison with rainforests. Biodiversity II: Understanding and Protecting Our Biological Resources, M. L. Reaka-Kudla, and D. E. Wilson, Eds., Joseph Henry Press, 551.\nSweatman, H., S. Delean, and C. Syms, 2011: Assessing loss of coral cover on Australia’s Great Barrier Reef over two decades, with implications for longer-term trends. Coral Reefs, 2, 521-531.\nTanzil, J., B. Brown, A. Tudhope, and R. Dunne, 2009: Decline in skeletal growth of the coral Porites lutea from the Andaman Sea, South Thailand between 1984 and 2005. Coral Reefs, DOI 10.1007/s00338-008-0457-5.\nWachenfeld, D., 1997: Long-term trends in the status of coral reef-flat benthos-The use of historical photographs. 134-148.\nAdvanced rebuttal written by Ove Hoegh-Guldberg\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nAdditional videos from the MOOC\nExpert interview with Annamieke Van De Heuvel\nExpert interview with Gregg Webb\nLast updated on 6 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Greenhouse effect has been falsified",
   "paragraph": "Has the greenhouse effect been falsified?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe greenhouse effect is standard physics and confirmed by observations.\nClimate Myth...\nGreenhouse effect has been falsified\n\"[T]he influence of so-called greenhouse gases on near-surface temperature - is not yet absolutely proven. In other words, there is as yet no incontrovertible proof either of the greenhouse effect, or its connection with alleged global warming.\nThis is no surprise, because in fact there is no such thing as the greenhouse effect: it is an impossibility. The statement that so-called greenhouse gases, especially CO2, contribute to near-surface atmospheric warming is in glaring contradiction to well-known physical laws relating to gas and vapour, as well as to general caloric theory.' (Heinz Thieme)\nSome climate change skeptics dispute the so-called ‘greenhouse effect’, which keeps the surface temperature of the Earth approximately 33 degrees C warmer than it would be if there were no greenhouse gases in the atmosphere. In other words, without the greenhouse effect, the Earth would be largely uninhabitable.\nHow do we know for sure this effect is real? The principle is demonstrated through basic physics, because a bare rock orbiting the sun at the distance of the Earth should be far colder than the Earth actually is. The explanation for this observation was based on the work of John Tyndall, who discovered in 1859 that several gases, including carbon dioxide and water vapour, could trap heat. This was the first evidence for what we know now as greenhouse gases. Then, towards the end of the same century, a Swedish scientist named Svante Arrhenius proved the relationship between greenhouse gas concentrations and surface temperatures.\nEmpirical Evidence for the Greenhouse Effect\nWe only have to look to our moon for evidence of what the Earth might be like without an atmosphere that sustained the greenhouse effect. While the moon’s surface reaches 130 degrees C in direct sunlight at the equator (266 degrees F), when the sun ‘goes down’ on the moon, the temperature drops almost immediately, and plunges in several hours down to minus 110 degrees C (-166F).\nSince the moon is virtually the same distance from the sun as we are, it is reasonable to ask why at night the Earth doesn’t get as cold as the moon. The answer is that, unlike the Earth, the moon has no water vapour or other greenhouse gases, because of course it has no atmosphere at all. Without our protective atmosphere and the greenhouse effect, the Earth would be as barren as our lifeless moon; without the heat trapped overnight in the atmosphere (and in the ground and oceans) our nights would be so cold that few plants or animals could survive even a single one.\nThe most conclusive evidence for the greenhouse effect – and the role CO2 plays – can be seen in data from the surface and from satellites. By comparing the Sun’s heat reaching the Earth with the heat leaving it, we can see that less long-wave radiation (heat) is leaving than arriving (and since the 1970s, that less and less radiation is leaving the Earth, as CO2 and equivalents build up). Since all radiation is measured by its wavelength, we can also see that the frequencies being trapped in the atmosphere are the same frequencies absorbed by greenhouse gases.\nDisputing that the greenhouse effect is real is to attempt to discredit centuries of science, laws of physics and direct observation. Without the greenhouse effect, we would not even be here to argue about it.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "Greenland has only lost a tiny fraction of its ice mass",
   "paragraph": "Why Greenland's ice loss matters\nLink to this page\nWhat the science says...\nMultiple lines of evidence indicate Greenland's ice loss is accelerating and will contribute sea level rise in the order of metres over the next few centuries.\nClimate Myth...\nGreenland has only lost a tiny fraction of its ice mass\n'Greenland is losing about 0.007% of its total mass every year … seven thousandths of one percent lost annually, be still, my beating heart … And if that terrifying rate of loss continues unabated, of course, it will all be gone in a mere 15,000 years.' (Willis Eschenbach)\nA number of independent lines of evidence show that overall, the Greenland ice sheet is losing ice. In fact, the rate of ice loss is accelerating. In light of this unequivocal evidence, a follow-up argument is \"okay, it's happening but it's not so bad. Greenland is losing 286 gigatonnes of ice per year? There's around 3 million gigatonnes still remaining in the huge ice sheet.\"\nIn On Being the Wrong Size, Willis Eschenbach argues that Greenland is losing about 0.007% of its total mass every year. At that rate, it will take 15,000 years to dissipate. Here is a visual comparison of 2009 ice loss to the total ice sheet:\nThe important point to remember here is that ice loss is accelerating. In 2002, the ice loss was 137 gigatonnes per year (Velicogna 2009). At that rate, the ice sheet would take nearly 22,000 years to dissipate. By 2009, this rate had more than doubled to 286 gigatonnes per year, reducing the ice sheet \"lifetime\" to 10,500 years. As the rate of ice loss increases, the ice sheet's lifetime is also diminishing.\nSo the crucial question is how will the Greenland ice sheet behave in the future? There are several different ways to approach the problem. One method is to study the physics of glacier movements. One paper calculates glacier dynamics factoring Greenland's topography, the cross-sectional area of its glaciers and whether the bedrock is based below sea level (Pfeffer 2008). Including contributions from Greenland and Antarctica, the study estimates global sea level rise between 80 cm to 2 metres by 2100.\nA semi-empirical technique looks at how sea level and global temperature have changed in the past (Vermeer 2009). Sea level change can then be expressed as a function of temperature change and future projections of global temperature can be used to simulate future sea levels. This method predicts global sea level rise of 75cm to 180cm by 2100.\nClimate modelling of the Greenland ice sheet predicts eventual collapse of the Greenland ice sheet if CO2 levels go over 400 parts per million (ppm). We're currently at 392 ppm. At 400 ppm, they predict that over the next 400 years, the ice sheet will lose between 20 to 41% of its volume (Stone 2010). This is equivalent to roughly 1.4 to 2.8 metres of sea level rise just from Greenland.\nLastly, we learn much about how the Greenland ice sheet behaves by looking at sea level change in the past. The more optimistic IPCC emission scenarios predict warming of 1 to 2°C. The last time temperatures were this warm was 125,000 years ago. At this time, sea levels were over 6 metres higher than current levels (Kopp 2009). This tells us the Greenland and Antarctic ice sheets are highly sensitive to sustained, warmer temperatures and are likely to contribute sea level rise measured in metres in future centuries.\nThe vast amount of ice still left in the Greenland ice sheet is a vivid reminder of Greenland's potential to contribute significantly to sea level rise in the future. And multiple lines of peer-reviewed evidence, both modelled and empirical, all paint a similar picture. The Greenland ice sheet is highly sensitive to warmer temperatures and is likely to contribute sea level rise in the order of metres over the next few centuries.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nAdditional video from the MOOC\nInterviews with various experts\nLast updated on 8 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Greenland ice sheet won't collapse",
   "paragraph": "The past tells us Greenland's ice sheet is vulnerable to global warming\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nWhen Greenland was 3 to 5 degrees C warmer than today, a large portion of the Ice Sheet melted.\nClimate Myth...\nGreenland ice sheet won't collapse\n\"A July 6, 2007 study published in the journal Science about Greenland by an international team of scientists found DNA “evidence that suggests the frozen shield covering the immense island survived the Earth’s last period of global warming,” according to a Boston Globe Article. ... [T]he study indicates “Greenland’s ice may be less susceptible to the massive meltdown predicted by computer models of climate change, the main author ... said in an interview. ... The study found “Greenland really was green, before Ice Age glaciers enshrouded vast swaths of the Northern Hemisphere…somewhere between 450,000 and 800,000 years ago,” according to the article. (Marc Morano, discussing the views of Eske Willerslev)\nClimate change skeptics like Marc Morano employ gross exaggeration to dismiss or diminish the potential disruption that climate change is likely to bring about. In the Inhofe EWP press blog, Morano made much of this statement:\n“...evidence that suggests the frozen shield covering the immense island survived the Earth’s last period of global warming”\nIrrespective of what it means to claim the ice sheet ‘survived’ (a rather unqualified claim since survival could be taken to mean that 99% or 1% of the ice was left), it is generally recognised that a complete melt-down of the Greenland ice sheet is far less likely than partial melting. The time-scales over which such a dramatic and complete failure could occur must also be reckoned in centuries rather than decades. Given how much uncertainty surrounds even the accurate measurement of negative mass balance (how much the ice is reducing per year), projections on the century scale are too speculative to be helpful when considering the current problem, which is sea level rise.\nSea level rise will depend on how much water is currently held in the Greenland ice sheet, because the sheer volume of water is so immense that even a small loss of ice will produce considerable rises in sea level – and concerns about loss of mass from ice sheets focuses on sea level rise because this is one of the most serious threats climate change may invoke.\nSo let’s consider the ice sheets, individually and collectively. Estimates suggest that if the Greenland ice sheet was to melt away to nothing, sea levels would rise around 7 metres. To put that a different way, a loss of just one percent of the ice cap would result in a sea level rise of 7cm. Consider this in context: if the West Antarctic Ice Sheet (WAIS) were to melt, this would add around 6 metres to sea levels. If the East Antarctic Ice Sheet (EAIS) were to melt, seas would rise by around 70 metres. So a mere 1% loss of ice from these three sources would produce a likely increase in sea levels of around 83cm - from these ice formations alone.\nIt is important when considering the impact of ice sheet mass balance to bear in mind that a global phenomenon like climate change will produce negative mass balance at both poles, and the shrinking glaciers will also contribute to sea level increases.\nWhile the complete disappearance of Greenland’s ice sheet is hard to predict and the probability lower than a partial collapse, it is clear that even a relatively small loss of ice through melting will produce considerable, and very disruptive, increases in sea levels.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 10 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Greenland is gaining ice",
   "paragraph": "Is Greenland gaining or losing ice?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nGreenland on the whole is losing ice, as confirmed by multiple satellite and on the ground field measurements.\nClimate Myth...\nGreenland is gaining ice\n“[E]ven if it were true that Greenland’s ice had been melting at ‘new record’ rates, after seven and a half years of global cooling global warming cannot be the cause. The true position in Greenland is to be found in Johannessen et al. (2005), where satellite altimetry established that the mean thickness of the entire Greenland ice sheet had increased at 2 inches per year – a total of almost 2 feet – in the 11 years 1993-2003.” (Christopher Monckton)\nConfusion caused by anecdotes of structures being buried by accumulating snow on Greenland's ice sheet leads some skeptics to believe Greenland is Gaining Ice. As always, the best way to tease out the truth here by following the research of scientists investigating Greenland's ice mass balance.\nIn general, the best available science tells us that Greenland is losing ice extensively (Figure 1) and that these losses have drastically increased since the year 2000.\nFigure 1: Estimated Greenland Ice Sheet mass balance changes since 1950 using three different methods (Jiang 2010). Mass Balance Measurement Techniques are discussed here.\nThe evidence suggested by a multitude of different measurement techniques suggests that not only is Greenland losing ice but that these ice losses are accelerating at a rapid pace (Velicogna 2009). Further evidence suggests that although ice losses have up to this point primarily occurred in the South and Southwest portions of Greenland, these losses are now spreading to the Northwest sector of the ice sheet (Khan et al 2010).\nAlthough there have been some gains at high altitudes, significant ice losses are occurring at low altitudes (Wouters 2008) along the coastline where glaciers are calving ice into the oceans far quicker than ice is being accumulated at the top of the ice sheet (Rignot and Kanagaratnam 2006).\nIn conclusion Greenland is losing ice extensively along its margins where fast flowing ice streams are pushing more ice into the ocean than is gained in the center of the ice sheet. For more information on how ice sheets lose mass, a more comprehensive discussion is available here.\nBasic rebuttal written by Robert way\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 27 August 2018 by pattimer. View Archives"
  },
  {
   "title": "Greenland was green",
   "paragraph": "Greenland used to be green\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe Greenland ice sheet is at least 400,000 years old and warming was not global when Europeans settled in Greeland 1,000 years ago\nClimate Myth...\nGreenland was green\n“CfA's Sallie Baliunas […] refers to the medieval Viking sagas as examples of unusual warming around 1003 A.D. ‘The Vikings established colonies in Greenland at the beginning of the second millennium, but they died out several hundred years later when the climate turned colder,’ she notes.” (William Cromie)\nGreenland is a large area situated east of Canada, between the Arctic and Atlantic oceans. About 80% of the island is covered by the Greenland ice sheet. During the 980s, Scandinavian and Icelandic exporers established two or three settlements on the south-west coast of Greenland. So what were the conditions in Greenland like 1,000 years ago? More precisely, let's explore the three following questions:\nHow old is the Greenland ice sheet?\nIs there evidence of global warming at that time?\nWhat factors cause climate change?\nThe Greenland ice sheet is at least 400,000 years old\nScientists have estimated that the Greenland ice sheet is between 400,000 and 800,000 years old. This means that the island today is unlikely to have been markedly different when Europeans settled there. However, there is evidence that the settled areas were warmer than today, with large birch woodlands providing both timber and fuel. This warmth coincided with the period known as the Medieval Climatic Anomaly, also known as the Medieval Warm Period, which we will discuss below.\nSo how did Greenland get its name? According to the Icelandic sagas, Erik the Red named it Greenland in an attempt to lure settlers in search of land and the promise of a better life. However, the age of the ice sheet, which is more than 3 kilometres thick in places and covers 80% of Greenland, proves that the opportunities to establish communities would have been limited to rather small areas.\nWarming during the Medieval Climatic Anomaly was not global\nDuring the Medieval Climatic Anomaly, some areas, most notably in the North Atlantic and parts of Europe, were at least as warm as today, if not warmer. However, other areas were colder, and overall evidence suggests that global temperatures during this period were similar to those at the beginning or middle of the 20th century, and colder than today. This period is explored in more depth here.\nSo not only was Greenland already mostly covered in ice when Europeans settled there, but also the relatively warm conditions during this period were not a global phenomenon. This contrasts with what we are seeing today, where warming is truly global. Figure 1 is a map showing reconstructions of temperature anomalies during the Medieval Warm Period. Blue colours show lower temperatures and warm colours show higher temperatures when compared to the 1961-1990 reference period.\nFigure 1 - Reconstructed surface temperature anomalies for the Medieval Warm Period (950-1250) compared to a 1961-1990 reference period. (Source: Mann et al., 2009)\nWe can compare this with a similar reconstruction looking at surface temperature anomalies for the 1999 to 2008 period. This clearly shows the global nature of recent warming.\nFigure 2 - Surface temperature anomaly for period 1999 to 2008, relative to the 1961– 1990 reference period. (Source: NOAA)\nNatural versus man-made climate change\nWarming can be the result of a number of factors, so that the cause of past climate change is not necessarily implicated in current climate change. For instance, the Medieval Climatic Anomaly was characterised by relatively high solar activity, low volcanic activity and possible changes in ocean circulation patterns. These factors can explain both the scale and pattern of warmth at that time. However, they cannot explain recent warming. More to the point, changes in natural factors would probably have led to cooling in the past few decades. This contrasts with the multiple lines of evidence pointing to the role played by humans in recent warming, as illustrated by the the graph below.\nConclusion\nGreenland is unlikely to have been radically different 1,000 years ago since the ice sheet is at least 400,000 years old. So the evidence shows that not only was Greenland not green, the warmth was mainly a regional phenomenon caused by natural factors. Compare this with the unequivocal findings of the scientific community regarding ongoing warming: climate change now is global and in all likelihood driven primarily by human activities.\nThe key points can be summarised as follows:\nThe Greenland ice sheet already covered large sections of Greenland when Europeans established communities there 1,000 years ago\nWarming was not global during the Medieval Climatic Anomaly; average global temperatures were lower than today\nNatural factors behind regional warming in medieval Greenland are probably not responsible for today's global warming\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 15 May 2016 by pattimer. View Archives"
  },
  {
   "title": "Hansen predicted the West Side Highway would be underwater",
   "paragraph": "Examining Hansen's prediction about the West Side Highway\nLink to this page\nWhat the science says...\nHansen was speculating on changes that might happen if CO2 doubled.\nClimate Myth...\nHansen predicted the West Side Highway would be underwater\n'What I can comment on is this prediction by Dr. Hansen: “The West Side Highway [which runs along the Hudson River] will be under water.” As you can clearly see in the Google Earth images, the West Side Highway remains dry and open. Sea level (at which the Hudson River at that point becomes) is not encroaching on the highway' (Anthony Watts)\nJames Hansen made his statement in response to a question by Bob Reiss, a journalist and author, in 1988. He did not predict that the West Side Highway would be underwater in 20 years.\nBob Reiss reports the conversation as follows:\n\"When I interviewe­­d James Hansen I asked him to speculate on what the view outside his office window could look like in 40 years with doubled CO2. I'd been trying to think of a way to discuss the greenhouse effect in a way that would make sense to average readers. I wasn't asking for hard scientific studies. It wasn't an academic interview. It was a discussion with a kind and thoughtful man who answered the question. You can find the descriptio­­n in two of my books, most recently The Coming Storm.\"\nJames Hansen reports the conversation as follows:\n\"Reiss asked me to speculate on changes that might happen in New York City in 40 years assuming CO2 doubled in amount.\"\nThe book The Coming Storm and the salon.com article are different. In The Coming Storm the question includes the conditions of doubled CO2 and 40 years, while the salon.com article which is quoted by skeptics does not mention doubled CO2, and involves only 20 years.\nTo understand the discrepancy between these two published accounts, it helps to look at the timeline of events. The original conversation was in 1988. Ten years later, referring to his notes, Bob Reiss recounted the conversation in his book The Coming Storm. James Hansen confirmed the conversation and said he would not change a thing he said. After the book was published, Bob Reiss was talking to a journalist at salon.com about it. As he puts it,\n\"although the book text is correct, in remembering our original conversation, during a casual phone interview with a Salon magazine reporter in 2001 I was off in years.”\nWe can check back in 2028, the 40 year mark, and also when and if we reach 560 ppm CO2 (a doubling from pre-industrial levels). In the meantime, we can stop using this conversation from 1988 as a reason to be skeptical about the human origins of global warming.\nReferences:\nThe Coming Storm by Bob Reiss, copyright 2001\nBook review in Salon. Com: http://dir.salon.com/books/int/2001/10/23/weather/index.html\nAs reported by Anthony Watts:\nCommunication from James Hansen, January 26, 2011\nEmail from Bob Reiss, February 15, 2011\nLast updated on 28 February 2011 by ClimateHawk."
  },
  {
   "title": "Hansen's 1988 prediction was wrong",
   "paragraph": "What do we learn from James Hansen's 1988 prediction?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nHansen's 1988 results are evidence that the actual climate sensitivity is about 3°C for a doubling of atmospheric CO2.\nClimate Myth...\nHansen's 1988 prediction was wrong\n'On June 23, 1988, NASA scientist James Hansen testified before the House of Representatives that there was a strong \"cause and effect relationship\" between observed temperatures and human emissions into the atmosphere. At that time, Hansen also produced a model of the future behavior of the globe’s temperature, which he had turned into a video movie that was heavily shopped in Congress. That model predicted that global temperature between 1988 and 1997 would rise by 0.45°C (Figure 1). Ground-based temperatures from the IPCC show a rise of 0.11°C, or more than four times less than Hansen predicted. The forecast made in 1988 was an astounding failure, and IPCC’s 1990 statement about the realistic nature of these projections was simply wrong.' (Pat Michaels)\nIn 1988, James Hansen projected future warming trends. He used 3 different scenarios, identified as A, B, and C. Each represented different levels of greenhouse gas emissions. Scenario A assumed greenhouse gas emissions would continue to accelerate. Scenario B assumed a slowing and eventually constant rate of growth. Scenario C assumed a rapid decline in greenhouse gas emissions around the year 2000. The actual greenhouse gas emissions since 1988 have been closest to Scenario B. As shown below, the actual warming has been less than Scenario B.\nFigure 1: Global surface temperature computed for scenarios A, B, and C, compared with observational data\nAs climate scientist John Christy noted, \"this demonstrates that the old NASA [global climate model] was considerably more sensitive to GHGs than is the real atmosphere.\" However, Dr. Christy did not investigate why the climate model was too sensitive. There are two main reasons for Hansen's overestimate:\nScenario B, which was the closest to reality, slightly overestimated how much the atmospheric greenhouse gases would increase. This isn't just carbon dioxide. It also includes methane and chlorofluorocarbons (CFCs).\nHansen's climate model had a rather high climate sensitivity parameter. Climate sensitivity describes how sensitive the global climate is to a change in the amount of energy reaching the Earth's surface and lower atmosphere.\nIf we take into account the lower atmospheric greenhouse gas increases, we can compare the observed versus projected global temperature warming rates, as shown in the Advanced version of this rebuttal. To accurately predict the global warming of the past 22 years, Hansen's climate model would have needed a climate sensitivity of about 3.4°C for a doubling of atmospheric CO2. This is within the likely range of climate sensitivity values listed as 2-4.5°C by the IPCC for a doubling of CO2. It is even a bit higher than the most likely value currently widely accepted as 3°C.\nIn short, the main reason Hansen's 1988 warming projections were too high is that he used a climate model with a high climate sensitivity. His results are actually evidence that the true climate sensitivity parameter is within the range accepted by the IPCC.\nBasic rebuttal written by John Cook\nNov. 13, 2017 - updated graphic with data through 2016 (BaerbelW)\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 14 November 2017 by pattimer. View Archives"
  },
  {
   "title": "Heatwaves have happened before",
   "paragraph": "Global warming is increasing the risk of heatwaves\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nGlobal warming is increasing the frequency, duration and intensity of heatwaves.\nClimate Myth...\nHeatwaves have happened before\n\"Australia has always had extreme heat, droughts, bushfire and flooding rains... Whatever is the extent of global warming and any human contribution to climate change, exaggerating the 2013 heatwave is just another green lie which will blow up in all our faces.\" (Miranda Devine)\nGlobal warming is causing more frequent heatwaves. Record-breaking temperatures are already happening five times more often than they would without any human-caused global warming. This means that there is an 80% chance that any monthly heat record today is due to human-caused global warming.\nFigure 1: Likelihood of heat records compared to those expected in a world without global warming.\nWhat will heatwaves be like in the future? If we contine to rely heavily on fossil fuels, extreme heatwaves will become the norm across most of the world by the late 21st century. However, if we take major steps to reduce human greenhouse gas emissions, the number of extreme heatwaves will stabilize after 2040. Either way, we will see more heatwaves, but how much more depends on us.\nHowever, the growing risk from heatwaves is ignored by some who argue that heatwaves have happened in the past, hence current heatwaves must be natural. This line of argument is logically flawed, using a logical fallacy called a non sequitor (Latin for 'it does not follow'). This is a fallacy where your starting statement does not lead to your conclusion. For example, this is like arguing that people have died of cancer long before cigarettes were invented, hence smoking can't cause cancer.\nThe longer we continue to rely on fossil fuels and the higher our greenhouse gas emissions, the more extreme heat we'll lock in. If we manage to take serious action to reduce our greenhouse gas emissions, we can limit global warming to a level where extreme heat events will become more commonplace, but we can manage to adapt to.\nBasic rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Himalayan glaciers are not shrinking",
   "paragraph": "Shrinking Himalayan glaciers\nLink to this page\nWhat the science says...\nHimalayan glaciers supporting hundreds of millions of people are showing consistent loss of ice.\nClimate Myth...\nHimalayan glaciers are not shrinking\n\"...the [Himalayan] glaciers are not shrinking. A new report by a senior Indian glaciologist states that the glaciers remain frozen and quite intact, thank you.\"\n(Doug Hoffman)\nThe lofty mountains at the Roof of the World, the Himalayas, harbor many strange and mythical things. From the Abominable Snowman (myth) to the magnificent Potala Palace (wondrously real), to the far horizon of lost Shambhala (mythical [?] Shangri-La) to the manifold tales of the Vedas (also real), many a fanciful tale emerge to titillate the uninitiated. To this we can add the \"fake-skeptic\" meme that Himalayan glaciers are growing, as recent scientific assessments belie that unsupported assertion.\nFig 1. Pohtala Palace\nIn the high mountains of Central Asia, detailed glacier mapping inventories, from GLIMS: (Global Land Ice Measurements from Space), ICIMOD (International Centre for Integrated Mountain Development) and ISRO ( Indian Space Research Organization) of thousands of glaciers have indicated increased strong thinning and area loss since 2000 throughout the region except the Karakoram. The inventories rely on repeat imagery from ASTER, Corona, Landsat, IKONOS and SPOT imagery.\nFig 2. Area of the Himalaya (background image courtesy NOAA, cartography by Daniel Bailey). Really super-big version here.\nGlacier change in Himalaya, Altai, Tien Shan, Pamir and Qilian Ranges\nIn the Russian Altai, mapping of 126 glaciers indicates a 19.7 % reduction in glacier area 1952-2004, with a sharp increase after 1997 (Shahgedanova et al., 2010). In Garhwal Himalaya, India, of 58 glaciers examined from 1990-2006 area loss was 6% (Bhambri et al, 2011). They also noted the number of glaciers increased from 69 (1968) to 75 (2006) due to the disintegration of ice bodies. Examination of 466 glaciers in the Chenab, Parbati and Baspa Basin, India found a 21% decline in glacier area from 1962 to 2004 (Kulkarni, 2007). Glacier fragmentation was also observed in this study, which for some fragments represents a loss of the accumulation area, which means the glacier will not survive (Pelto, 2010). In the Nepal Himalaya area loss from 1963-2009 is nearly 20% (Bajracharya et al., 2011).\nFig 3. Map of the Karakoram and Himalaya showing the major river basins and the locations of measured rates of change in area and of a sample of glacier length change and mass budget measurements (Bolch et al, 2012)\nThe Langtang sub-basin is a small northeast-southwest elongated basin, tributary of Trishuli River north of Kathmandu and bordered with China to the north. The basin encompasses an area of 554 km2. The basin contained 192 km2 of glacier area in 1977, 171 km2 in 1988, 152 km2 in 2000 and 142 km2 in 2009. In 32 years from 1977 to 2009 the glacier area declined by 26% (Bajracharya et al., 2011). The only published catchment study indicates a 75% loss of ice by 2088 (Immerzeel, 2012).\nIn the Khumbu region, Nepal volume losses due to thickness changes increased from an average of 320 mm/yr 1962-2002 to 790 mm/yr from 2002-2007, including area losses at the highest elevation on the glaciers (Bolch et al., 2011). The high elevation loss is also noted in Tibet on Naimona’nyi Glacier which has not retained accumulation even at 6000 meters. This indicates a lack of high altitude snow-ice gain (Kehrwald et al, 2008).\nThe Dudh Koshi basin is the largest glacierized basin in Nepal. It has 278 glaciers of which 40, amounting to 70% of the area, are valley-type. Almost all the glaciers are retreating at rates of 10–59 m/year, but the rate has accelerated after 2001 (Bajracharya and Mool, 2009).\nIn the Tien Shan Range over 1700 glaciers were examined from 1970-2000 glacier area decreased by 13%, from 2000-2007 glacier area shrank by 4% a faster rate than from 1970-2000 (Narama et al, 2010). An inventory of 308 glaciers in the Nam Co Basin, Tibet, noted an increased loss of area for the 2001-2009 period, 6% area loss (Bolch et al., 2010). Zhou et al (2009) looking at the Nianchu River basin southern Tibet found a 5% area loss, 1990-2005. Cao et al (2010) completed an inventory of 244 glaciers in Lenglongling Range of Eastern Qilian Mountains from 1972 to 2007 and found a 23.5% loss in glacier area. The highest rate of 1% per year of area loss was identified from 2000 to 2007. Pan et al (2011) looking at the Gongga Mountains, China found a 11.3% area loss from 1966-2009.\nIn the Wakhan Corridor, Pamir Rage, Afghanistan 30 glaciers were examined over a 27 year period, 1976-2003, indicating that 28 of the glacier retreated with an average retreat of 294 m, just over 10 meters/yr (Haritashya et al, 2009).\nThe consistent picture that emerges is net ice loss in most parts of the Himalaya. Measurements suggest that the rate of loss has increased since about 1995 (Bolch et al, 2012).\nFig 4. The glacier retreat since the mid-19th century is obvious in the Himalaya, with the exception of the glaciers at Nanga Parbat in the northwest (RA, CL). Glaciers in the Karakoram show complex behavior (Bolch et al, 2012).\nThe Karakoram\nThe Karakoram is the one range where a mix of expansion and retreat is seen. The anomalous expansions are confined to the highest relief glaciers and appeared suddenly and sporadically (Hewitt, 2005). After decades of decline, glaciers in the highest parts of the central Karakoram expanded, advanced, and thickened in the late 1990s. Many of the largest glaciers in the Karakoram are still retreating, including the largest Baltoro, Panmah and Biafo Glacier, albeit slowly (Hewitt, 2011). Measurements indicate a possible mass-gain from 2002-2006 with a decrease thereafter (Bolch et al, 2012), (Cogley, 2012); the estimated contribution of the glaciers of the Karakoram to sea level rise is lower than previously suggested (Gardelle et al, 2012).\nFig 5. The Karakoram mountains in the western Himalayas as seen from a NASA satellite. New research published in the journal Nature Geoscience is showing that some of the glaciers in the region has experiences small gains in mass in the 21st century (Image courtesy CNN and NASA).\nGlacier Runoff Importance\nAbout 800 million people live in the watersheds of the Brahmaputra, Ganges and Indus rivers and rely (to various degrees) upon the water released from glaciers. The glaciers of the Himalaya are natural buffers of hydrological seasonality, releasing meltwater during summer and early autumn, in particular. This meltwater is a major source of stream flow in the Karakoram and the northwest parts of the Himalaya, which receive about two-thirds of their high-altitude snowfall from the westerly cyclones (mainly in winter).\nFig 6. Main wind systems (Bolch et al, 2012)\nIt is less important in the monsoon-dominated areas, such as the central and eastern Himalaya, which have abundant summer precipitation (during which they receive 80% of their yearly total).\nFig 7. Mean precipitation in January and July (Bolch et al, 2012)\nOverall Changes\nA new means of assessing glacier volume is the Gravity Recovery and Climate Experiment (GRACE), which cannot look at specific changes of individual glaciers or watersheds. In the high mountains of Central Asia, GRACE imagery found mass losses of -264 mm/a for the 2003-2009 period (Matsuo and Heki, 2010). This result is in relative agreement with the other satellite image assessments, but is at odds with the recent global assessment from GRACE (Jacobs et al, 2012), which estimated Himalayan glacier losses at 10% of that found in the aforementioned examples for volume loss for the 2003-2010 period. At this point the detailed inventories of thousands of glaciers are better validated and illustrate the widespread significant loss in glacier area and volume, though not all glaciers are retreating.\nTo sum up, Himalayan glaciers supporting hundreds of millions of people are showing consistent loss of ice, even in the Karakoram (but to a lesser degree here for a variety of reasons). When glacier ice is lost in the long-term, the annual water balance is affected.\nSummer runoff in the central and eastern Himalaya should be little affected, provided the monsoons still occur in their normal timings and intensities. Runoff in the remainder of the year is expected to decrease as glacial mass continues to decrease. Runoff from the Karakoram glaciers is not expected to diminish before the end of the 21st century, with the Indus River valley the primary beneficiary recipient (Rees and Collins, 2006).\nSo when you hear someone say \"the Himalayas are gaining ice\", remember to not check your skepticism at the door with the Yeti coat-check girl...\nRecommended Reading\nFrom a Glaciers Perspective\nHimalaya Glacier Index\nGlacier Index of Posts\nWorld Glacier Monitoring Service\nLast updated on 30 May 2012 by dana1981. View Archives"
  },
  {
   "title": "Hockey stick is broken",
   "paragraph": "What evidence is there for the hockey stick?\nLink to this page\nWhat the science says...\nSince the hockey stick paper in 1998, there have been a number of proxy studies analysing a variety of different sources including corals, stalagmites, tree rings, boreholes and ice cores. They all confirm the original hockey stick conclusion: the 20th century is the warmest in the last 1000 years and that warming was most dramatic after 1920.\nClimate Myth...\nHockey stick is broken\n“In 2003 Professor McKitrick teamed with a Canadian engineer, Steve McIntyre, in attempting to replicate the chart and finally debunked it as statistical nonsense. They revealed how the chart was derived from \"collation errors, unjustified truncation or extrapolation of source data, obsolete data, incorrect principal component calculations, geographical mislocations and other serious defects\" -- substantially affecting the temperature index.” (John McLaughlin)\nThe \"hockey stick\" describes a reconstruction of past temperature over the past 1000 to 2000 years using tree-rings, ice cores, coral and other records that act as proxies for temperature (Mann 1999). The reconstruction found that global temperature gradually cooled over the last 1000 years with a sharp upturn in the 20th Century. The principal result from the hockey stick is that global temperatures over the last few decades are the warmest in the last 1000 years.\nFigure 1: Northern Hemisphere temperature changes estimated from various proxy records shown in blue (Mann 1999). Instrumental data shown in red. Note the large uncertainty (grey area) as you go further back in time.\nA critique of the hockey stick was published in 2004 (McIntyre 2004), claiming the hockey stick shape was the inevitable result of the statistical method used (principal components analysis). They also claimed temperatures over the 15th Century were derived from one bristlecone pine proxy record. They concluded that the hockey stick shape was not statistically significant.\nAn independent assessment of Mann's hockey stick was conducted by the National Center for Atmospheric Research (Wahl 2007). They reconstructed temperatures employing a variety of statistical techniques (with and without principal components analysis). Their results found slightly different temperatures in the early 15th Century. However, they confirmed the principal results of the original hockey stick - that the warming trend and temperatures over the last few decades are unprecedented over at least the last 600 years.\nFigure 2: Original hockey stick graph (blue - MBH1998) compared to Wahl & Ammann reconstruction (red). Instrumental record in black (Wahl 2007).\nWhile many continue to fixate on Mann's early work on proxy records, the science of paleoclimatology has moved on. Since 1999, there have been many independent reconstructions of past temperatures, using a variety of proxy data and a number of different methodologies. All find the same result - that the last few decades are the hottest in the last 500 to 2000 years (depending on how far back the reconstruction goes). What are some of the proxies that are used to determine past temperature?\nChanges in surface temperature send thermal waves underground, cooling or warming the subterranean rock. To track these changes, underground temperature measurements were examined from over 350 bore holes in North America, Europe, Southern Africa and Australia (Huang 2000). Borehole reconstructions aren't able to give short term variation, yielding only century-scale trends. What they find is that the 20th century is the warmest of the past five centuries with the strongest warming trend in 500 years.\nFigure 3: Global surface temperature change over the last five centuries from boreholes (thick red line). Shading represents uncertainty. Blue line is a five year running average of HadCRUT global surface air temperature (Huang 2000).\nStalagmites (or speleothems) are formed from groundwater within underground caverns. As they're annually banded, the thickness of the layers can be used as climate proxies. A reconstruction of Northern Hemisphere temperature from stalagmites shows that while the uncertainty range (grey area) is significant, the temperature in the latter 20th Century exceeds the maximum estimate over the past 500 years (Smith 2006).\nFigure 4: Northern Hemisphere annual temperature reconstruction from speleothem reconstructions shown with 2 standard error (shaded area) (Smith 2006).\nHistorical records of glacier length can be used as a proxy for temperature. As the number of monitored glaciers diminishes in the past, the uncertainty grows accordingly. Nevertheless, temperatures in recent decades exceed the uncertainty range over the past 400 years (Oerlemans 2005).\nFigure 5: Global mean temperature calculated form glaciers. The red vertical lines indicate uncertainty.\nOf course, these examples only go back around 500 years - this doesn't even cover the Medieval Warm Period. When you combine all the various proxies, including ice cores, coral, lake sediments, glaciers, boreholes & stalagmites, it's possible to reconstruct Northern Hemisphere temperatures without tree-ring proxies going back 1,300 years (Mann 2008). The result is that temperatures in recent decades exceed the maximum proxy estimate (including uncertainty range) for the past 1,300 years. When you include tree-ring data, the same result holds for the past 1,700 years.\nFigure 6: Composite Northern Hemisphere land and land plus ocean temperature reconstructions and estimated 95% confidence intervals. Shown for comparison are published Northern Hemisphere reconstructions (Mann 2008).\nPaleoclimatology draws upon a range of proxies and methodologies to calculate past temperatures. This allows independent confirmation of the basic hockey stick result: that the past few decades are the hottest in the past 1,300 years.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nAdditional video from the MOOC\nInterviews with various experts\nLast updated on 12 October 2016 by pattimer. View Archives"
  },
  {
   "title": "Human CO2 is a tiny % of CO2 emissions",
   "paragraph": "How do human CO2 emissions compare to natural CO2 emissions?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe natural cycle adds and removes CO2 to keep a balance; humans add extra CO2 without removing any.\nClimate Myth...\nHuman CO2 is a tiny % of CO2 emissions\n“The oceans contain 37,400 billion tons (GT) of suspended carbon, land biomass has 2000-3000 GT. The atpmosphere contains 720 billion tons of CO2 and humans contribute only 6 GT additional load on this balance. The oceans, land and atpmosphere exchange CO2 continuously so the additional load by humans is incredibly small. A small shift in the balance between oceans and air would cause a CO2 much more severe rise than anything we could produce.” (Jeff Id)\nBefore the industrial revolution, the CO2 content in the air remained quite steady for thousands of years. Natural CO2 is not static, however. It is generated by natural processes, and absorbed by others.\nAs you can see in Figure 1, natural land and ocean carbon remains roughly in balance and have done so for a long time – and we know this because we can measure historic levels of CO2 in the atmosphere both directly (in ice cores) and indirectly (through proxies).\nFigure 1: Global carbon cycle. Numbers represent flux of carbon dioxide in gigatons (Source: Figure 7.3, IPCC AR4).\nBut consider what happens when more CO2 is released from outside of the natural carbon cycle – by burning fossil fuels. Although our output of 29 gigatons of CO2 is tiny compared to the 750 gigatons moving through the carbon cycle each year, it adds up because the land and ocean cannot absorb all of the extra CO2. About 40% of this additional CO2 is absorbed. The rest remains in the atmosphere, and as a consequence, atmospheric CO2 is at its highest level in 15 to 20 million years (Tripati 2009). (A natural change of 100ppm normally takes 5,000 to 20,000 years. The recent increase of 100ppm has taken just 120 years).\nHuman CO2 emissions upset the natural balance of the carbon cycle. Man-made CO2 in the atmosphere has increased by a third since the pre-industrial era, creating an artificial forcing of global temperatures which is warming the planet. While fossil-fuel derived CO2 is a very small component of the global carbon cycle, the extra CO2 is cumulative because the natural carbon exchange cannot absorb all the additional CO2.\nThe level of atmospheric CO2 is building up, the additional CO2 is being produced by burning fossil fuels, and that build up is accelerating.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 July 2015 by skeptickev. View Archives"
  },
  {
   "title": "Humans are too insignificant to affect global climate",
   "paragraph": "Are humans too insignificant to affect global climate?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nHumans are small but powerful, and human CO2 emissions are causing global warming.\nClimate Myth...\nHumans are too insignificant to affect global climate\nTo suggest that humanity is capable of impacting and disturbing forces of such magnitude is reflective of a self-centred arrogance that is mind numbing. Humanity is a subset of Nature. Nature is not a subset of humanity. We have travelled full circle. We are back in the mindset that prevailed when Society’s leaders dictated what people in Copernicus’ days may or may not think. The Earth is once again flat. (source: Financial Sense University)\nWhen we experience weather events like hurricanes and floods, it’s very easy for us to feel insignificant and powerless in the face of such massive natural forces. How can humans influence this? Well, yes, we can. Of course we can’t influence a single weather event, but we can and do have a long term influence on the climate that causes it.\nSince the industrial revolution, with ever-increasing supplies of fossil fuels, the activities of a dramatically expanding world population have made significant alterations to the make-up of our atmosphere.\nIn some cases human-caused change is direct and unambiguous. The harmful effect of the human release of CFCs on the ozone layer is well documented and not disputed. Down on the ground, draining of marshland and deforestation can produce a significant decrease in water vapour in the atmosphere downwind; while the introduction of irrigation for agriculture has the opposite effect. Over time, both of these human activities can alter patterns of rainfall, turning deserts into green areas and green areas into deserts.\nIn other cases the human causes of climate change are more complex. Emissions from cement production, pollution and the release of particulates to form smog in the atmosphere, all affect climate.\nWithout doubt the most significant of all the human causes of changing climate is the dramatic increase in CO2. After remaining relatively steady for the last 650,000 years or more, in just the last two hundred years the concentration of CO2 in the atmosphere has suddenly shot up from 280, to more than 380 parts per million. And it’s still rising. This dramatic 30% increase has all taken place at the same time as humans have been burning fossil fuels at a greater and greater rate.\nOf course there are also natural sources of the CO2 in the atmosphere, such as vegetation, but fortunately there are differences that scientists can measure between the CO2 derived from fossil fuels and the CO2 derived from plants. The changing concentrations of the two types demonstrate that the additional CO2 can only be the result of human activity.\nThis graph, based on the comparison of atmospheric samples contained in ice cores and more recent direct measurements, provides evidence that atmospheric CO2 has increased since the Industrial Revolution. (Credit: Vostok ice core data/J.R. Petit et al.; NOAA Mauna Loa CO2 record.)\nOf course, as CO2 is the most common of greenhouse gasses, the additional concentration is what causes most of the rise in temperature. This is resulting in a change in weather patterns and ocean currents; the melting of global ice formations; and an increase in extreme weather events.\nSo, yes; though we might be pretty helpless when it comes to controlling the weather, humans are certainly capable of changing the world’s climate.\nBasic rebuttal written by John Russell\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 30 November 2015 by MichaelK. View Archives"
  },
  {
   "title": "Humans survived past climate changes",
   "paragraph": "How past climate change impacted the human species\nLink to this page\nWhat the science says...\nHumans have been through climate changes before- but mostly cold ones and mostly in our far distant past.\nClimate Myth...\nHumans survived past climate changes\n\"Yes, our climates change. They've been changing ever since the earth was formed.\" (Rick Perry)\nPrevious major global climate changes were glacial cycles that happened long before human civilization developed.\nThe human species evolved during the last 2.5 million years. Our far distant ancestors survived through multiple gradual cycles of cold ice ages, but did not experience any previous \"hot ages.\"\nWe homo sapiens in our current form appeared only about 200,000 years ago. So our species has survived two ice ages. In each ice age global temperatures were colder by 4 °C. The warmest period ever experienced by early humans was about 1 °C warmer (global average) than today. That period occured between the two most recent ice ages, 120,000 years ago (Eemian). Over the next 100,000 years temperatures gradually decreased into a new ice age. During that colder period humans began to expand out of Africa and across the globe. Ever since the Eemian much cooler temperatures have been the norm.\nImage by John Garrett.\nHuman civilization is roughly 12,000 years old, as defined by the start of permanent settlements and agriculture. Agriculture became established as the glaciers retreated from the last ice age. Modern society has developed entirely in our current geological epoch, the Holocene. Global temperatures haven't varied by more than ±1 °C since. There have been regional shifts in climate (Medieval Warm Period, Little Ice Age, etc), but since civilization began humans have never experienced a hotter global climate than now.\nGoing back further, over a million years or so, our pre-human predecessors experienced a series of long cold glacial cycles. Several short interglacial periods were as warm or slightly warmer than our current climate. For example, the climate 400 kyrs ago, was slightly warmer than now. But more typically for the last million years it's been 4 to 8 °C colder. Each transition from warm to glacial ages and back took thousands of years, giving humans and prehumans many generations to adjust.\nSo, really, the climate hasn't changed much since we settled into towns, invented plumbing, and started calling ourselves civilized.\nSince humans and our human ancestors have been on Earth, average global temperatures have never been 3 °C warmer than now. In the next 100 years our children will be the first people ever to experience that kind of climate.\nBut, perhaps Mr. Perry is thinking he'd like to live in a climate eons ago, closer to when the Earth was formed.\nDigging way back in time, we know that Earth's climate has certainly been very different than it is now: 2 billion years ago there was not even any oxygen in the atmosphere. 550 million years ago high CO2 levels caused extreme greenhouse conditions. Humans were not around to care; the most advanced life form at that time was a flatworm. Humans could not physically survive over most of the planet in the age of the dinosaurs (Cretaceous, 100-65 Myr ago). Only very small mammals were beginning to evolve. Global average temperatures were 10-12 °C hotter than today. Most places on land were so hot that humans would risk fatal heat stroke every summer.\nThe geological record shows many ancient changes in climate, including massive ice ages, hot-house conditions, oxygen-free and acidic oceans, and massive extinction events. These changes happened millions of years before humans, most occurred before even primitive mammals, appeared on the scene. Previous climate changes were caused by orbital wobbles, solar fluctuations, and movement of continents. None of those effects are causing the current heating http://sks.to/past.\nLast updated on 9 March 2012 by dana1981. View Archives"
  },
  {
   "title": "Humidity is falling",
   "paragraph": "What does the full body of evidence tell us about humidity?\nLink to this page\nWhat the science says...\nTo claim that humidity is decreasing requires you ignore a multitude of independent reanalyses that all show increasing humidity. It requires you accept a flawed reanalysis that even its own authors express caution about. It fails to explain how we can have short-term positive feedback and long-term negative feedback. In short, to insist that humidity is decreasing is to neglect the full body of evidence.\nClimate Myth...\nHumidity is falling\n\"...the largest of all the positive or temperature-amplifying feedbacks in the UN’s arsenal is the water-vapor feedback. The UN gets this feedback wrong in numerous fundamental ways. For instance, its models tend to treat column absolute humidity as being uniform at all altitudes, when in fact – as Paltridge et al. (2009) have demonstrated recently – the upper troposphere (the only place where adding CO2 to the atmosphere could make any difference to temperature) is considerably drier than the models are tuned to expect.\" (Christopher Monckton)\nWater vapor provides the most powerful feedback in the climate system. When surface temperature warms, this leads to an increase in atmospheric humidity. Because water vapor is a greenhouse gas, the increase in humidity causes additional warming. This positive feedback has the capacity to double the initial surface warming. So when temperatures rise, we expect humidity to also increase. However, one study using weather balloon measurements found decreasing humidity (Paltridge et al 2009). To get to the truth of the matter, the full body of evidence regarding humidity is perused in a new paper Trends in tropospheric humidity from reanalysis systems (Dessler & Davis 2010).\nTo give an overview of humidity trends, Dessler and David compare the results from Paltridge's 2009 paper to a number of other reanalyses of humidity. Figure 1 shows the trend in specific humidity from 1973 to 2007 over the tropics. The Paltridge reanalysis (thick black line) shows considerable divergence in the upper troposphere, with a strong negative trend while the other reanalyses all give consistent results, both with each other and theoretical expectations.\nFigure 1: Various reanalyses showing the trend in specific humidity from 1973 to 2007 in the tropics (Dessler 2010 also looks at the Northern and Southern extra-tropics - only the tropic data is shown here for simplicity and as it shows the greatest contrast between Paltridge 2009 and the other reanalyses).\nTo gain more insight into the nature of the observed water vapor feedback, Dessler and Davis examine the relationship between humidity and surface temperature. They plot specific humidity directly against surface temperature - this gives a measure of the amount of water vapor feedback. They compare the short-term trend in water vapor feedback (under 10 years) to the long-term trend (greater than 10 years) for the 5 different reanalyses:\nFigure 2: Short-term (a) and Long-term (b) plots of the slopes of the regression between specific humidity and surface temperature, in the tropics. Trends are divided by the average specific humidity over the entire time period, so they are expressed in percent per degree K.\nFor the short-term trends, all five reanalyses produce consistent results, with surface warming associated with increasing humidity (eg - positive water vapor feedback). However, there is poorer agreement in the long-term trends. The Paltridge 2009 reanalysis is a distinct outlier, with long-term and short-term trends going in opposite directions, unlike the results from the other studies.\nThis leads to an interesting question: could water vapor feedback be opposite over short and long-term time scales? There is no theory that can explain how short-term feedback could be positive while long-term feedback is negative. The water vapor response to a climate fluctuation with a time scale of a few years (e.g., ENSO) should be about the same as for long-term warming.\nLong-term positive feedback is confirmed by several independent sources. An analysis of long-term measurements of upper tropospheric water vapor shows a positive water vapor feedback in 22 years of satellite data (Soden et al 2005). In addition, analysis of long-term paleoclimate records is also inconsistent with a negative long-term water vapor feedback (Köhler et al 2010).\nSo why does Paltridge 2009 show decreasing humidity? The authors of Paltridge 2009 themselves point out the well-documented problems with radiosonde humidity observations in the upper troposphere. Comparisons of Paltridge 2009 with satellite measurements (NASA’s Atmospheric Infrared Sounder - AIRS) find the Paltridge 2009 reanalysis has large biases in specific humidity in the tropical upper troposphere. Additionally, Paltridge 2009 doesn't show any large increase in specific humidity during the 1998 El Niño. Direct measurements indicate the tropical atmosphere does indeed moisten during El Niño events and such moistening is seen in the other reanalyses.\nTwo of the newer reanalyses shown in the figures above, MERRA and ECMWF-Interim, correct for well documented biases introduced by changes in the observing system. These newer reanalyses are in better agreement with theory, other reanalyses and independent observations.\nTo claim that humidity is decreasing requires you ignore a multitude of independent reanalyses, including newer ones with improved algorithms, that all show increasing humidity. It requires you accept a flawed reanalysis that even its own authors express caution about. It fails to explain how we can have short-term positive feedback and long-term negative feedback (indeed there is no known mechanism that can explain it). In short, to insist that humidity is decreasing is to neglect the full body of evidence.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 26 October 2016 by pattimer. View Archives"
  },
  {
   "title": "Ice age predicted in the 70s",
   "paragraph": "What were climate scientists predicting in the 1970s?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe vast majority of climate papers in the 1970s predicted warming.\nClimate Myth...\nIce age predicted in the 70s\n\"[M]any publications now claiming the world is on the brink of a global warming disaster said the same about an impending ice age – just 30 years ago. Several major ones, including The New York Times, Time magazine and Newsweek, have reported on three or even four different climate shifts since 1895.\" (Fire and Ice)\nIn the thirty years leading up to the 1970s, available temperature recordings suggested that there was a cooling trend. As a result some scientists suggested that the current inter-glacial period could rapidly draw to a close, which might result in the Earth plunging into a new ice age over the next few centuries. This idea could have been reinforced by the knowledge that the smog that climatologists call ‘aerosols’ – emitted by human activities into the atmosphere – also caused cooling. In fact, as temperature recording has improved in coverage, it’s become apparent that the cooling trend was most pronounced in northern land areas and that global temperature trends were in fact relatively steady during the period prior to 1970.\nAt the same time as some scientists were suggesting we might be facing another ice age, a greater number published contradicting studies. Their papers showed that the growing amount of greenhouse gasses that humans were putting into the atmosphere would cause much greater warming – warming that would exert a much greater influence on global temperature than any possible natural or human-caused cooling effects.\nBy 1980 the predictions about ice ages had ceased, due to the overwhelming evidence contained in an increasing number of reports that warned of global warming. Unfortunately, the small number of predictions of an ice age appeared to be much more interesting than those of global warming, so it was those sensational 'Ice Age' stories in the press that so many people tend to remember.\nThe fact is that around 1970 there were 6 times as many scientists predicting a warming rather than a cooling planet. Today, with 30+years more data to analyse, we've reached a clear scientific consensus: 97% of working climate scientists agree with the view that human beings are causing global warming.\nBasic rebuttal written by John Russell\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Ice isn't melting",
   "paragraph": "Global ice melt is accelerating\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nArctic summer sea ice has shrunk by an area equal to Western Australia, and might be all gone in a decade.\nClimate Myth...\nIce isn't melting\nThanks to a rapid rebound in recent months, global sea ice levels now equal those seen 29 years ago, when the year 1979 also drew to a close. In fact, the rate of increase from September onward is the fastest rate of change on record, either upwards or downwards. The data is being reported by the University of Illinois's Arctic Climate Research Center, and is derived from satellite observations of the Northern and Southern hemisphere polar regions (Daily Tech).\nIce is melting at accelerating rates in the Arctic, Antarctica, Greenland, and glaciers all over the world.\nIce sheets are beginning to shrink\nAn ice sheet is a huge layer of land ice. The only ice sheets are in Antarctica and Greenland.\nThe Greenland ice sheet is losing mass at an accelerating rate. In recent years the ice loss has spread from the south coast around to the northwest.\n(Image source: Climate Signals.)\nSimilarly, Antarctica is also losing ice at an accelerating rate. Antarctica is basically divided into two distinct ice sheets, the West Antarctic and East Antarctic. The East Antarctic ice sheet, which is much bigger than the West Antarctic one, was until recently considered stable, but has also begun losing ice.\n(Image source: NASA.)\nIce shelves are collapsing\nIce shelves are thick, floating platforms of ice formed when glaciers flow from the land onto the ocean surface.\nThe Antarctic Peninsula is warming rapidly. Several ice shelves have collapsed completely,including one covering 3,250 km2, almost twice the area of urban Sydney.\n(Image source: National Snow and Ice Data Center.)\nGlaciers are retreating\nGlaciers are retreating around the globe. Although one can point to particular glaciers that are growing, glaciologists look for trends in the total mass of glaciers worldwide. It turns out the world’s glaciers are losing ice at an accelerating rate.\n(Image data source: Cogley 2009.)\nAnd despite all the hype about a certain mistake in the 2007 IPCC report, the Himalayan glaciers are in fact melting.\nSouthern sea ice not doing much\nSea ice floats on the ocean surface, and is not to be confused with ice sheets on land. Even though the Antarctic ice sheet is losing mass, the extent of sea ice around the coast of the continent has grown slightly.\nThis is because of a complex variety of factors, and despite the warming of the Southern Ocean. The trend is expected to reverse in coming decades as the Antarctic continues to warm.\nArctic sea ice in a death spiral?\nArctic sea ice grows and shrinks seasonally, with an annual minimum in September. In 1979, when satellites first measured it, September Arctic sea ice extent was roughly equivalent to the area of Australia. Since then it has declined by about a third, equivalent to losing Western Australia – outstripping all projections.\n(Image source: Copenhagen Diagnosis.)\n2010 had the third lowest minimum on record (after 2007 and 2008). Two expeditions successfully circumnavigated the Arctic Ocean in a single summer, something that would have been impossible just a few years earlier or any time in recorded history.\nContrarians claim Arctic sea ice has “recovered” since the record low extent of 2007. But sea ice exists in three dimensions, and it has continued to thin rapidly. Ice volume data paints a picture even more dire: the Arctic has actually lost not one third but two thirds of September sea ice. What’s more, the volume reached a record low in 2010 – not an encouraging sign of recovery.\n2010 set the stage for continued melting. At the end of the summer, a record-breaking 86% of ice cover was less than two years old; ice older than five years has all but disappeared. The remaining new ice is thinner and much easier to melt than older ice.\n(Image source: National Snow and Ice Data Center.)\nIce-free summers are now probably inevitable, but it’s not clear how soon because the Arctic is melting much faster than any model predicted. Mark Serreze, Director of the US National Snow and Ice Data Center, says we’re “looking at a seasonally ice-free Arctic in twenty to thirty years.” A few scientists argue that September sea ice could be essentially gone within the next decade.\nBasic rebuttal written by James Wight\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 13 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "Increasing CO2 has little to no effect",
   "paragraph": "How do we know more CO2 is causing warming?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nAn enhanced greenhouse effect from CO2 has been confirmed by multiple lines of empirical evidence.\nClimate Myth...\nIncreasing CO2 has little to no effect\n\"While major green house gas H2O substantially warms the Earth, minor green house gases such as CO2 have little effect.... The 6-fold increase in hydrocarbon use since 1940 has had no noticeable effect on atmospheric temperature ... \" (Environmental Effects of Increased Atmospheric Carbon Dioxide)\nPredicting the Future\nGood scientific theories are said to have ‘predictive power’. In other words, armed only with a theory, we should be able to make predictions about a subject. If the theory’s any good, the predictions will come true.\nHere’s an example: when the Table of Elements was proposed, many elements were yet to be discovered. Using the theory behind the Periodic Table, the Russian chemist Dmitri Mendeleev was able to predict the properties of germanium, gallium and scandium, despite the fact they hadn’t been discovered.\nThe effect of adding man-made CO2 is predicted in the theory of greenhouse gases. This theory was first proposed by Swedish scientist Svante Arrhenius in 1896, based on earlier work by Fourier and Tyndall. Many scientist have refined the theory in the last century. Nearly all have reached the same conclusion: if we increase the amount of greenhouse gases in the atmosphere, the Earth will warm up.\nWhat they don’t agree on is by how much. This issue is called ‘climate sensitivity’, the amount the temperatures will increase if CO2 is doubled from pre-industrial levels. Climate models have predicted the least temperature rise would be on average 1.65°C (2.97°F) , but upper estimates vary a lot, averaging 5.2°C (9.36°F). Current best estimates are for a rise of around 3°C (5.4°F), with a likely maximum of 4.5°C (8.1°F).\nWhat Goes Down…\nThe greenhouse effect works like this: Energy arrives from the sun in the form of visible light and ultraviolet radiation. The Earth then emits some of this energy as infrared radiation. Greenhouse gases in the atmosphere 'capture' some of this heat, then re-emit it in all directions - including back to the Earth's surface.\nThrough this process, CO2 and other greenhouse gases keep the Earth’s surface 33°Celsius (59.4°F) warmer than it would be without them. We have added 42% more CO2, and temperatures have gone up. There should be some evidence that links CO2 to the temperature rise.\nSo far, the average global temperature has gone up by about 0.8 degrees C (1.4°F):\n\"According to an ongoing temperature analysis conducted by scientists at NASA’s Goddard Institute for Space Studies (GISS)…the average global temperature on Earth has increased by about 0.8°Celsius (1.4°Fahrenheit) since 1880. Two-thirds of the warming has occurred since 1975, at a rate of roughly 0.15-0.20°C per decade.\"\nThe temperatures are going up, just like the theory predicted. But where’s the connection with CO2, or other greenhouse gases like methane, ozone or nitrous oxide?\nThe connection can be found in the spectrum of greenhouse radiation. Using high-resolution FTIR spectroscopy, we can measure the exact wavelengths of long-wave (infrared) radiation reaching the ground.\nFigure 1: Spectrum of the greenhouse radiation measured at the surface. Greenhouse effect from water vapour is filtered out, showing the contributions of other greenhouse gases (Evans 2006).\nSure enough, we can see that CO2 is adding considerable warming, along with ozone (O3) and methane (CH4). This is called surface radiative forcing, and the measurements are part of the empirical evidence that CO2 is causing the warming.\n...Must Go Up\nHow long has CO2 been contributing to increased warming? According to NASA, “Two-thirds of the warming has occurred since 1975”. Is there a reliable way to identify CO2’s influence on temperatures over that period?\nThere is: we can measure the wavelengths of long-wave radiation leaving the Earth (upward radiation). Satellites have recorded the Earth's outbound radiation. We can examine the spectrum of upward long-wave radiation in 1970 and 1997 to see if there are changes.\nFigure 2: Change in spectrum from 1970 to 1996 due to trace gases. 'Brightness temperature' indicates equivalent blackbody temperature (Harries 2001).\nThis time, we see that during the period when temperatures increased the most, emissions of upward radiation have decreased through radiative trapping at exactly the same wavenumbers as they increased for downward radiation. The same greenhouse gases are identified: CO2, methane, ozone etc.\nThe Empirical Evidence\nAs temperatures started to rise, scientists became more and more interested in the cause. Many theories were proposed. All save one have fallen by the wayside, discarded for lack of evidence. One theory alone has stood the test of time, strengthened by experiments.\nWe know CO2 absorbs and re-emits longwave radiation (Tyndall). The theory of greenhouse gases predicts that if we increase the proportion of greenhouse gases, more warming will occur (Arrhenius).\nScientists have measured the influence of CO2 on both incoming solar energy and outgoing long-wave radiation. Less longwave radiation is escaping to space at the specific wavelengths of greenhouse gases. Increased longwave radiation is measured at the surface of the Earth at the same wavelengths.\nThese data provide empirical evidence for the predicted effect of CO2.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 1 August 2015 by MichaelK. View Archives"
  },
  {
   "title": "Infrared Iris will reduce global warming",
   "paragraph": "Body of research undermines infrared iris hypothesis\nLink to this page\nWhat the science says...\nThe infrared iris hypothesis has not withstood the test of time. All subsequent research has found that if the iris effect even exists, it is much smaller than originally hypothesized, and may even amplify global warming rather than reducing it.\nClimate Myth...\nInfrared Iris will reduce global warming\n\"a [cloudcover] change in the Tropics could lead to a negative feedback in the global climate, with a feedback factor of about –1.1, which if correct, would more than cancel all the positive feedbacks in the more sensitive current climate models\" (Lindzen et al. 2001)\nIn 2001, Lindzen et al. published Does the Earth Have an Adaptive Infrared Iris?. The infrared iris hypothesis suggets that increased sea surface temperature in the tropics would result in reduced cirrus clouds and thus more infrared radiation leakage from Earth's atmosphere. This radiation leakage in turn would have a cooling effect, dampening global warming as a negative feedback. NASA explains the hypothesis and why it's called the iris effect:\n\"Much like the iris in a human eye contracts to allow less light to pass through the pupil in a brightly lit environment, Lindzen suggests that the area covered by high cirrus clouds contracts to allow more heat to escape into outer space from a very warm environment.\"\nLindzen et al. was published over a decade ago, so how has the iris hypothesis withstood the test of time?\nSubsequent Research\nIn a very short timeframe, a number of other studies had investigated the iris hypothesis. Approximately 6 months after Lindzen et al. was published, Fu et al. (2001) (revised in early 2002) published a paper which found evidence of an iris effect, but that it was significantly smaller than Lindzen et al. suggested:\n\"We argue that the water vapor feedback is overestimated in Lindzen et al. (2001) by at least 60%, and that the high cloud feedback should be small. Although not mentioned by Lindzen et al, tropical low clouds make a significant contribution to their negative feedback, which is also overestimated. Using more realistic parameters in the model of Lindzen et al., we obtain a feedback factor in the range of −0.15 to −0.51, compared to their larger negative feedback factor of −0.45 to −1.03 [W m-2 K-1].\"\nA few months later, Lin et al. (2002) published another study examining the iris hypothesis. Lin et al. took observational data from the Clouds and the Earth's Radiant Energy System (CERES) over the tropical oceans and plugged them into the same model that Lindzen used. This observational data dramatically changed the iris hypothesis, because it showed that the clouds in the tropics are significantly more reflective (a.k.a. higher albedo) and have a weaker warming effect than in the Lindzen model.\nClouds have two competing effects on global temperature - cooling by reflecting solar radiation, and warming by increasing the greenhouse effect. Which of these effects is larger depends on the type of cloud.\nThe model used by Lindzen et al. had concluded that for clouds in the tropics, the warming effect was greater. Thus the decrease in cloudcover hypothetically caused by the iris effect would result in less cloud warming, and thus a negative feedback.\nUsing the CERES data, Lin et al. concluded that the cooling effect is actually larger for tropical clouds, and thus Lindzen's iris effect (if it existed, which this study didn't investigate) would actually result in a weak positive feedback:\n\"The observations show that the clouds have much higher albedos and moderately larger longwave fluxes than those assumed by Lindzen et al. As a result, decreases in these clouds would cause a significant but weak positive feedback to the climate system, instead of providing a strong negative feedback.\"\nLess than a year after Lindzen et al., a third response paper, Hartmann and Michelsen (2002), was published. They analysed the same sea surface temperature (SST) and cloud data as Lindzen et al., but concluded that the changes could not be attributed to an iris effect:\n\"It is unreasonable to interpret these changes as evidence that deep tropical convective anvils contract in response to SST increases. Moreover, the nature of the cloudweighted SST statistic is such that any variation in cloud fraction over the coldest water must produce a negative correlation with cloud fraction, a fact that has no useful interpretation in climate sensitivity analysis. Therefore, the observational analysis in [Lindzen et al.] lends no support to the hypothesis that increased SST decreases the area covered by tropical anvil cloud.\"\nOne year after the publication of the iris hypothesis, Del Genio & Kovari (2002) also found errors in the Lindzen analysis which dramatically changed the conclusions:\n\"A clustering algorithm is used to define the radiative, hydrological, and microphysical properties of precipitating convective events in the equatorial region observed by the Tropical Rainfall Measuring Mission (TRMM) satellite....The adaptive iris hypothesis (clouds thinning with warming) is clearly not supported by the TRMM data. TRMM storms become larger and cover a greater fractional area, and the largest of them become somewhat brighter, at higher SST. Several flaws in reasoning lead Lindzen et al. (2001) to their conclusion.\"\nSo within a year of the publication of Lindzen et al., there was one paper concluding they had significantly overestimated the iris effect, a second concluding that if the iris effect existed, it would lead to increased warming, and a third and fourth papers finding no evidence for the iris effect.\nLike Lin et al., Chambers et al. (2002) examined data from CERES to look for evidence of the iris effect. As with previous results, they found that the feedback effect is much smaller than proposed in Lindzen et al., and probably slightly positive.\n\"Regardless of definition, the radiative properties are found to be different from those assigned in the original Iris hypothesis. As a result, the strength of the feedback effect is reduced by a factor of 10 or more. Contrary to the initial Iris hypothesis, most of the definitions tested in this paper result in a small positive feedback. Thus, the existence of an effective infrared iris to counter greenhouse warming is not supported by the CERES data.\"\nLin et al. (2004) compared Earth Radiation Budget Satellite (ERBS) decadal observational data with the predictions of the iris hypothesis using 3.5-box model, also replacing the modeled radiative properties in Lindzen's paper with CERES data, as Lin et al. (2002) had previously done. The study concluded as follows.\n\"On the decadal time scale, the predicted tropical mean radiative flux anomalies are generally significantly different from those of the ERBS measurements, suggesting that the decadal ERBS nonscanner radiative energy budget measurements do not support the strong negative feedback of the Iris effect. Poor agreements between the satellite data and model predictions even when the tropical radiative properties from CERES observations (LaRC parameters) are used imply that besides the Iris-modeled tropical radiative properties, the unrealistic variations of tropical high cloud generated from the detrainment of deep convection with SST assumed by the Iris hypothesis are likely to be another major factor for causing the deviation between the predictions and observations.\"\nRapp et al. (2005) similarly found little evidence for the iris effect in their research:\n\"this study addresses some of the criticisms of the Lindzen et al. study by eliminating their more controversial method of relating bulk changes of cloud amount and SST across a large domain in the Tropics. The current analysis does not show any significant SST dependence of the ratio of cloud area to surface rainfall for deep convection in the tropical western and central Pacific....the interaction between the SST and effective cloud size may even have a slight positive relationship, not the inverse relationship suggested by [Lindzen et al.]\"\nSpencer et al. (2007) did find a short-term reduction in cloudcover which is at least nominally consistent with the iris hypothesis, with some caveats. This study examined the daily evolution of tropical intraseasonal oscillations in satellite-observed tropospheric temperature, precipitation, radiative fluxes, and cloud properties, and found:\n\"The decrease in ice cloud coverage is conceptually consistent with the ‘‘infrared iris’’ hypothesized by Lindzen et al. [2001]....We caution, though, that the ice cloud reduction with tropospheric warming reported here is on a time scale of weeks; it is not obvious whether similar behavior would occur on the longer time scales associated with global warming.\"\nHowever, Dessler (2010) did not find evidence of a significant negative cloud feedback, as was suggested by Spencer et al. (2007).\n\"There have been inferences of a large negative cloud feedback in response to short-term climate variations that can substantially cancel the other feedbacks operating in our climate system. This would require the cloud feedback to be in the range of –1.0 to –1.5 W/m2/K or larger, and I see no evidence to support such a large negative cloud feedback\"\nDessler concluded that the short-term global cloudcover feedback is probably positive, and unlikely to be strongly negative, with a 95% confidence range at 0.54 ± 0.74 W m-2 K-1. Therefore, even if the iris effect exists in the tropics, it won't be able to offset very much (if any) global warming, according to Dessler's results.\nIris Hypothesis Never Got off the Ground\nIn short, much research has focused on Lindzen's iris hypothesis, but very little supporting evidence has been uncovered. On the contrary, studies have consistenly shown that Lindzen dramatically overestimated the iris effect in his initial study, and that if the effect exists, it may even amplify warming as opposed to dampening it. There certainly isn't any evidence that the infrared iris will result in enough of a negative feedback to significantly slow down global warming.\nLast updated on 13 December 2011 by dana1981. View Archives"
  },
  {
   "title": "IPCC admits global warming has paused",
   "paragraph": "The global climate continues to warm rapidly\nLink to this page\nWhat the science says...\nThe IPCC report shows that when we account for the warming of the entire climate system, global warming continues at a rapid rate, equivalent to 4 Hiroshima atomic bomb detonations per second.\nClimate Myth...\nIPCC admits global warming has paused\n\"[The IPCC] recognise the global warming ‘pause’...is real – and concede that their computer models did not predict it. But they cannot explain why world average temperatures have not shown any statistically significant increase since 1997.\" (David Rose)\nMany popular climate myths share the trait of vagueness. For example, consider the argument that climate has changed naturally in the past. Well of course it has, but what does that tell us? It's akin to telling a fire investigator that fires have always happened naturally in the past. That would doubtless earn you a puzzled look from the investigator. Is the implication that because they have occurred naturally in the past, humans can't cause fires or climate change?\nThe same problem applies to the 'pause' (or 'hiatus' or better yet, 'speed bump') assertion. It's true that the warming of average global surface temperatures has slowed over the past 15 years, but what does that mean? One key piece of information that's usually omitted when discussing this subject is that the overall warming of the entire climate system has continued rapidly over the past 15 years, even faster than the 15 years before that.\nEnergy accumulation within distinct components of Earth’s climate system from 1971 to 2010. From the 2013 IPCC report.\nThe speed bump only applies to surface temperatures, which only represent about 2 percent of the overall warming of the global climate. Can you make out the tiny purple segment at the bottom of the above figure? That's the only part of the climate for which the warming has 'paused'. As the IPCC figure indicates, over 90 percent of global warming goes into heating the oceans, and it continues at a rapid pace, equivalent to 4 Hiroshima atomic bomb detonations per second.\nAnother important piece of oft-omitted information: while the warming of surface temperatures was relatively slow from 1998 to 2012, it was relatively fast from 1990 through 2006. Over longer time frames, for example from 1990 to 2012, average global surface temperatures have warmed as fast as climate scientists and their models expected.\nSo what's changed over the past 10 to 15 years? The IPCC attributes the recent slowing of surface temperatures to a combination of external and internal climate factors. For example, solar activity has been relatively low and volcanic activity has been relatively high, causing less solar energy to reach the Earth's surface. At the same time, we're in the midst of cool ocean cycle phases, for example with a preponderance of La Niña events since 1999. A number of recent studies have suggested that most of the recent slowing of surface warming is due to these ocean cycles.\nWhat does that mean for the future? It means more global warming. A number of papers from climate 'skeptics' have sought to fit the surface temperature measurements with various cycles. Some have tried to attribute these changes to astronomical cycles, others to ocean cycles, others to 'stadium waves'. Ultimately these papers are just trying to explain the short-term wiggles in the data. For example, as Marcia Wyatt, lead author of the recent Wyatt & Curry 'stadium waves' paper explained,\n\"While the results of this study appear to have implications regarding the hiatus in warming, the stadium wave signal does not support or refute anthropogenic global warming. The stadium wave hypothesis seeks to explain the natural multi-decadal component of climate variability.\"\nIn other words, the surface temperature speed bump is mainly due to the short-term influences of natural climate variability on top of the long-term human-caused warming trend. As Mark Boslough noted, it all boils down to physics and conservation of energy. We continue to increase the greenhouse effect by burning more and more fossil fuels. The extra energy trapped in the Earth's climate system by that increased greenhouse effect can't just disappear, it has to go somewhere. Right now it just so happens that more is going into the oceans, whereas in the 1990s more was going into the atmosphere.\nSome have asked if the 'pause' is real or a result of cherry picking. The answer is that there is a 'pause' if the data are cherry picked. First we have to cherry pick the 2 percent of global warming represented by surface temperatures and ignore the other 98 percent. Then we have to cherry pick a sufficiently short time frame to find a flat trend.\nAverage of NASA GISS, NOAA NCDC, and HadCRUT4 monthly global surface temperature anomalies from January 1970 through November 2012 (green) with linear trends applied to the time frames Jan '70 - Oct '77, Apr '77 - Dec '86, Sep '87 - Nov '96, Jun '97 - Dec '02, and Nov '02 - Nov '12.\nDespite this double cherry picking, ignoring 98 percent of global warming, and despite the sun and volcanoes and ocean cycles all acting in the cooling direction over the past decade, the best climate contrarians can do is find a flat 10-year surface temperature trend. Can you guess what's going to happen the next time the oceans shift to a warm cycle? That's the thing about cycles – they're cyclical.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 9 July 2015 by pattimer. View Archives"
  },
  {
   "title": "IPCC edited out natural causes of climate change",
   "paragraph": "IPCC concludes humans outweigh natural contributions to climate change\nLink to this page\nWhat the science says...\nThe 2007 IPCC report summary for policymakers explicitly discusses natural contributions to climate change in many different locations, but concludes that the natural contribution to recent climate change is small.\nClimate Myth...\nIPCC edited out natural causes of climate change\n\"The long version of the IPCC report does mention natural causes of climate change, like the sun and oscillating ocean currents. But they no longer appear in the summary for politicians. They were simply edited out.\" (Fritz Vahrenholt)\nThe following quote are taken directly from the 2007 IPCC Fourth Assessment Report Summary for Policymakers (SPM). The first quote is the first sentence in the SPM.\n* \"The Working Group I contribution to the IPCC Fourth Assessment Report describes progress in understanding of the human and natural drivers of climate change, observed climate change, climate processes and attribution, and estimates of projected future climate change.\"\n* \"Human and Natural Drivers of Climate Change\nChanges in the atmospheric abundance of greenhouse gases and aerosols, in solar radiation and in land surface properties alter the energy balance of the climate system. These changes are expressed in terms of radiative forcing, which is used to compare how a range of human and natural factors drive warming or cooling influences on global climate.\"\n* \"Changes in solar irradiance since 1750 are estimated to cause a radiative forcing of +0.12 [+0.06 to +0.30] W m–2, which is less than half the estimate given in the TAR.\"\n* \"It is very unlikely that climate changes of at least the seven centuries prior to 1950 were due to variability generated within the climate system alone. A significant fraction of the reconstructed Northern Hemisphere inter-decadal temperature variability over those centuries is very likely attributable to volcanic eruptions and changes in solar irradiance, and it is likely that anthropogenic forcing contributed to the early 20th century warming evident in these records.\"\n* \"The observed widespread warming of the atmosphere and ocean, together with ice mass loss, support the conclusion that it is extremely unlikely that global climate change of the past 50 years can be explained without external forcing, and very likely that it is not due to known natural causes alone.\"\nThere are more such examples. Then there's Figures SPM.2 and SPM.4:\nFigure SPM.2: Global average radiative forcing (RF) estimates and ranges in 2005 for anthropogenic carbon dioxide (CO2 ), methane (CH4 ), nitrous oxide (N2O) and other important agents and mechanisms, together with the typical geographical extent (spatial scale) of the forcing and the assessed level of scientific understanding (LOSU). The net anthropogenic radiative forcing and its range are also shown. These require summing asymmetric uncertainty estimates from the component terms, and cannot be obtained by simple addition. Additional forcing factors not included here are considered to have a very low LOSU. Volcanic aerosols contribute an additional natural forcing but are not included in this figure due to their episodic nature. The range for linear contrails does not include other possible effects of aviation on cloudiness.\nFigure SPM.4 - Comparison of observed continental- and global-scale changes in surface temperature with results simulated by climate models using natural and anthropogenic forcings. Decadal averages of observations are shown for the period 1906 to 2005 (black line) plotted against the centre of the decade and relative to the corresponding average for 1901–1950. Lines are dashed where spatial coverage is less than 50%. Blue shaded bands show the 5–95% range for 19 simulations from fi ve climate models using only the natural forcings due to solar activity and volcanoes. Red shaded bands show the 5–95% range for 58 simulations from 14 climate models using both natural and anthropogenic forcings.\nClearly the SPM explicitly discusses natural contributions to global warming, and explains that according to the body of scientific evidence, their contribution to the observed warming is small:\nNet human and natural percent contributions to the observed global surface warming over the past 50-65 years according to Tett et al. 2000 (T00, dark blue), Meehl et al. 2004 (M04, red), Stone et al. 2007 (S07, green), Lean and Rind 2008 (LR08, purple), Huber and Knutti 2011 (HK11, light blue), and Gillett et al. 2012 (G12, orange). This has been added to the SkS Climate Graphics Page.\nLast updated on 13 February 2012 by dana1981. View Archives"
  },
  {
   "title": "IPCC global warming projections were wrong",
   "paragraph": "IPCC global surface warming projections have been accurate\nLink to this page\nWhat the science says...\nGlobal surface temperature measurements fall within the range of IPCC projections.\nClimate Myth...\nIPCC global warming projections were wrong\n\"Yet the leaked report makes the extraordinary concession that over the past 15 years, recorded world temperatures have increased at only a quarter of the rate of IPCC claimed when it published its last assessment in 2007. Back then, it said observed warming over the 15 years from 1990-2005 had taken place at a rate of 0.2C per decade, and it predicted this would continue for the following 20 years, on the basis of forecasts made by computer climate models. But the new report says the observed warming over the more recent 15 years to 2012 was just 0.05C per decade - below almost all computer predictions.\" (David Rose)\nThe figure below from the 2013 Intergovernmental Panel on Climate Change (IPCC) report compares the global surface warming projections made in the 1990, 1995, 2001, and 2007 IPCC reports to the temperature measurements.\nIPCC AR5 Figure 1.4. Solid lines and squares represent measured average global surface temperature changes by NASA (blue), NOAA (yellow), and the UK Hadley Centre (green). The colored shading shows the projected range of surface warming in the IPCC First Assessment Report (FAR; yellow), Second (SAR; green), Third (TAR; blue), and Fourth (AR4; red).\nSince 1990, global surface temperatures have warmed at a rate of about 0.15°C per decade, within the range of model projections of about 0.10 to 0.35°C per decade. As the IPCC notes,\n\"global climate models generally simulate global temperatures that compare well with observations over climate timescales ... The 1990–2012 data have been shown to be consistent with the [1990 IPCC report] projections, and not consistent with zero trend from 1990 ... the trend in globally-averaged surface temperatures falls within the range of the previous IPCC projections.\"\nWhat about the Naysayers?\nIn the weeks and months leading up to the publication of the final 2013 IPCC report, there has been a flood of opinion articles in blogs and the mainstream media claiming that the models used by the IPCC have dramatically over-predicted global warming and thus are a failure. This narrative clearly conflicts with the IPCC model-data comparison figure shown above, so what's going on?\nThese mistaken climate contrarian articles have all suffered from some combination of the following errors.\n1) Publicizing the flawed draft IPCC model-data comparison figure\nLate last year, an early draft of the IPCC report was leaked, including the first draft version of the figure shown above. The first version of the graph had some flaws, including a significant one immediately noted by statistician and climate blogger Tamino.\n\"The flaw is this: all the series (both projections and observations) are aligned at 1990. But observations include random year-to-year fluctuations, whereas the projections do not because the average of multiple models averages those out ... the projections should be aligned to the value due to the existing trend in observations at 1990.\nAligning the projections with a single extra-hot year makes the projections seem too hot, so observations are too cool by comparison.\"\nIn the draft version of the IPCC figure, it was simply a visual illusion that the surface temperature data appeared to be warming less slowly than the model projections, even though the measured temperature trend fell within the range of model simulations. Obviously this mistake was subsequently corrected.\nThis illustrates why it's a bad idea to publicize material in draft form, which by definition is a work in progress.\n2) Ignoring the range of model simulations\nA single model run simulates just one possible future climate outcome. In reality, there are an infinite number of possible outcomes, depending on how various factors like greenhouse gas emissions and natural climate variability change. This is why climate modelers don't make predictions; they make projections, which say in scenario 'x', the climate will change in 'y' fashion. The shaded regions in the IPCC figure represent the range of outcomes from all of these individual climate model simulations.\nThe IPCC also illustrates the \"multi-model mean,\" which averages together all of the individual model simulation runs. This average makes for an easy comparison with the observational data; however, there's no reason to believe the climate will follow that average path, especially in the short-term. If natural factors act to amplify human-caused global surface warming, as they did in the 1990s, the climate is likely to warm faster than the model average in the short-term. If natural factors act to dampen global surface warming, as they have in the 2000s, the climate is likely to warm more slowly than the model average.\nWhen many model simulations are averaged together, the random natural variability in the individual model runs cancel out, and the steady human-caused global warming trend remains left over. But in reality the climate behaves like a single model simulation run, not like the average of all model runs.\nThis is why it's important to retain the shaded range of individual model runs.\n3) Cherry Picking\nMost claims that the IPCC models have failed are based on surface temperature changes over the past 15 years (1998–2012). During that period, temperatures have risen about 50 percent more slowly than the multi-model average, but have remained within the range of individual model simulation runs.\nHowever, 1998 represented an abnormally hot year at the Earth's surface due to one of the strongest El Niño events of the 20th century. Thus it represents a poor choice of a starting date to analyze the surface warming trend (selectively choosing convenient start and/or end points is also known as 'cherry picking'). For example, we can select a different 15-year period, 1992–2006, and find a surface warming trend nearly 50 percent faster than the multi-model average, as statistician Tamino helpfully illustrates in the figure below.\nGlobal surface temperature data 1975–2012 from NASA with a linear trend (black), with trends for 1992–2006 (red) and 1998–2012 (blue).\nIn short, if climate contrarians weren't declaring that global surface warming was accelerating out of control in 2006, then he has no business declaring that global surface warming has 'paused' in 2013. Both statements are equally wrong, based on cherry picking noisy short-term data.\nIPCC models have been accurate\nFor 1992–2006, the natural variability of the climate amplified human-caused global surface warming, while it dampened the surface warming for 1997–2012. Over the full period, the overall warming rate has remained within the range of IPCC model projections, as the 2013 IPCC report notes.\n\"The long-term climate model simulations show a trend in global-mean surface temperature from 1951 to 2012 that agrees with the observed trend (very high confidence). There are, however, differences between simulated and observed trends over periods as short as 10 to 15 years (e.g., 1998 to 2012).\"\nThe IPCC also notes that climate models have accurately simulated trends in extreme cold and heat, large-scale precipitation pattern changes, and ocean heat content (where most global warming goes). Models also now better simulate the Arctic sea ice decline, which they had previously dramatically underestimated.\nAll in all, the IPCC models do an impressive job accurately representing and projecting changes in the global climate, contrary to contrarian claims. In fact, the IPCC global surface warming projections have performed much better than predictions made by climate contrarians.\nIt's important to remember that weather predictions and climate predictions are very different. It's harder to predict the weather further into the future. With climate predictions, it's short-term variability (like unpredictable ocean cycles) that makes predictions difficult. They actually do better predicting climate changes several decades into the future, during which time the short-term fluctuations average out.\nThat's why climate models have a hard time predicting changes over 10–15 years, but do very well with predictions several decades into the future, as the IPCC illustrates. This is good news, because with climate change, it's these long-term changes we're worried about:\nIPCC AR5 projected global average surface temperature changes in a high emissions scenario (RCP8.5; red) and low emissions scenario (RCP2.6; blue).\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 9 July 2015 by pattimer. View Archives"
  },
  {
   "title": "IPCC human-caused global warming attribution confidence is unfounded",
   "paragraph": "The IPCC confidence in human-caused global warming is based on solid scientific research\nLink to this page\nWhat the science says...\nThe IPCC confidence in human-caused global warming is based on summarizing the body of peer-reviewed scientific research.\nClimate Myth...\nIPCC human-caused global warming attribution confidence is unfounded\n\"[The IPCC 95% confidence in human-caused global warming] is incomprehensible to me ... IPCC projections are overconfident\" (Judith Curry)\nThe fifth Intergovernmental Panel on Climate Change (IPCC) report states with 95 percent confidence that humans are the main cause of the current global warming. Many media outlets have reported that this is an increase from the 90 percent certainty in the fourth IPCC report, but actually the change is much more significant than that. In fact, if you look closely, the IPCC says that humans have most likely caused all of the global warming over the past 60 years.\nWhat's causing global warming: human greenhouse gas emissions.\n\"The best estimate of the human-induced contribution to warming is similar to the observed warming over this period ... The observed warming since 1951 can be attributed to the different natural and anthropogenic drivers and their contributions can now be quantified. Greenhouse gases contributed a global mean surface warming likely to be in the range of 0.5°C to 1.3 °C over the period 1951−2010, with the contributions from other anthropogenic forcings, including the cooling effect of aerosols, likely to be in the range of −0.6°C to 0.1°C.\"\nWhat's not causing global warming: natural external factors like solar activity, and natural internal factors like ocean cycles.\n\"The contribution from natural forcings is likely to be in the range of −0.1°C to 0.1°C, and from internal variability is likely to be in the range of −0.1°C to 0.1°C.\"\nWe've observed about 0.6°C average global surface warming over the past 60 years. During that time, the IPCC best estimate is that greenhouse gases have caused about 0.9°C warming, which was partially offset by about 0.3°C cooling from human aerosol emissions. During that time, natural external factors had no net influence on global temperatures. For example, solar activity has been flat since 1950.\nAnnual global temperature change (thin light red) with 11 year moving average of temperature (thick dark red). Temperature from NASA GISS. Annual Total Solar Irradiance (TSI; thin light blue) with 11 year moving average of TSI (thick dark blue). TSI from 1880 to 1978 from Krivova et al (2007). TSI from 1979 to 2009 from PMOD.\nAs for the natural internal variability of the Earth's climate system, short-term noise averages out to zero over long time frames. Warm and cool ocean cycles cancel each other out, and thus internal variability has no long-term influence on average global temperatures.\nPut it all together, and the IPCC is 95 percent confident that humans have caused most of the observed global surface warming over the past 60 years. Their best estimate is that humans have caused 100 percent of that global warming.\nIPCC is Summarizing the Scientific Research\nThe IPCC does not conduct any original research; it's a summary report, and these statements accurately reflect the body of climate science research. For example, last year climate scientists Tom Wigley and Ben Santer published a paper concluding that human climate influences were responsible for 50 to 150 percent of the observed warming from 1950 to 2005.\nLike this new IPCC statement, they found 95 percent probability that humans have caused at least half the observed warming since 1950, and most likely all of it. It's also possible that humans have caused more warming than has been observed because natural factors may have had a net cooling effect. The Wigley and Santer results are consistent with the body of scientific research on the causes of global warming.\nNet human and natural percent contributions to the observed global surface warming over the past 50-65 years according to Tett et al. 2000 (T00, dark blue), Meehl et al. 2004 (M04, red), Stone et al. 2007 (S07, light green), Lean and Rind 2008 (LR08, purple), Huber and Knutti 2011 (HK11, light blue), Gillett et al. 2012 (G12, orange), Wigley and Santer 2012 (WS12, dark green), and Jones et al. 2013 (J12, pink).\nThe 'fingerprints' of climate change are also all consistent with what we expect to see as a result of human-caused global warming, for example changes in the atmosphere, as another paper by Ben Santer recently concluded.\nSummary of observational evidence that human carbon dioxide emissions are causing the climate to warm.\nWhat About the Naysayers?\nA few naysayers like Judith Curry from Georgia Tech have disputed the IPCC confidence on this question, for example in an interview with the reliably inaccurate David Rose.\nHowever, while Curry is a climate scientist, she doesn't research the causes of global warming. She also has a history of exaggerating climate uncertainties. Her comments are inconsistent with the body of scientific research on the subject. Put simply, she is speaking outside her area of expertise, like a podiatrist giving advice on open heart surgery.\nThe 97 Percent Consensus is Evidence-Based\nThis is why there's a 97 percent consensus amongst climate experts and in the climate science literature that humans are causing global warming. The scientific evidence on this question is overwhelming.\nMany commenters have noted that the expert consensus is itself not scientific evidence for human-caused global warming. That's true. The expert consensus is however based on the scientific evidence. The fact that 97 percent of climate experts agree on this subject also demonstrates the strength of the scientific evidence on human-caused global warming. And the strength of the evidence is why the IPCC is able to say with 95 percent confidence that humans are the main cause of the current global warming.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 11 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "IPCC is alarmist",
   "paragraph": "How the IPCC is more likely to underestimate the climate response\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nNumerous papers have documented how IPCC predictions are more likely to underestimate the climate response.\nClimate Myth...\nIPCC is alarmist\n\"Unquestionably, the U.N. Intergovernmental Panel on Climate Change (IPCC) was formed to build the scientific case for humanity being the primary cause of global warming. Such a goal is fundamentally unscientific, as it is hostile to alternative hypotheses for the causes of climate change.\" (Roy Spencer)\n\"Unquestionably, the U.N. Intergovernmental Panel on Climate Change (IPCC) was formed to build the scientific case for humanity being the primary cause of global warming. Such a goal is fundamentally unscientific, as it is hostile to alternative hypotheses for the causes of climate change.\"\nClimate scientist Roy Spencer made this statement. He starts by suggesting something highly questionable isn’t open to being questioned. What he seeks to do is suggest, by inference, that the IPCC has an agenda, and this distorts the reports they produce. In other words, Spencer (and others) suggest that the IPCC exaggerates what the science says in favour of anthropogenic global warming. It is perfectly legitimate to question this assertion, since Spencer and others offer no evidence to support it.\nSome critics go further, suggesting that the IPCC actively suppresses science that doesn’t support the theory that climate change is being caused by human activities. It is notable this ‘other science’ is rarely produced to support the accusation.\nDoes the IPCC accurately report the findings of science?\nThe IPCC was formed to report on a broad range of scientific enquiries into the climate, and our effects on it, and to summarise the science for laypeople. The science they summarise is published so it is simple to compare the primary science with the IPCC reports, and compare both to what actually took place.\nThere are numerous instances where the IPCC reports, which are summaries of published climate change science, have understated the case - hardly suggesting exaggeration in pursuit of an agenda. Here are some examples:\nCO2 output from fossil fuels: observed emissions are close to the worst-case projections made by the IPCC, despite them offering a range of potential emission scenarios. (In fact, atmospheric CO2 is increasing ten times faster than any rate detected in ice core data over the last 22,000 years).\nSea-level rise is accelerating faster than the IPCC predicted. Actual sea-level rise is 80% higher than the median IPCC projection. By 2100 sea-level rise was predicted by the IPCC to be in the range of 18-59 cm. It is now believed that figure may be far too low, because estimates of contributions from Greenland and Antarctic ice-caps were excluded from AR4 because the data was not considered reliable. (This omission hardly supports the notion that the IPCC seeks to exaggerate global warming trends).\nEach Arctic summer, sea-ice is melting faster than average predictions in the last IPCC report. The Arctic is experiencing a long-term loss of multi-year ice which is also accelerating.\nThe body of scientific literature has consistently shown that human greenhouse gas emissions are responsible for more global surface warming than has been observed over the past half century, whereas the IPCC only says that greenhouse gases are responsible for \"most\" observed warming over this timeframe.\nIn many similar cases, the evidence suggests that changes in climate are occurring faster, and with more intensity, than the IPCC have predicted. It is not credible to suggest the reports were biased in favour of the theory of anthropogenic global warming when the evidence demonstrates the IPCC were, in fact, so cautious.\nIn fact, there is evidence however to suggest that the exact opposite is actually the case, both in terms of the scientific evidence itself (see below) and the way the work of the IPCC is reported. A recent study (Freudenburg 2010) investigated what it calls 'the Asymmetry of Scientific Challenge', the phenomenon in which reports on science fail to evaluate all outcomes, favoring certain probabilities while ignoring others. They found that \"...new scientific findings were more than twenty times as likely to support the ASC perspective [that disruption through AGW may be far worse than the IPCC has suggested] than the usual framing of the issue in the U.S. mass media\".\nClaims that the IPCC is alarmist are not supported by evidence, and there are clear indications that the opposite may be the case.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 July 2015 by skeptickev. View Archives"
  },
  {
   "title": "IPCC overestimate temperature rise",
   "paragraph": "Monckton misuses IPCC equation\nLink to this page\nWhat the science says...\nSelect a level... Intermediate Advanced\nThe IPCC surface temperature projections have been exceptionally accurate thus far.\nClimate Myth...\nIPCC overestimate temperature rise\n\"The IPCC’s predicted equilibrium warming path bears no relation to the far lesser rate of “global warming” that has been observed in the 21st century to date.\" (Christopher Monckton)\n1990 IPCC FAR\nThe Intergovernmental Panel on Climate Change (IPCC) First Assessment Report (FAR)was published in 1990. The FAR used simple global climate models to estimate changes in the global-mean surface air temperature under various CO2 emissions scenarios. Details about the climate models used by the IPCC are provided in Chapter 6.6 of the report.\nThe IPCC FAR ran simulations using various emissions scenarios and climate models. The emissions scenarios included business as usual (BAU) and three other scenarios (B, C, D) in which global human greenhouse gas emissions began slowing in the year 2000. The FAR's projected BAU greenhouse gas (GHG) radiative forcing (global heat imbalance) in 2010 was approximately 3.5 Watts per square meter (W/m2). In the B, C, D scenarios, the projected 2011 forcing was nearly 3 W/m2. The actual GHG radiative forcing in 2011 was approximately 2.8 W/m2, so to this point, we're actually closer to the IPCC FAR's lower emissions scenarios.\nThe IPCC FAR ran simulations using models with climate sensitivities (the total amount of global surface warming in response to a doubling of atmospheric CO2, including amplifying and dampening feedbacks) of 1.5°C (low), 2.5°C (best), and 4.5°C (high) for doubled CO2 (Figure 1). However, because climate scientists at the time believed a doubling of atmospheric CO2 would cause a larger global heat imbalance than is currently believed, the actual climate sensitivities were approximatly 18% lower (for example, the 'Best' model sensitivity was actually closer to 2.1°C for doubled CO2).\nFigure 1: IPCC FAR projected global warming in the BAU emissions scenario using climate models with equilibrium climate sensitivities of 1.3°C (low), 2.1°C (best), and 3.8°C (high) for doubled atmospheric CO2\nFigure 2 accounts for the lower observed GHG emissions than in the IPCC BAU projection, and compares its 'Best' adjusted projection with the observed global surface warming since 1990.\nFigure 2: IPCC FAR BAU global surface temperature projection adjusted to reflect observed GHG radiative forcings 1990-2011 (blue) vs. observed surface temperature changes (average of NASA GISS, NOAA NCDC, and HadCRUT4; red) for 1990 through 2012.\nScorecard\nThe IPCC FAR 'Best' BAU projected rate of warming fro 1990 to 2012 was 0.25°C per decade. However, that was based on a scenario with higher emissions than actually occurred. When accounting for actual GHG emissions, the IPCC average 'Best' model projection of 0.2°C per decade is within the uncertainty range of the observed rate of warming (0.15 ± 0.08°C) per decade since 1990.\n1995 IPCC SAR\nThe IPCC Second Assessment Report (SAR)was published in 1995, and improved on the FAR by estimating the cooling effects of aerosols — particulates which block sunlight. The SAR included various human GHG emissions scenarios, so far its scenarios IS92a and b have been closest to actual emissions.\nThe SAR also maintained the \"best estimate\" equilibrium climate sensitivity used in the FAR of 2.5°C for a doubling of atmospheric CO2. However, as in the FAR, because climate scientists at the time believed a doubling of atmospheric CO2 would cause a larger global heat imbalance than is currently believed, the actual \"best estimate\" model sensitivity was closer to 2.1°C for doubled CO2.\nUsing that sensitivity, and the various IS92 emissions scenarios, the SAR projected the future average global surface temperature change to 2100 (Figure 3).\nFigure 3: Projected global mean surface temperature changes from 1990 to 2100 for the full set of IS92 emission scenarios. A climate sensitivity of 2.12°C is assumed.\nFigure 4 compares the IPCC SAR global surface warming projection for the most accurate emissions scenario (IS92a) to the observed surface warming from 1990 to 2012.\nFigure 4: IPCC SAR Scenario IS92a global surface temperature projection (blue) vs. observed surface temperature changes (average of NASA GISS, NOAA NCDC, and HadCRUT4; red) for 1990 through 2012.\nScorecard\nThe IPCC SAR IS92a projected rate of warming from 1990 to 2012 was 0.14°C per decade. This is within the uncertainty range of the observed rate of warming (0.15 ± 0.08°C) per decade since 1990, and very close to the central estimate.\n2001 IPCC TAR\nThe IPCC Third Assessment Report (TAR) was published in 2001, and included more complex global climate models and more overall model simulations. The IS92 emissions scenarios used in the SAR were replaced by the IPCC Special Report on Emission Scenarios (SRES), which considered various possible future human development storylines.\nThe IPCC model projections of future warming based on the varios SRES and human emissions only (both GHG warming and aerosol cooling, but no natural influences) are show in Figure 5.\nFigure 5: Historical human-caused global mean temperature change and future changes for the six illustrative SRES scenarios using a simple climate model. Also for comparison, following the same method, results are shown for IS92a. The dark blue shading represents the envelope of the full set of 35 SRES scenarios using the simple model ensemble mean results. The bars show the range of simple model results in 2100.\nThus far we are on track with the SRES A2 emissions path. Figure 6 compares the IPCC TAR projections under Scenario A2 with the observed global surface temperature change from 1990 through 2012.\nFigure 6: IPCC TAR model projection for emissions Scenario A2 (blue) vs. observed surface temperature changes (average of NASA GISS, NOAA NCDC, and HadCRUT4; red) for 1990 through 2012.\nScorecard\nThe IPCC TAR Scenario A2 projected rate of warming from 1990 to 2012 was 0.16°C per decade. This is within the uncertainty range of the observed rate of warming (0.15 ± 0.08°C) per decade since 1990, and very close to the central estimate.\n2007 IPCC AR4\nIn 2007, the IPCC published its Fourth Assessment Report (AR4). In the Working Group I (the physical basis) report, Chapter 8 was devoted to climate models and their evaluation. Section 8.2 discusses the advances in modeling between the TAR and AR4. Essentially, the models became more complex and incoporated more climate influences.\nAs in the TAR, AR4 used the SRES to project future warming under various possible GHG emissions scenarios. Figure 7 shows the projected change in global average surface temperature for the various SRES.\nFigure 7: Solid lines are multi-model global averages of surface warming (relative to 1980–1999) for the SRES scenarios A2, A1B, and B1, shown as continuations of the 20th century simulations. Shading denotes the ±1 standard deviation range of individual model annual averages. The orange line is for the experiment where concentrations were held constant at year 2000 values. The grey bars at right indicate the best estimate (solid line within each bar) and the likely range assessed for the six SRES marker scenarios.\nWe can therefore again compare the Scenario A2 multi-model global surface warming projections to the observed warming, in this case since 2000, when the AR4 model simulations began (Figure 8).\nFigure 8: IPCC AR4 multi-model projection for emissions Scenario A2 (blue) vs. observed surface temperature changes (average of NASA GISS, NOAA NCDC, and HadCRUT4; red) for 2000 through 2012.\nScorecard\nThe IPCC AR4 Scenario A2 projected rate of warming from 2000 to 2012 was 0.18°C per decade. This is within the uncertainty range of the observed rate of warming (0.06 ± 0.16°C) per decade since 2000, though the observed warming has likely been lower than the AR4 projection. As we will show below, this is due to the preponderance of natural temperature influences being in the cooling direction since 2000, while the AR4 projection is consistent with the underlying human-caused warming trend.\nIPCC Projections vs. Observed Warming Rates\nTamino at the Open Mind blog has also compared the rates of warming projected by the FAR, SAR, and TAR (estimated by linear regression) to the observed rate of warming in each global surface temperature dataset. The results are shown in Figure 9.\nFigure 9: IPCC FAR (yellow) SAR (blue), and TAR (red) projected rates of warming vs. observations (black) from 1990 through 2012.\nAs this figure shows, even without accounting for the actual GHG emissions since 1990, the warming projections are consistent with the observations, within the margin of uncertainty.\nRahmstorf et al. (2012) Verify TAR and AR4 Accuracy\nA paper published in Environmental Research Letters by Rahmstorf, Foster, and Cazenave (2012) applied the methodology of Foster and Rahmstorf (2011), using the statistical technique of multiple regression to filter out the influences of the El Niño Southern Oscillation (ENSO) and solar and volcanic activity from the global surface temperature data to evaluate the underlying long-term human-caused trend. Figure 10 compares their results with (pink) and without (red) the short-tern noise from natural temperature influences to the IPCC TAR (blue) and AR4 (green) projections.\nFigure 10: Observed annual global temperature, unadjusted (pink) and adjusted for short-term variations due to solar variability, volcanoes, and ENSO (red) as in Foster and Rahmstorf (2011). 12-month running averages are shown as well as linear trend lines, and compared to the scenarios of the IPCC (blue range and lines from the 2001 report, green from the 2007 report). Projections are aligned in the graph so that they start (in 1990 and 2000, respectively) on the linear trend line of the (adjusted) observational data.\nTAR Scorecard\nFrom 1990 through 2011, the Rahmstorf et al. unadjusted and adjusted trends in the observational data are 0.16 and 0.18°C per decade, respectively. Both are consistent with the IPCC TAR Scenario A2 projected rate of warming of approximately 0.16°C per decade.\nAR4 Scorecard\nFrom 2000 through 2011, the Rahmstorf et al. unadjusted and adjusted trends in the observational data are 0.06 and 0.16°C per decade, respectively. While the unadjusted trend is rather low as noted above, the adjusted, underlying human-caused global warming trend is consistent with the IPCC AR4 Scenario A2 projected rate of warming of approximately 0.18°C per decade.\nFrame and Stone (2012) Verify FAR Accuracy\nA paper published in Nature Climate Change, Frame and Stone (2012), sought to evaluate the FAR temperature projection accuracy by using a simple climate model to simulate the warming from 1990 through 2010 based on observed GHG and other global heat imbalance changes. Figure 11 shows their results. Since the FAR only projected temperature changes as a result of GHG changes, the light blue line (model-simuated warming in response to GHGs only) is the most applicable result.\nFigure 11: Observed changes in global mean surface temperature over the 1990–2010 period from HadCRUT3 and GISTEMP (red) vs. FAR BAU best estimate (dark blue), vs. projections using a one-dimensional energy balance model (EBM) with the measured GHG radiative forcing since 1990 (light blue) and with the overall radiative forcing since 1990 (green). Natural variability from the ensemble of 587 21-year-long segments of control simulations (with constant external forcings) from 24 Coupled Model Intercomparison Project phase 3 (CMIP3) climate models is shown in black and gray. From Frame and Stone (2012).\nFAR Scorecard\nNot surprisingly, the Frame and Stone result is very similar to our evaluation of the FAR projections, finding that they accurately simulated the global surface temperature response to the increased greenhouse effect since 1990. The study also shows that the warming since 1990 cannot be explained by the Earth's natural temperature variability alone, because the warming (red) is outside of the range of natural variability (black and gray).\nIPCC Trounces Contrarian Predictions\nAs shown above, the IPCC has thus far done remarkably well at predicting future global surface warming. The same cannot be said for the climate contrarians who criticize the IPCC and mainstream climate science predictions.\nRichard Lindzen\nOne year before the FAR was published, Richard Lindzen gave a talk at MIT in 1989 which we can use to reconstruct what his global temperature prediction might have looked like. In that speech, Lindzen remarked\n\"I would say, and I don't think I'm going out on a very big limb, that the data as we have it does not support a warming...I personally feel that the likelihood over the next century of greenhouse warming reaching magnitudes comparable to natural variability seems small\"\nThe first statement in this quote referred to past temperatures — Lindzen did not believe the surface temperature record was accurate, and did not believe that the planet had warmed from 1880 to 1989 (in reality, global surface temperatures warmed approximately 0.5°C over that timeframe). The latter statement suggests that the planet's surface would not warm more than 0.2°C over the following century, which is approximately the range of natural variability. In reality, as Frame and Stone showed, the surface warming already exceeded natural variability two decades after Lindzen's MIT comments.\nDon Easterbrook\nClimate contrarian geologist Don Easterbook has been predicting impending global cooling since 2000, based on expected changes in various oceanic cycles (including ENSO) and solar activity. Easterbrook made two specific temperature projections based on two possible scenarios. As will be shown below, neither has fared well.\nSyun-Ichi Akasofu\nIn 2009, Syun-Ichi Akasofu (geophysicist and director of the International Arctic Research Center at the University of Alaska-Fairbanks) released a paper which argued that the recent global warming is due to two factors: natural recovery from the Little Ice Age (LIA), and \"the multi-decadal oscillation\" (oceanic cycles). Based on this hypothesis, Akasofu predicted that global surface temperatures would cool between 2000 and 2035.\nJohn McLean\nJohn McLean is a data analyst and member of the climate contrarian group Australian Climate Science Coalition. He was lead author on McLean et al. (2009), which grossly overstates the influence of the El Niño Southern Oscillation (ENSO) on global temperatures. Based on the results of that paper, McLean predicted:\n\"it is likely that 2011 will be the coolest year since 1956 or even earlier\"\nIn 1956, the average global surface temperature anomaly in the three datasets (NASA GISS, NOAA NCDC, and HadCRUT4) was -0.21°C. In 2010, the anomaly was 0.61°C. Therefore, McLean was predicting a greater than 0.8°C global surface cooling between 2010 and 2011. The largest year-to-year average global temperature change on record is less than 0.3°C, so this was a rather remarkable prediction.\nIPCC vs. Contrarians Scorecard\nFigure 12 compares the four IPCC projections and the four contrarian predictions to the observed global surface temperature changes. We have given Lindzen the benefit of the doubt and not penalized him for denying the accuracy of the global surface temperature record in 1989. Our reconstruction of his prediction takes the natural variability of ENSO, the sun, and volcanic eruptions from Foster and Rahmstorf (2011) (with a 12-month running average) and adds a 0.02°C per decade linear warming trend. All other projections are as discussed above.\nFigure 12: IPCC temperature projections (red, pink, orange, green) and contrarian projections (blue and purple) vs. observed surface temperature changes (average of NASA GISS, NOAA NCDC, and HadCRUT4; red) for 1990 through 2012.\nNot only has the IPCC done remarkably well in projecting future global surface temperature changes thus far, but it has also performed far better than the few climate contrarians who have put their money where their mouth is with their own predictions.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 13 July 2015 by pattimer. View Archives"
  },
  {
   "title": "IPCC were wrong about Amazon rainforests",
   "paragraph": "Comparing what the IPCC and peer-reviewed science say about Amazonian forests\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe IPCC statement on Amazon rainforests was correct, and was incorrectly reported in some media.\nClimate Myth...\nIPCC were wrong about Amazon rainforests\n\"The IPCC also made false predictions on the Amazon rain forests, referenced to a non peer-reviewed paper produced by an advocacy group working with the WWF. This time though, the claim made is not even supported by the report and seems to be a complete fabrication.\" (EU Referendum)\nAn article in a British newspaper claimed that the Intergovernmental Panel on Climate Change (IPCC) published wrong information about the Amazon Rainforest in their 2007 report. The issue centred on the statement that about 40% of the Amazon was susceptible to the effects of drought, or more specifically \"slight reductions in rainfall\".\nThe Amazon is the world's largest tropical rainforest, and due to its immense size, has a global effect on the Earth's climate. Despite being well adapted and resilient to wet and dry periods which occur throughout the year, the rainforest is vulnerable to extended periods of drought. Any major decline in the health of the Amazon rainforest is likely to impact the world climate.\nThe skeptic claims relate to section 13.4.1 of the IPCC Fourth Assessment Report (2007) which made the statement: 'Up to 40% of the Amazonian forests could react drastically to even a slight reduction in precipitation; this means that the tropical vegetation, hydrology and climate system in South America could change very rapidly to another steady state, not necessarily producing gradual changes between the current and the future situation' (Rowell and Moore, 2000)\nThe reference is to a non-peer reviewed report prepared by the World Wide Fund for Nature (WWF) which itself cites an original peer reviewed study (Nepstad 1999) as the basis for the claim. The citations in the WWF and IPCC reports are not complete, Nepstad 1994, Nepstad 1999 and Nepstad 2004 support the claim that up to half the Amazon rainforest were severely affected by drought. Further studies, carried out since the 2007 IPCC report, reinforce the Amazon's susceptibilty to long term reductions in rainfall .\nThe IPCC could have avoided confusion by simply citing the peer reviewed studies themselves, rather than the WWF report and perhaps \"slight reduction\" should have been better defined or qualified. Despite the error in citation, the statement made by the IPCC is factually correct. Maybe the last word should go to the lead author of the papers upon which the statements were based, Daniel Nepstad, who made a public press release to clear up the mainstream media confusion over the subject. Nepstad concludes:\n\"In sum, the IPCC statement on the Amazon was correct. The report that is cited in support of the IPCC statement (Rowell and Moore 2000) omitted some citations in support of the 40% value statement.\nLast updated on 30 October 2010 by Rob Painting."
  },
  {
   "title": "IPCC were wrong about Himalayan glaciers",
   "paragraph": "Himalayan glaciers: how the IPCC erred and what the science says\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nGlaciers are in rapid retreat worldwide, despite 1 error in 1 paragraph in a 3000 page IPCC report.\nClimate Myth...\nIPCC were wrong about Himalayan glaciers\n\"In 1999 New Scientist reported a comment by the leading Indian glaciologist Syed Hasnain, who said in an email interview with this author that all the glaciers in the central and eastern Himalayas could disappear by 2035.\nHasnain, of Jawaharlal Nehru University in Delhi, who was then chairman of the International Commission on Snow and Ice's working group on Himalayan glaciology, has never repeated the prediction in a peer-reviewed journal. He now says the comment was \"speculative\".\nDespite the 10-year-old New Scientist report being the only source, the claim found its way into the IPCC fourth assessment report published in 2007. Moreover the claim was extrapolated to include all glaciers in the Himalayas.\" (Fred Pearce)\nThe IPCC made an error about the Himalayan glaciers. Section 10.6.2 of the Fourth Assessment Report (AR4) states, “the likelihood of [the Himalayan Glaciers] disappearing by the year 2035 and perhaps sooner is very high if the Earth keeps warming at the current rate.” This statement did not come from peer-reviewed literature, nor did it meet the IPCC standards of evidence.\nThe error has raised some criticisms - both legitimate and illegitimate - about the the IPCC, the AR4, and climate science in general:\nDid the IPCC respond to this error quickly and diligently? The answer here is unfortunately no. According to a review by the InterAcademy Council on the IPCC processes and procedures, the IPCC took more than a month to respond to the Himalayan Glacier error, and even then did not explicitly acknowledge the error or issue a retraction. To make matters worse, it has been documented that the IPCC had responded more quickly to other supposed errors in the report (Leake, 2010; Reuters, 2010). Though the IPCC has been recognized for its scientific contributions, there is certainly room for improvement in terms of communications.\nIs the AR4 terribly flawed? It is important to note that this is one error in a roughly 3000 page technical document, an error percentage similar to the Encyclopedia Britannica. The 2035 claim was not included in the Technical Summary, the Summary for Policymakers, or the Synthesis Report.\nDoes this error show the IPCC has an ‘alarmist’ bias – a tendency to exaggerate the negative impacts of climate change? In fact, there are far more documented instances of the AR4 being too conservative, rather than too alarmist, on emissions scenarios, sea level rise, and Arctic sea-ice melt.\nDoes this in anyway undermine climate science in general? To claim this error undermines the basic conclusions of climate change is absurd. The error is part of Working Group II: Impacts, Adaptation and Vulnerability, not Working Group I: The Physical Science Basis. Anthropogenic climate change is still supported by multiple lines of independent empirical evidence, and nearly every national and international scientific body.\nSo what does the peer-reviewed science say about the Himalayan Glaciers?\nMany of the Himalayan Glaciers are retreating at an accelerating rate (Ren 2006) and roughly 500 million people depend on the melt water from these glaciers (Kehrwald 2008).\nThe IPCC made an unfortunate error in a very long technical document. Moreover, the response to this error was far from exemplary. Highlighting this error to undermine climate science, however, is a classic example of cherry picking – a dangerous game to play with 500 million livelihoods at stake.\nLast updated on 17 September 2010 by Nicholas Berini."
  },
  {
   "title": "IPCC ‘disappeared’ the Medieval Warm Period",
   "paragraph": "IPCC update temperature graphs with best available data\nLink to this page\nWhat the science says...\nThe original global temperature schematic which appeared in the IPCC First Assessment Report and seemed to show the Medieval Warm Period (MWP) hotter than Present was based on the central England temperature record, and ended in the 1950s. It was only a schematic, and based on one isolated geographic location. Subsequent IPCC reports showed actual hemispheric temperature reconstructions. They did not \"disappear\" the MWP, they simply presented the best available data.\nClimate Myth...\nIPCC ‘disappeared’ the Medieval Warm Period\n\"by the 2001 [IPCC] climate assessment...the Medieval Warm Period had been ingeniously wiped out. This was the headline graph in the 2001 report. It was reproduced six times in the report, large, and in full color, the only graph to be so favored. However, the graph was bogus. The warm period during the Middle Ages had been artificially eradicated\" (Christopher Monckton)\nOrigins of the Myth\nThe myth that the Intergovernmental Panel on Climate Change (IPCC) \"disappeared\" the Medieval Warm Period (MWP) seems to be based on Figure 7.1c from the IPCC First Assessment Report (FAR):\nFigure 1: IPCC FAR Figure 7.1c - Schematic diagram of global temperature variations for the last thousand years. The dotted line represents conditions near the beginning of the twentieth century.\nAs you can see, IPCC FAR Figure 7.1c appears to show the MWP quite prominently, and warmer than the temperature as the end of the graph. But it's a rather strange figure - the temperature axis doesn't even have any numbers, and it looks hand-drawn. Where did it come from?\nPulling a Lamb out of a Hat\nJones et al. (2009) explore the origins of this figure. They note that the figure caption specifically stated that it was a schematic diagram, and not an actual temperature reconstruction.\n\"as far as palaeoclimatologists were concerned the diagram was nothing more than how it was originally described in the caption: a schematic.\"\nJones et al. trace the schematic diagram back to a series used by H.H. Lamb, representative of central England, last published by Lamb (1982). However, Lamb is plotting 50-year averages here, and the final data point appears to be 1950. Jones et al. superimpose IPCC FAR Figure 7.1c (black) with Lamb's central England temperature (red) and added the Central England Temperature data up to 2007 (blue):\nFigure 2: The black curve and the x- and y-axes are a redrawn version of IPCC FAR Figure 7.1c. The red curve is from Lamb (1982). The amplitude of this curve has been scaled to correspond to that of the black curve. The Lamb (1982) time series does have an explicit temperature scale, and the best-fit scaling between this curve and the IPCC curve indicates that one tick-mark interval on the IPCC figure corresponds almost exactly with 1°C. The blue curve is a smoothed version of the annual instrumental Central England Temperature record from Manley (1974, updated) including the last complete year of 2007. This has been smoothed with a 50-yr Gaussian weighted filter with padding.\nCentral England temperatures have risen by over 1°C since Lamb's last measurement. Jones et al. also note about Lamb's schematic:\n\"At no place in any of the Lamb publications is there any discussion of an explicit calibration against instrumental data, just Lamb’s qualitative judgement and interpretation of what he refers to as the ‘evidence’....Greater amounts of documentary data (than available to Lamb in the early 1970s) were collected and used in the Climatic Research Unit in the 1980s. These studies suggest that the sources used and the techniques employed by Lamb were not very robust (see, eg, Ogilvie and Farmer, 1997).\"\nIn short, Figure 7.1c from the IPCC FAR was based on Lamb's approximation of the central England temperature. It was intended only as a schematic diagram, and known not to accurately reflect the global average temperature.\nAdvancements in Science\nThe Lamb diagram was dropped from the Supplementary IPCC report in 1992. Subsequent IPCC reports showed the first northern hemisphere temperature reconstructions based on proxy data. The IPCC Second Assessment Report (SAR) examined Bradley and Jones (1993). The IPCC Third Assessment Report (TAR) examined Jones et al. (1998); Briffa (2000); Crowley and Lowery (2000); and of course Mann, Bradley, and Hughes (1998) and (1999) (the so-called \"hockey stick\").\nThe IPCC TAR featured the \"hockey stick\" fairly prominently. The rather widespread belief that \"the hockey stick is broken\" and \"suppressed\" the MWP may contribute to this myth that the IPCC \"disappeared\" the MWP. However, although the MWP was not shown as prominently in the \"hockey stick\" as most other reconstructions due to the statistical methods employed in the study, all subsequent northern hemisphere temperature reconstructions have confirmed the general \"hockey stick\" shape.\nFigure 3: Various northern hemisphere temperature reconstructions using climate proxy records (Source: NOAA NCDC)\nFurthermore, all of these reconstructions demonstrate that the present average northern hemisphere temperature is likely hotter than at the peak of the MWP. It's important to note that northern hemisphere temperature reconstructions are obviously much more representative of global temperatures than the temperature in central England. And it's also worth noting that most of the warming during the MWP was geographically located at high latitudes in the northern hemisphere.\nMagicians without Secrets\nOn top of all this, the IPCC is very careful to discuss exactly what we know about MWP temperatures. Here is an excerpt from the IPCC Fourth Assessment Report (WG1 Chapter 6 page 468):\n\"The evidence currently available indicates that NH mean temperatures during medieval times (950–1100) were indeed warm in a 2-kyr context and even warmer in relation to the less sparse but still limited evidence of widespread average cool conditions in the 17th century (Osborn and Briffa, 2006). However, the evidence is not sufficient to support a conclusion that hemispheric mean temperatures were as warm, or the extent of warm regions as expansive, as those in the 20th century as a whole, during any period in medieval times (Jones et al., 2001; Bradley et al., 2003a,b; Osborn and Brif a, 2006).\"\nThe bottom line is that the IPCC did not \"disappear\" the MWP. Rather, the IPCC has done its job correctly by evaluating the best available data and scientific studies at the time of each report. As climate scientists obtained more temperature data, it became clearer and clearer that the MWP was not hotter than present temperatures. A schematic of central England temperatures to 1950 is not quite the same as a reconstruction of northern hemisphere temperatures to 2007! The IPCC has done some impressive work, but it's no Harry Houdini.\nContradicting Low Climate Sensitivity\nA favorite \"skeptic\" argument is \"climate sensitivity is low\". But if the MWP was particularly hot, that means there was a fairly large temperature change about 1,000 years ago. The hotter the peak of the MWP, the larger the temperature change, and the more sensitive the climate was to the factors causing that change (mainly increased solar activity and decreased volcanic activity). Arguing for a hot MWP is arguing for a high climate sensitivity to these natural factors, and if the climate is sensitive to an energy imbalance caused by a change in solar or volcanic activity, there's no reason it wouldn't also be sensitive to changes in greenhouse gases as well. In short, a hot MWP also means a high climate sensitivity. Sorry \"skeptics\", you can't have it both ways.\nIntermediate rebuttal written by dana1981\nUpdate August 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 4 August 2015 by MichaelK. View Archives"
  },
  {
   "title": "It cooled mid-century",
   "paragraph": "Why did climate cool in the mid-20th Century?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nMid-century cooling involved aerosols and is irrelevant for recent global warming.\nClimate Myth...\nIt cooled mid-century\n\"It was the post war industrialization that caused the rapid rise in global CO2 emissions, but by 1945 when this began, the Earth was already in a cooling phase that continued until 1975. With 32 years of rapidly increasing global temperatures and only a minor increase in global CO2 emissions, followed by 33 years of slowly cooling global temperatures with rapid increases in global CO2 emissions, it was deceitful for the IPCC to make any claim that CO2 emissions were primarily responsible for observed 20th century global warming.\" (Norm Kalmanovitch)\nAlthough temperatures increased overall during the 20th century, three distinct periods can be observed. Global warming occurred both at the beginning and at the end of the 20th century, but a cooling trend is seen from about 1940 to 1975. As a result, changes in 20th century trends offer a good framework through which to understand climate change and the role of numerous factors in determining the climate at any one time.\nEarly and late 20th century warming has been explained primarily by increasing solar activity and increasing CO2 concentrations, respectively, with other factors contributing in both periods. So what caused the cooling period that interrupted the overall trend in the middle of the century? The answer seems to lie in solar dimming, a cooling phenomenon caused by airborne pollutants.\nThe main culprit is likely to have been an increase in sulphate aerosols, which reflect incoming solar energy back into space and lead to cooling. This increase was the result of two sets of events.\nIndustrial activities picked up following the Second World War. This, in the absence of pollution control measures, led to a rise in aerosols in the lower atmosphere (the troposphere).\nA number of volcanic eruptions released large amounts of aerosols in the upper atmosphere (the stratosphere).\nCombined, these events led to aerosols overwhelming the warming trend at a time when solar activity showed little variation, leading to the observed cooling. Furthermore, it is possible to draw similar conclusions by looking at the daily temperature cycle. Because sunlight affects the maximum day-time temperature, aerosols should have a noticeable cooling impact on it. Minimum night-time temperatures, on the other hand, are more affected by greenhouse gases and therefore should not be affected by aerosols. Were these differences observed? The answer is yes: maximum day-time temperatures fell during this period but minimum night-time temperatures carried on rising.\nThe introduction of pollution control measures reduced the emission of sulphate aerosols. Gradually the cumulative effect of increasing greenhouse gases started to dominate in the 1970s and warming resumed.\nAs a final point, it should be noted that in 1945, the way in which sea temperatures were measured changed, leading to a substantial drop in apparent temperatures. Once the data are corrected, it is expected that the cooling trend in the middle of the century will be less pronounced.\nBasic rebuttal written by Anne-Marie Blackburn\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 9 July 2015 by pattimer. View Archives"
  },
  {
   "title": "It warmed before 1940 when CO2 was low",
   "paragraph": "What caused early 20th Century warming?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nEarly 20th century warming is due to several causes, including rising CO2.\nClimate Myth...\nIt warmed before 1940 when CO2 was low\n\"Of the rise in temperature during the 20th century, the bulk occurred from 1900 to 1940. It was followed by the aforementioned cooling trend from 1940 to around 1975. Yet the concentration of greenhouse gases was measurably higher in that later period than in the former. That drop in temperature came after what was described in the National Geographic as 'six decades of abnormal warmth'.\" (James Schlesinger)\nThe climate at any one time is affected by several factors which can act independently or together. The main factors include solar variability, volcanic activity, atmospheric composition, the amount of sunlight reflected back into space, ocean currents and changes in the Earth's orbit.\nBefore 1940, the increase in temperature is believed to have been caused mainly by two factors:\nIncreasing solar activity; and\nLow volcanic activity (as eruptions can have a cooling effect by blocking out the sun).\nOther factors, including greenhouse gases, also contributed to the warming and regional factors played a significant role in increasing temperatures in some regions, most notably changes in ocean currents which led to warmer-than-average sea temperatures in the North Atlantic. Does this mean that solar activity is also primarily responsible for late 20th century warming? In short, no. Solar activity since the 1950s has been relatively stable and therefore cannot explain recent trends. Similarly, increased volcanic activity may actually have had a cooling effect in recent decades. On the other hand, greenhouse gas concentrations, which were relatively low pre-1940, have increased considerably and are now dominating the climate system. This highlights the need to look at all factors when determining which factors are likely to be affecting climate at any one time.\nIn short, there's no reason to assume that because the sun was responsible for early 20th century, it is responsible for all warming. The evidence strongly suggests that current warming is mainly the result of increasing greenhouse gas levels.\nBasic rebuttal written by Anne-Marie Blackburn\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 9 July 2015 by pattimer. View Archives"
  },
  {
   "title": "It's a 1500 year cycle",
   "paragraph": "What relevance does past natural cycles have with recent global warming?\nLink to this page\nWhat the science says...\nAncient natural cycles are irrelevant for attributing recent global warming to humans.\nClimate Myth...\nIt's a 1500 year cycle\n“Every one of them [climate records of the past] show this (roughly 1500 year) cycle. It was first discovered in ice cores in Greenland. Then it was seen in ocean sediments in the Atlantic. And now it’s been found everywhere, including in stalagmites in caves. […] it could well account for the current warming.” (Fred Singer)\nFor someone to state that the global warming we’re experiencing is actually part of a 1500-year natural cycle of global temperature variation is interesting for two reasons. First — in contradiction to the great majority of sceptic arguments that actually deny global warming — this argument requires that the person promoting this explanation must first agree that climate change is, indeed, happening.\nSecond, they must also refuse to accept the greenhouse effect, a theory first proposed more than 100 years ago and which even many sceptics of the human contribution to climate change, readily accept.\nThe 1500-year cycle in question has been observed mainly through ice core data as a warming in the northern hemisphere matched at precisely the same time by a cooling in the southern hemisphere. So it’s a heat distribution issue: a global temperature ‘see-saw’ effect. The total heat in the global system remains constant.\nIn contrast, human-produced global warming has been caused by the rapidly increasing CO2 concentrations in the atmosphere over the last 200 years — rising over 400 parts per million after remaining below 300 parts per million for the previous 800,000 years. And unlike natural heat variations, the current temperature increase caused by CO2 is being recorded occurring all around the globe – on the ground, in the air and in the oceans.\nLast updated on 16 October 2018 by John Russell. View Archives"
  },
  {
   "title": "It's a climate regime shift",
   "paragraph": "Internal variability, climate shifts and human-caused global warming\nLink to this page\nWhat the science says...\nA full reading of Tsonis and Swanson's research shows that internal variability from climate shifts merely cause temporary slow downs or speeding up of the long-term warming trend. When the internal variability is removed from the temperature record, what we find is nearly monotonic, accelerating warming throughout the 20th Century.\nClimate Myth...\nIt's a climate regime shift\n\"For small changes in climate associated with tenths of a degree, there is no need for any external cause. The earth is never exactly in equilibrium. The motions of the massive oceans where heat is moved between deep layers and the surface provides variability on time scales from years to centuries. Recent work (Tsonis et al, 2007), suggests that this variability is enough to account for all climate change since the 19th Century.\" (Richard Lindzen)\nThe work of Tsonis and Swanson are often cited as evidence against man-made global warming. Their research suggests our climate is subject to dramatic regime shifts. At key moments, the climate shifts from a warm regime to a cool regime, or vica versa. They claim climate shifts occured around 1910, 1940, 1976 and 2001. Some have interpreted this work to say climate shifts can explain the last few decades of global warming. Richard Lindzen's take is that 'this variability is enough to account for all climate change since the 19th Century'. Is this what Tsonis and Swanson's research shows? The best people to answer this question are the authors themselves as they address this very question in their peer-reviewed work.\nThe initial paper by Tsonis, Swanson and Kravtsov proposed that climate is subject to a phenomenon called synchronised chaos (Tsonis et al 2007). When examining a number of ocean cycles such as the El Nino Southern Oscillation and North Atlantic Oscillation, it was observed that the various ocean cycles synchronised at certain moments after which climate seemed to shift to a new regime. In 1910, the synchronisation was followed by a warmer regime and several decades of warming. Another synchronisation occured in 1940, switching to a cooler regime. This coincided with mid-century cooling from 1940 to 1970. In the 1970s, the planet began warming again.\nFigure 1: HadCRUT3 global mean temperature over the 20th century, with approximate breaks in temperature indicated. The cross-hatched areas indicated time periods when synchronization is accompanied by increasing coupling (Swanson & Tsonis 2009).\nConventional understanding for the switch to warming in the 1970s is that warming from CO2 overcame cooling from forcings such as sulfate aerosols. Tsonis and Swanson suggest an 'alternative hypothesis, namely that the climate shifted after the 1970s event to a different state of a warmer climate, which may be superimposed on an anthropogenic warming trend'. It's this final phrase, 'superimposed on an anthropogenic warming trend', that Swanson and Tsonis explore further in a subsequent research.\nIn 2009, they continue to examine the coupling of ocean cycles, stressing 'caution that the shifts described here are presumably superimposed upon a long term warming trend due to anthropogenic forcing' (Swanson & Tsonis 2009). They extend their analysis further in a paper that uses climate modelling to separate man-made and natural variability (Swanson et al 2009). When internal variability is filtered from the smoothed observed temperature (solid black line), the cleaned signal (dashed line) shows nearly monotonic warming throughout the 20th Century. In fact, the cleaned signal fits a quadratic shape which indicates the warming is accelerating.\nFigure 2: Observed GISS 21-year running mean global mean surface temperature (heavy solid) along with that temperature cleaned of the internal signal (dashed). The cleaned global mean temperature warms monotonically, and closely resembles a quadratic fit to the observed 20th century global mean temperature (thin solid) (Swanson 2009).\nIf climate shifts do actually occur, Tsonis and Swanson's research finds they are not responsible for the warming found over the 20th Century. Instead, they superimpose variability over the long-term trend which is that of steadily accelerating warming. This is consistent with observations which find the planet has been accumulating heat since 1950 (Murphy 2009). Climate shifts do not stop the planet's energy imbalance. They merely cause temporary slow downs or speeding up of surface temperature warming.\nNevertheless, the theory of climate shifts has some unresolved issues. A key result of Tsonis and Swanson's work is that a shift to a cooler regime occured around 2001/2002. This shift is more marked in the HadCRUT record which is not a global temperature record. When Arctic regions are included, the global warming trend is greater in recent years and hence the 2001/2002 shift is not so pronounced. Hence the theory is dependent somewhat on an incomplete global record.\nAnother issue discussed in Swanson 2009 is that if climate is more sensitive to internal variability than currently thought, this would also mean climate is more sensitive to imposed forcings. This includes radiative forcings such as a warming sun, cooling from sulfate aerosols or warming from CO2. This leads to a crucial question that the authors themselves raise but don't answer. Conventional thought is that the warming sun and reduced volcanic activity caused much of the early 20th Century warming. Similarly, cooling from increased sulfate aerosols was a major contributor to mid-century cooling. In suggesting climate shifts as the cause, the authors offer no physical explanation as to why the warming sun and cooling aerosols didn't have their expected effect?\nNevertheless, if these issues are resolved and Tsonis and Swanson's theory is found to be valid, it's clear that climate shifts do not invalidate the human influence on climate. On the contrary, they show that underneath internal variability is a long-term trend. Tsonis and Swanson's analysis finds that imposed forcings have exerted a monotonic and accelerating warming trend throughout the 20th Century.\nIntermediate rebuttal written by vrooomie\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 29 October 2016 by pattimer. View Archives"
  },
  {
   "title": "It's a climate shift step function caused by natural cycles",
   "paragraph": "Global warming and step functions\nLink to this page\nWhat the science says...\nA linear warming trend plus natural cycles can be mistaken for a step function, but physically the global warming is caused by an external radiative forcing (i.e. human greenhouse gas emissions). Natural cycles do not create or retain heat, and thus do not cause long-term global warming. If natural cycles were to cause a 'step up' in their positive phase, then they would also cause a 'step down' in their negative phase, thus having zero net long-term effect on global temperatures.\nClimate Myth...\nIt's a climate shift step function caused by natural cycles\n\"the temperature increase in the second half of the 20th century could have taken place in steps driven by major ENSO events\" (Jens Raunsø Jensen)\nAs we discussed in Going Down the Up Escalator, Part 1, it's a very common mistake - even amongst some climate scientists - to confuse short-term climate noise with long-term global warming signal. Our very popular Figure 1 below illustrates this confusion very nicely:\nFigure 1: BEST land-only surface temperature data (green) with linear trends applied to the timeframes 1973 to 1980, 1980 to 1988, 1988 to 1995, 1995 to 2001, 1998 to 2005, 2002 to 2010 (blue), and 1973 to 2010 (red). Created by Dana Nuccitelli. Hat-tip to Skeptical Science contributor Sphaerica for identifying all of these \"cooling trends.\" (Figure 1 has been added to the SkS Climate Graphics Page).\nSome climate \"skeptics\" have suggested explanations as to why their interpretation of global warming shown in Figure 1 is actually the correct one, arguing that global warming is really just a 'step function' caused by natural cycles and 'climate shifts.'\nStep Functions\nGuest poster Jens Raunsø Jensen on WUWT created a very similar graphic to Figure 1, trying to argue that global warming can be modeled with step functions rather than a linear trend.\nFigure 2: Jensen's plot of Denmark and global surface temperature, modeled as step functions triggered by Pacific Decadal Oscillation (PDO) shifts and El Niño events\nOn Judith Curry's blog, \"skeptic\" Donald Rapp makes much the same argument:\n\"Thorne et al. (2011) seemed not to be able to recognize the obvious fact shown in Figure 1 that tropospheric temperatures made a step function rise after the great El Niño of 1998 and was fairly constant before and after.\"\nFigure 3: Donald Rapp's tropospheric temperature \"step function\"\nNote by the way that each \"skeptic\" analysis relies on either HadCRUT or UAH/RSS temperature data, which are either known to or very possibly have a cool bias.\nBob Tisdale frequently makes the same argument on WUWT:\n\"The El Niño events of 1986/87/88 and 1997/98 are shown to be the cause of the rise in sea surface temperatures since November 1981, not anthropogenic greenhouse gases.\"\nAnd Roger Pielke Sr. echoes a similar argument from David Douglass on his blog.\n\"[Douglass] would have “shouted” that calculating trends across a climate shift has no meaning.\"\nA \"Climate Shift\"?\nThis concept of a short-term \"climate shift\" may come from Tsonis et al. 2007 and Swanson and Tsonis 2009, whose work John Cook has previously discussed. In short, Swanson and Tsonis hypothesized that when various natural oceanic cycles (PDO, AMO, etc.) synchronize (i.e. in their positive or negative phases), they can cause a short-term warming or cooling which could be described as a \"climate regime shift.\"\nAs John discussed in his post, there are some issues with this hypothesis (i.e. we know observed forcings like solar irradiance and aerosols can explain most past short-term temperature changes without requiring major contributions from these \"climate shifts\"). But more importantly, as Swanson and Tsonis put it, these shifts are superimposed on an anthropogenic warming trend. As Swanson himself put it,\n\"What do our results have to do with Global Warming, i.e., the century-scale response to greenhouse gas emissions? VERY LITTLE, contrary to claims that others have made on our behalf.\"\nFurther, Swanson 2009 discussed that if climate is more sensitive to internal variability than currently thought, this would also mean climate is more sensitive to imposed forcings, which means that we would still expect CO2 and other anthropogenic forcings to cause substantial warming.\nCreating Our Own Artificial \"Climate Shifts\"\nWe can very easily demonstrate the fundamental flaw in this \"climate shifts\" argument by creating our own simulated temperature data. Figure 4 shows the following panels:\n1) A 0.2°C per decade global warming trend\n2) Two 'natural cycles' (cosine functions) both with 0.15°C amplitude and periods of 10 and 20 years, respectively\n3) Random noise with 0.07°C amplitude\n4) The sum of the warming trend, cycles, and noise\n5) The sum fit with a step function with three steps: linear trends from 1950 to 1963, 1967 to 1986, and 1987 to 2003 (light blue)\n6) The sum with a linear trend fit from 1950 to 2010. The linear trend (0.21°C per decade) is almost identical to the global warming signal (0.2°C per decade)\nFigure 4: Simulated temperature data with a global warming signal (0.2°C/decade), natural cycles of 10 and 20 years, random noise, and the sum fit by a step function (blue and black) and a linear trend (red). Created by Dana Nuccitelli.\nThe point here is of course that while natural cycles superimposed on a linear global warming trend can be fit with a step function, the global warming is entirely caused by the linear trend (the radiative forcing, in the real world). The natural cycles have zero long-term trend and thus while they contribute to short-term temperature changes, contribute nothing to the long-term global warming.\nThus the \"skeptic\" conclusion that the step function is the appropriate model, and that natural cycles are what's causing global warming, is simply physically wrong.\nPhysical Reality\nThe Achilles Heel of the \"climate shifts are causing global warming\" hypothesis is that it's simply not a physical argument, for several reasons.\n1. Why would the average global temperature jump to a new warmer state after an El Niño or positive PDO, but not drop back down to its cooler state after a La Niña or negative PDO, for example? If the \"skeptic\" climate shift argument is correct, the planet will continue to warm indefinitely.\n2. Oceanic cycles don't create or retain heat, they simply move it around. So if these climate shifts are causing the surface air to warm, they should also be causing the oceans to cool. That simply is not the case. The oceans, surface, and troposphere are all warming because the Earth's total heat content is increasing (Figure 5).\nFigure 5: Land, atmosphere, and ice heating (red), 0-700 meter OHC increase (light blue), 700-2,000 meter OHC increase (dark blue). From Nuccitelli et al. (2012).\n3. The reason global heat content is increasing is that there is a global energy imbalance caused primarily by the anthropogenic greenhouse gas forcing. Arguing that the warming is caused by a \"climate shift\" ignores the physical reality that this forcing and energy imbalance must result in global warming. Not coincidentally, there is a strong correlation between the temperature and greenhouse gas forcing increases (Figure 6).\nFigure 6: BEST land-only surface tempeature (yellow, left axis) vs. greenhouse gas radiative forcing (black, right axis). Created by Robert Way.\nThe bottom line here is that climate \"skeptics\" need to stop looking for excuses to use short-term noise to argue that global warming has stopped. It hasn't, and it won't until we get the radiative forcing in Figure 5 under control, which won't happen until we get our greenhouse gas emissions under control. That's simply physical reality, and denying physical reality won't change it.\nThe George Santayana quote \"Those who cannot remember the past are condemned to repeat it\" comes to mind. Climate \"skeptics\" keep arguing that the anthropogenic global warming theory is wrong by fitting trends to short-term data, and ignoring the underlying long-term trend. In each case their arguments have been proven demonstrably wrong, and yet they keep coming up with new variations on the same fundamentally flawed premise. All the while, the long-term global warming trend continues upward.\nLast updated on 13 October 2012 by dana1981. View Archives"
  },
  {
   "title": "It's a natural cycle",
   "paragraph": "Human fingerprints on climate change rule out natural cycles\nLink to this page\nWhat the science says...\nA natural cycle requires a forcing, and no known forcing exists that fits the fingerprints of observed warming - except anthropogenic greenhouse gases.\nClimate Myth...\nIt's a natural cycle\n\"Global warming (i.e, the warming since 1977) is over. The minute increase of anthropogenic CO2 in the atmosphere (0.008%) was not the cause of the warming—it was a continuation of natural cycles that occurred over the past 500 years.\" (Don Easterbrook)\n\"What if global warming is just a natural cycle?\" This argument is, perhaps, one of the most common raised by the average person, rather than someone who makes a career out of denying climate change. Cyclical variations in climate are well-known to the public; we all studied the ice ages in school. However, climate isn't inherently cyclical.\nA common misunderstanding of the climate system characterizes it like a pendulum. The planet will warm up to \"cancel out\" a previous period of cooling, spurred by some internal equilibrium. This view of the climate is incorrect. Internal variability will move energy between the ocean and the atmosphere, causing short-term warming and cooling of the surface in events such as El Nino and La Nina, and longer-term changes when similar cycles operate on decadal scales. However, internal forces do not cause climate change. Appreciable changes in climate are the result of changes in the energy balance of the Earth, which requires \"external\" forcings, such as changes in solar output, albedo, and atmospheric greenhouse gases. These forcings can be cyclical, as they are in the ice ages, but they can come in different shapes entirely.\nFor this reason, \"it's just a natural cycle\" is a bit of a cop-out argument. The Earth doesn't warm up because it feels like it. It warms up because something forces it to. Scientists keep track of natural forcings, but the observed warming of the planet over the second half of the 20th century can only be explained by adding in anthropogenic radiative forcings, namely increases in greenhouse gases such as carbon dioxide.\nOf course, it's always possible that some natural cycle exists, unknown to scientists and their instruments, that is currently causing the planet to warm. There's always a chance that we could be totally wrong. This omnipresent fact of science is called irreducible uncertainty, because it can never be entirely eliminated. However, it's very unlikely that such a cycle exists.\nFirstly, the hypothetical natural cycle would have to explain the observed \"fingerprints\" of greenhouse gas-induced warming. Even if, for the sake of argument, we were to discount the direct measurements showing an increased greenhouse effect, other lines of evidence point to anthropogenic causes. For example, the troposphere (the lowest part of the atmosphere) is warming, but the levels above, from the stratosphere up, are cooling, as less radiation is escaping out to space. This rules out cycles related to the Sun, as solar influences would warm the entire atmosphere in a uniform fashion. The only explanation that makes sense is greenhouse gases.\nWhat about an internal cycle, perhaps from volcanoes or the ocean, that releases massive amounts of greenhouse gases? This wouldn't make sense either, not only because scientists keep track of volcanic and oceanic emissions of CO2 and know that they are small compared to anthropogenic emissions, but also because CO2 from fossil fuels has its own fingerprints. Its isotopic signature is depleted in the carbon-13 isotope, which explains why the atmospheric ratio of carbon-12 to carbon-13 has been going up as anthropogenic carbon dioxide goes up. Additionally, atmospheric oxygen (O2) is decreasing at the same rate that CO2 is increasing, because oxygen is consumed when fossil fuels combust.\nA natural cycle that fits all these fingerprints is nearly unfathomable. However, that's not all the cycle would have to explain. It would also have to tell us why anthropogenic greenhouse gases are not having an effect. Either a century of basic physics and chemistry studying the radiative properties of greenhouse gases would have to be proven wrong, or the natural cycle would have to be unbelievably complex to prevent such dramatic anthropogenic emissions from warming the planet.\nIt is indeed possible that multidecadal climate variability, especially cycles originating in the Atlantic, could be contributing to recent warming, particularly in the Arctic. However, the amplitude of the cycles simply can't explain the observed temperature change. Internal variability has always been superimposed on top of global surface temperature trends, but the magnitude - as well as the fingerprints - of current warming clearly indicates that anthropogenic greenhouse gases are the dominant factor.\nDespite all these lines of evidence, many known climatic cycles are often trumpeted to be the real cause, on the Internet and in the media. Many of these cycles have been debunked on Skeptical Science, and all of them either aren't in the warming phases, don't fit the fingerprints, or both.\nFor example, we are warming far too fast to be coming out of the last ice age, and the Milankovitch cycles that drive glaciation show that we should be, in fact, very slowly going into a new ice age (but anthropogenic warming is virtually certain to offset that influence).\nThe \"1500-year cycle\" that S. Fred Singer attributes warming to is, in fact, a change in distribution of thermal energy between the poles, not a net increase in global temperature, which is what we observe now.\nThe Little Ice Age following the Medieval Warm Period ended due to a slight increase in solar output (changes in both thermohaline circulation and volcanic activity also contributed), but that increase has since reversed, and global temperature and solar activity are now going in opposite directions. This also explains why the 11-year solar cycle could not be causing global warming.\nENSO (El Nino Southern Oscillation) and PDO (Pacific Decadal Oscillation) help to explain short-term variations, but have no long-term trend, warming or otherwise. Additionally, these cycles simply move thermal energy between the ocean and the atmosphere, and do not change the energy balance of the Earth.\nAs we can see, \"it's just a natural cycle\" isn't just a cop-out argument - it's something that scientists have considered, studied, and ruled out long before you and I even knew what global warming was.\nIntermediate rebuttal written by climatesight\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 17 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "It's aerosols",
   "paragraph": "Aerosols have partially masked human-caused global warming\nLink to this page\nWhat the science says...\nThe global dimming trend reversed around 1990 - 15 years after the global warming trend began in the mid 1970's.\nClimate Myth...\nIt's aerosols\nIs a Thinning Haze Unveiling the Real Global Warming? (Kerr 2007) points out that the sunlight-reflecting haze that cools much of the planet seems to have thinned over the past decade or so. If real, the thinning would not explain away a century of global warming but it might explain the unexpectedly strong global warming of late, the accelerating loss of glacial ice and much of rising sea levels.\nThe global dimming trend reversed around 1990 - 15 years after the global warming trend began around 1975. So it can't explain what began the global warming trend. Aerosols have a cooling effect on Earth's climate. When aerosols thin, the result is a lack of cooling, not a warming effect. That's not just semantics - take aerosols out of the equation and in the absence of any other forcings, global temperatures would remain steady.\nSo what is driving the warming? In the past, solar variations have been the main driver in climate change. A comparison of solar activity and temperature over the past 1150 years shows a close correlation between solar activity and temperature. However, the correlation ends around 1980 when temperatures started rising but solar levels remained steady.\nAnother suspect in climate change is cosmic radiation which is thought to increase cloud cover (hence cooling the earth). However, again there has been no correlation between temperature and cosmic ray flux since 1970. In fact, all the usual suspects in natural climate change - volcanic activity, orbit wobbles, solar variations are conspicuous in their absence over the past 30 years of long term global warming.\nThe only forcing that causes warming and also correlates with current temperature rises is atmospheric CO2. It's risen 100 parts per million over the past 120 years - in the past, that kind of change has taken 5,000 to 20,000 years. As CO2 rose over the 20th century, the only mystery has been why global temperatures actually cooled from 1950 to 1980. I even read one study in 1980 where the researcher posed the question \"why aren't we seeing any global warming with all this CO2 in the air?\"\nThe answer is now apparent with recent studies in aerosol levels and global dimming. Atmospheric aerosols caused a global dimming (eg - less radiation reaching the earth) from 1950 to 1985. In the mid-80's, the trend reversed and radiation levels at the Earth's surface began to brighten. From 1950 to the mid-80's, the cooling effect from aerosols was masking the warming effect from CO2. When aerosol cooling ended, the current global warming trend began.\nIntermediate rebuttal written by John Cook\nUpdate August 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 4 August 2015 by MichaelK. View Archives"
  },
  {
   "title": "It's albedo",
   "paragraph": "The albedo effect and global warming\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe long term trend from albedo is of cooling. Recent satellite measurements of albedo show little to no trend.\nClimate Myth...\nIt's albedo\n\"Earth’s Albedo has risen in the past few years, and by doing reconstructions of the past albedo, it appears that there was a significant reduction in Earth’s albedo leading up to a lull in 1997. The most interesting thing here is that the albedo forcings, in watts/sq meter seem to be fairly large. Larger than that of all manmade greenhouse gases combined.\" (Anthony Watts)\nThe Unsettled Science of Albedo\n“Clouds are very pesky for climate scientists…”\nKaren M. Shell, Associate Professor, College of Earth, Ocean, and Atmospheric Sciences, Oregon State University, writing about cloud feedback for RealClimate\nAlbedo is a measure of the reflectivity of a surface. The albedo effect when applied to the Earth is a measure of how much of the Sun's energy is reflected back into space. Overall, the Earth's albedo has a cooling effect. (The term ‘albedo’ is derived from the Latin for ‘whiteness’).\nThe basic principle is analogous to strategies employed by people who live in hot places. Building are finished with white exteriors to keep them cool, because white surfaces reflect the sun’s energy. Black surfaces reflect much less. People wear light colours in summer rather than dark ones for the same reason.\nThe Earth’s surface is a vast patchwork of colours, ranging from the dazzling white of ice and snow, to the dark surfaces of oceans and forests. Each surface has a specific effect on the Earth’s temperature. Snow and ice reflect a lot of the sun’s energy back into space. The darker oceans absorb energy, which warms the water. Oceans help keep the Earth warm because they absorb a lot of heat (approximately 90%). This warming increases water vapour, which acts as a greenhouse gas and helps to keep temperatures within ranges humans have largely taken for granted for millennia.\nA Cloudy Outlook\nIt isn’t just the Earth’s surface that has a reflective quality. Clouds also reflect sunlight, contributing to the cooling effect of albedo. They also contribute to warming at the same time, because they consist of condensed water vapour, which retains heat.\nAnd if clouds complicate matters, so too do the seasons. Every year, albedo peaks twice. The first peak occurs when the Antarctic sea-ice is at its winter maximum. The second peak, which is larger, occurs when there is snow cover over much of the Northern Hemisphere.\nAlbedo also changes due to human interaction. Forests have lower albedo than topsoil; deforestation increases albedo. Burning wood and fossil fuels adds black carbon to the atmosphere. Some black carbon settles on the surface of the ice, which reduces albedo.\nAlbedo and Global Warming\nThe most significant projected impact on albedo is through future global warming. With the exception of Antarctic sea-ice, recently increasing by 1% a year, nearly all the ice on the planet is melting. As the white surfaces decrease in area, less energy is reflected into space, and the Earth will warm up even more.\nThe loss of Arctic ice is of particular concern. The ice is disappearing quite fast; not only is albedo decreasing, but the loss triggers a positive feedback. By exposing the ocean surface to sunlight, the water warms up. This melts the ice from underneath, while man-made CO2 in the atmosphere warms the surface. Humidity also increases; water vapour is a powerful greenhouse gas. More ice therefore melts, which exposes more water, which melts more ice from underneath…\nThis loop fuels itself, the effect getting more and more pronounced. This is a good example of a positive feedback. Increased water vapour also has another effect, which is to increase the amount of cloud. As mentioned already, clouds can increase albedo (a negative feedback), but also warming (a positive feedback).\nMeasuring Albedo\nThe albedo of a surface is measured on a scale from 0 to 1, where 0 is a idealised black surface with no reflection, and 1 represents a white surface that has perfect reflection.\nTaking measurements of something with so many variables and influences is clearly going to be a challenge. Satellite data is constrained by the orbit of the satellite. Clouds can be hard to distinguish from white surfaces.\nIndirect measurement may also be problematic. The Earthshine project investigated a phenomenon where light reflected by Earth illuminates the dark side of the moon. By measuring the brightness, the amount of albedo - reflectivity - could be estimated.\nThe project reported a counter-intuitive finding. The Earth’s albedo was rising, even as the planet was warming. This seems contradictory, as Anthony Watts was quick to note when he voiced his sceptical argument in 2007. If higher albedo was having a cooling effect, how could global warming be taking place?\nTricky Business\nScience constantly seeks to improve itself. The first Earthshine paper (Palle 2004) claimed to have discovered a very significant cooling effect through a big increase in global albedo.\nThe results were problematic. They flatly contradicted the NASA CERES satellite observations, and the discrepancy became the subject of investigation. In 2004, a new telescope was installed at the Big Bear observatory, where the project was located. It became evident that the original analysis was in inaccurate. Once corrected, the Earthshine project and the satellite measurements were more consistent.\nFigure 1: Earth albedo anomalies as measured by earthshine. In black are the albedo anomalies published in 2004 (Palle 2004). In blue are the updated albedo anomalies after improved data analysis, which also include more years of data (Palle 2008).\nOver a five-year period, scientists found that albedo did increase slightly. Since 2003 the CERES satellite records shows a very slight reduction.\nFigure 2: CERES Terra SW TOA flux and MODIS cloud fraction for 30\u0003S–30\u0003N between March 2000 and February 2010 (Loeb et.al. 2012 - PDF)\nGlobal versus Local\nThere are contradictory assessments of current trends in global albedo, possibly because the changes and effects are small. Research is being conducted into the role of clouds, both as forcings and feedbacks, and the role of albedo in cloud formation.\nRecent research indicates that global albedo is fairly constant, and having no material effect on global temperatures. Local effects may be more pronounced. Loss of albedo in the Arctic could heat the water sufficiently to release methane stored in ice crystals called clathrates. (Methane is a greenhouse gas far more potent than CO2).\nLoss of albedo in the Arctic will accelerate warming across adjacent permafrost, releasing methane. Melting permafrost may reduce its albedo, another positive feedback that will accelerate warming. Ocean warming from reduced Arctic albedo will also accelerate melting at the edges of the Greenland ice cap, speeding up sea level rise.\nConclusions\nAlbedo is a subject needing a lot more research. It’s an important feature of our climate, and a complex one. It is not yet possible to make definitive statements about what the future may hold. In fact, it is a good example of the ‘unsettled’ nature of climate change science.\nWe know the planet is warming, and that human agency is causing it. What we cannot say yet is how climate change is affecting albedo, how it might be affected in the future, and what contribution to climate change - positive or negative - it may make.\nBasic rebuttal written by GPWayne\nLast updated on 23 October 2016 by gpwayne. View Archives"
  },
  {
   "title": "It's CFCs",
   "paragraph": "CFCs contribute only a fraction of global warming\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nCFCs contribute to global warming at a small level.\nClimate Myth...\nIt's CFCs\n\"The observed data show that CFCs conspiring with cosmic rays most likely caused both the Antarctic ozone hole and global warming... The total amount of CFCs, ozone-depleting molecules that are well-known greenhouse gases, has decreased around 2000. Correspondingly, the global surface temperature has also dropped.\" (Qing-Bin Lu)\nA paper published in an obscure physics journal by the University of Waterloo's Qing-Bin Lu (2013) has drawn quite a bit of media attention for blaming global warming not on carbon dioxide, but rather on chlorofluorocarbons (CFCs, which are also greenhouse gases). However, there are numerous fundamental flaws in the paper, which is based almost entirely on correlation (not causation) and curve fitting exercises.\nLu's hypothesis was disproven very simply by Nuccitelli et al. (2014). Lu argues that the radiative forcing (global energy imbalance) from CFCs matches global surface temperatures better than that from CO2 over the past decade. This is because as a result of the Montreal Protocol, CFC emissions (and emissions of hydrofluorocarbons, which replaced CFCs) have been flat over the past decade, and global surface air temperatures have also been essentially flat during that short timeframe, while CO2 emissions have continued to rise.\nHowever, a global energy imbalance doesn't just impact surface temperatures. In fact, only about 2% of global warming is used in heating the atmosphere, while about 90% heats the oceans. Over the past decade, ocean and overall global heating have continued to rise rapidly, accumulating the equivalent of about 4 Hiroshima atomic bomb detonations per second (Figure 1).\nFigure 1: Land, atmosphere, and ice heating (red), 0-700 meter OHC increase (light blue), 700-2,000 meter OHC increase (dark blue). From Nuccitelli et al. (2012).\nSo while CFCs might match surface temperature changes better than CO2 emissions over the past decade, CO2 emissions better match the relevant metric – overall global heat accumulation. Since a global energy imbalance influences global heat content and not just surface temperatures, this by itself is sufficient to falsify Lu's hypothesis (though the paper contains several other fundamental problems – see the Advanced level rebuttal for details).\nLast updated on 14 April 2014 by dana1981. View Archives"
  },
  {
   "title": "It's cooling",
   "paragraph": "Global cooling - Is global warming still happening?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nAll the indicators show that global warming is still happening.\nClimate Myth...\nIt's cooling\n\"In fact global warming has stopped and a cooling is beginning. No climate model has predicted a cooling of the Earth – quite the contrary. And this means that the projections of future climate are unreliable.\" (source: Henrik Svensmark)\nWhen looking for evidence of global warming, there are many different indicators that we should look for. Whilst it's natural to start with air temperatures, a more thorough examination should be as inclusive as possible; snow cover, ice melt, air temperatures over land and sea, even the sea temperatures themselves. The key indicators of global warming shown below are all moving in the direction expected of a warming globe.\nIndicators of a warming world based on surface, satellite, and ocean temperature measurements, satellite measurements of energy imbalance (the difference between incoming and outgoing energy at the top of the atmosphere), and of receding glaciers, sea ice, and ice sheets, rising sea level, and shifting seasons.\nThe question of global warming stopping is often raised in the light of a recent weather event - a big snowfall or drought breaking rain. Global warming is entirely compatible with these events; after all they are just weather. For climate change, it is the long term trends that are important; measured over decades or more, and those long term trends show that the globe is still, unfortunately, warming.\nBasic rebuttal written by LarryM\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 July 2015 by skeptickev. View Archives"
  },
  {
   "title": "It's cosmic rays",
   "paragraph": "What's the link between cosmic rays and climate change?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nCosmic ray counts have increased over the past 50 years, so if they do influence global temperatures, they are having a cooling effect.\nClimate Myth...\nIt's cosmic rays\n\"When the Sun is active, its magnetic field is better at shielding us against the cosmic rays coming from outer space, before they reach our planet. By regulating the Earth’s cloud cover, the Sun can turn the temperature up and down. ... As the Sun’s magnetism doubled in strength during the 20th century, this natural mechanism may be responsible for a large part of global warming seen then.\" (Henrik Svensmark)\nThe galactic cosmic ray (GCR) warming hypothesis is based on the premise that GCRs can \"seed\" clouds, and clouds reflect sunilight. So if there are fewer GCRs reaching Earth (because a strong solar magnetic field is deflecting them away), the hypothesis says there will be fewer clouds, more sunlight reaching the Earth's surface, and thus more global warming.\nSo more solar activity means a stronger solar magnetic field, which means fewer GCRs reaching Earth, which hypothetically means fewer clouds and more warming.\nThe body of scientific research has determined that GCRs are actually not very effective at seeding clouds. However, the hypothesis is also disproven just by examining the data. Over the past five decades, the number of GCRs reaching Earth has increased, and in recent years reached record high numbers. This means that if the GCR-warming hypothesis is correct, this increase in GCRs should actually be causing global cooling over the past five decades, and particularly cold temperatures in recent years.\nOn the contrary, while GCRs are up, global temperatures are also way up, and temperatures in recent years reached record highs.\nAnnual average GCR counts per minute (blue - note that numbers decrease going up the left vertical axis, because lower GCRs should mean higher temperatures) from the Neutron Monitor Database vs. annual average global surface temperature (red, right vertical axis) from NOAA NCDC, both with second order polynomial fits.\nLast updated on 4 October 2015 by dana1981. View Archives"
  },
  {
   "title": "It's El Niño",
   "paragraph": "Global warming and the El Niño Southern Oscillation\nLink to this page\nWhat the science says...\nThe El Nino Southern Oscillation shows close correlation to global temperatures over the short term. However, it is unable to explain the long term warming trend over the past few decades.\nClimate Myth...\nIt's El Niño\n\"Three Australasian researchers have shown that natural forces are the dominant influence on climate, in a study just published in the highly-regarded Journal of Geophysical Research. According to this study little or none of the late 20th century global warming and cooling can be attributed to human activity. The close relationship between ENSO and global temperature, as described in the paper, leaves little room for any warming driven by human carbon dioxide emissions. The available data indicate that future global temperatures will continue to change primarily in response to ENSO cycling, volcanic activity and solar changes.\" (Climate Depot)\nThe paper claiming a link between global warming and the El Niño Southern Oscillation (ENSO) is Influence of the Southern Oscillation on tropospheric temperature (McLean 2009). What does the paper find? According to one of it's authors, Bob Carter,\n\"The close relationship between ENSO and global temperature, as described in the paper, leaves little room for any warming driven by human carbon dioxide emissions.\"\nIn other words, they claim that any global warming over the past few decades can be explained by El Niño activity.\nHow do they arrive at this conclusion? They begin by comparing satellite measurements of tropospheric temperature to El Niño activity. Figure 1 plots a 12 month running average of Global Tropospheric Temperature Anomaly (GTTA, the light grey line) and the Southern Oscillation Index (SOI, the black line).\nFigure 1: Twelve-month running means of SOI (dark line) and MSU GTTA (light line) for the period 1980 to 2006 with major periods of volcanic activity indicated (McLean 2009).\nThe Southern Oscillation Index shows no long term trend (hence the term Oscillation) while the temperature record shows a long term warming trend. Consequently, they find only a weak correlation between temperature and SOI. Next, they compare derivative values of SOI and GTTA. This is done by subtracting the 12 month running average from the same average 1 year later. They do this to \"remove the noise\" from the data. They fail to mention it also removes any linear trend, which is obvious from just a few steps of basic arithmetic. It is also visually apparent when comparing the SOI derivative to the GTTA derivative in Figure 2:\nFigure 2: Derivatives of SOI (dark line) and MSU GTTA (light line) for the period 1981–2007 after removing periods of volcanic influence (McLean 2009).\nThe linear warming trend has been removed from the temperature record, resulting in a close correlation between the filtered temperature and SOI. The implications from this analysis should be readily apparent. El Niño has a strong short term effect on global temperature but cannot explain the long term trend. In fact, this is a point made repeatedly on this website (eg - here and here).\nThis view is confirmed in other analyses. An examination of the temperature record from 1880 to 2007 finds internal variability such as El Nino has relatively small impact on the long term trend (Hoerling 2008). Instead, they find long term trends in sea surface temperatures are driven predominantly by the planet's energy imbalance.\nThere have been various attempts to filter out the ENSO signal from the temperature record. We've examined one such paper by Fawcett 2007 when addressing the global warming stopped in 1998 argument. Similarly, Thompson 2008 filters out the ENSO signal from the temperature record. What remains is a warming trend with less variability:\nFigure 3: Surface air temperature records with ENSO signal removed. HadCRUT corrections by Thompson 2008, GISTEMP corrections by Real Climate.\nFoster and Rahmstorf (2011) used a multiple linear regression approach to filter out the effects of volcanic and solar activity and ENSO. They found that ENSO, as measured through the the Multivariate ENSO Index (MEI), had a slight cooling effect of about -0.014 to -0.023°C per decade in the surface and lower troposphere temperatures, respectively from 1979 through 2010 (Table 1, Figure 4). This corresponds to 0.045 to 0.074°C cooling from ENSO since 1979, respectively. The results are essentially unchanged when using SOI as opposed to MEI.\nTable 1: Trends in °C/decade of the signal components due to MEI, AOD and TSI in the regression of global temperature, for each of the five temperature records from 1979 to 2010.\nFigure 4: Influence of exogenous factors on global temperature for GISS (blue) and RSS data (red). (a) MEI; (b) AOD; (c) TSI.\nLike Foster and Rahmstorf, Lean and Rind (2008) performed a multiple linear regression on the temperature data, and found that although ENSO is responsible for approximately 12% of the observed global warming from 1955 to 2005, it actually had a small net cooling effect from 1979 to 2005. Overall, from 1889 to 2005, ENSO can only explain approximately 2.3% of the observed global warming.\nUltimately, all the data analysis shouldn't distract us from the physical reality of what is happening to our climate. Over the past 4 decades, oceans all over the globe have been accumulating heat (Levitus 2008; Nuccitelli et al. 2012, Figure 5). The El Niño Southern Oscillation is an internal phenomenon where heat is exchanged between the atmosphere and ocean and cannot explain an overall buildup of global ocean heat. This points to an energy imbalance responsible for the long term trend (Wong 2005).\nFigure 5: Land, atmosphere, and ice heating (red), 0-700 meter OHC increase (light blue), 700-2,000 meter OHC increase (dark blue). From Nuccitelli et al. (2012),\nData analysis, physical observations and basic arithmetic all show ENSO cannot explain the long term warming trend over the past few decades. Hence the irony in Bob Carter's conclusion \"The close relationship between ENSO and global temperature leaves little room for any warming driven by human carbon dioxide emissions\". What his paper actually proves is once you remove any long term warming trend from the temperature record, it leaves little room for any warming.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 10 July 2015 by pattimer. View Archives"
  },
  {
   "title": "It's freaking cold!",
   "paragraph": "Does cold weather disprove global warming?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nA local cold day has nothing to do with the long-term trend of increasing global temperatures.\nClimate Myth...\nIt's freaking cold!\n\"Austria is today seeing its earliest snowfall in history with 30 to 40 centimetres already predicted in the mountains. Such dramatic falls in temperatures provide superficial evidence for those who doubt that the world is threatened by climate change.\" (Mail Online)\nIt's easy to confuse current weather events with long-term climate trends, and hard to understand the difference between weather and climate. It's a bit like being at the beach, trying to figure out if the tide is rising or falling just by watching individual waves roll in and out. The slow change of the tide is masked by the constant churning of the waves.\nIn a similar way, the normal ups and downs of weather make it hard to see slow changes in climate. To find climate trends you need to look at how weather is changing over a longer time span. Looking at high and low temperature data from recent decades shows that new record highs occur nearly twice as often as new record lows.\nNew records for cold weather will continue to be set, but global warming's gradual influence will make them increasingly rare.\nBasic rebuttal written by Jim Meador\nUpdate July 2015:\nHere are related lecture-videos from Denial101x - Making Sense of Climate Science Denial\nLast updated on 9 July 2015 by pattimer. View Archives"
  },
  {
   "title": "It's global brightening",
   "paragraph": "Could global brightening be causing global warming?\nLink to this page\nWhat the science says...\nGlobal brightening is caused by changes in cloud cover, reflective aerosols and absorbing aerosols. While changes in cloud cover & aerosols lead to more sunlight hitting the surface, this can be compensated by the cooling effect on the atmosphere due to fewer clouds trapping less warmth and fewer absorbing aerosols absorbing less sunlight.\nClimate Myth...\nIt's global brightening\n'...satellites confirmed measurements from ground stations show a considerable, and naturally-occurring, global brightening from 1983-2001 (Pinker et al., 2005). Elementary radiative-transfer calculations demonstrate that a natural surface global brightening amounting to ~1.9 Wm–2 over the 18-year period of study would be expected – using the IPCC’s own methodology – to have caused a transient warming of 1 K (1.8 F°). To put this naturally-occurring global brightening into perspective, the IPCC’s estimated total of all the anthropogenic influences on climate combined in the 256 years 1750-2005 is only 1.6 Wm–2.' (Christopher Monckton)\nOne skeptic argument, employed by Christopher Monckton in his testimony to US Congress, is that global brightening is the cause of global warming. From 1983 to 2001, the amount of sunlight hitting the Earth's surface has increased by 1.9 W/m2. Monckton compares this to the radiative forcing from manmade influence since pre-industrial times, estimated at 1.6 W/m2 (IPCC AR4). Monckton argues that these numbers prove global brightening is responsible for recent global warming. But is this the full picture?\nMonckton's numbers come from Do Satellites Detect Trends in Surface Solar Radiation? (Pinker et al 2005). This study analyses satellite measurements of solar radiation, upward radiation from the Earth and cloud cover fraction to model the amount of sunlight reaching the Earth's surface. They calculate an overall increase in surface solar radiation of 0.16 W/m2 per year. Once the satellite data is corrected to remove an orbital decay bias (ERBE 2005), Monckton calculates a net increase in surface radiative flux of 1.9 W/m2.\nFigure 1: Changes in solar radiation at the Earth's surface from 1983 to 2001. Solid line is linear fit, dotted line is quadratic fit. The linear slope (solid line) is positive at 0.16 W/m2 per year (Pinker et al 2005).\nIs it valid to compare changes in surface solar radiation to radiative forcing? A good person to answer this is Rachel Pinker herself who in responding to Monckton's argument, said the following:\n'The CO2 \"radiative forcing\" value that Mr. Christopher Monckton is quoting refers to the impact on the Earth’s Radiative balance as described above. The numbers that we quote in our paper represent the change in surface SW due to changes in the atmosphere (clouds, water vapor, aerosols). These two numbers cannot be compared at their face value.'\nWhy can't you compare the two numbers? Radiative forcing refers to a disturbance in the planet's energy balance. Forcings change the balance between incoming sunlight and outgoing radiation at the top of the atmosphere, causing the planet to lose or gain energy. Global temperatures will only respond to surface brightening if the total amount of solar energy absorbed by our climate system changes. To determine this, we need to understand what's causing global brightening.\nThere are three major contributors: a reduction in cloud cover, a reduction in scattering aerosols such as sulfates and a reduction in absorbing aerosols like soot (Wild 2009). Scattering aerosols reflect incoming sunlight, preventing it from reaching the Earth's surface. As the amount of sulfate pollution in the atmosphere lessens, more sunlight reaches the surface. If this was the sole cause of global brightening, then the increase in surface solar radiation would equal the extra energy absorbed by our climate (eg - the radiative forcing).\nHowever, changes in cloud cover and absorbing aerosols also contribute to global brightening. As well as reflect sunlight, clouds trap infrared radiation coming up from the surface. So while less clouds allow more sunlight to reach the surface which has a warming effect, they also let more infrared radiation escape to space which has a cooling effect.\nSimilarly, a decline in absorbing aerosols like soot means more sunlight reaches the Earth which has a warming effect. But they also absorb sunlight which warms the atmosphere so a decline in absorbing aerosols also has a cooling effect. Absorbing aerosols like black carbon have shown a large decreasing trend since the 1980s (Wild 2009).\nTo focus solely on the amount of sunlight hitting the Earth doesn't give you the full picture of global brightening. As absorbing aerosols and clouds are contributing factors, the change in surface solar radiation is expected to be much more than the net radiative forcing from global brightening. To gain a fuller understanding of climate, we need to consider all the various forcings together rather than take one small piece in isolation. These include the direct effect from reflective aerosols, the indirect effect of aerosols on cloud cover and the effect of absorbing aerosols (black carbon) to name just a few.\nFigure 2: Separate global climate forcings relative to their 1880 values (GISS).\nFigure 2 demonstrates that CO2 is not the only driver of climate. Nevertheless, it's clear that man-made greenhouse gases (of which CO2 is the greatest contributor) is currently the most dominant forcing and increasing faster than any other forcing.\nLast updated on 29 October 2016 by dana1981. View Archives"
  },
  {
   "title": "It's internal variability",
   "paragraph": "Internal variability swamped by human-caused global warming\nLink to this page\nWhat the science says...\nInternal variability can only account for ~0.3°C change in average global surface air temperature at most over periods of several decades, and scientific studies have consistently shown that it cannot account for more than a small fraction of the global warming over the past century.\nClimate Myth...\nIt's internal variability\nWhen you look at the possibility of natural unforced variability, you see that can cause excursions that we've seen recently (Dr. John Christy)\nA favorite argument among climate scientist \"skeptics\" like Christy, Spencer, and Lindzen is that \"internal variability\" can account for much or all of the global warming we've observed over the past century. As we will see here, natural variability cannot account for the large and rapid warming we've observed over the past century, and particularly the past 40 years.\nSwanson and Tsonis\nOne of the most widely-circulated papers on the impact of natural variability on global temperatures is Swanson et al. (2009) which John has previously discussed.\nAlthough Swanson 2009 was widely discussed throughout the blogosphere and mainstream media, the widespread beliefs that the study attributed global warming to natural variability and/or predicted global cooling were based on misunderstandings of the paper, as Dr. Swanson noted:\n\"What do our results have to do with Global Warming, i.e., the century-scale response to greenhouse gas emissions? VERY LITTLE, contrary to claims that others have made on our behalf. Nature (with hopefully some constructive input from humans) will decide the global warming question based upon climate sensitivity, net radiative forcing, and oceanic storage of heat, not on the type of multi-decadal time scale variability we are discussing here. However, this apparent impulsive behavior explicitly highlights the fact that humanity is poking a complex, nonlinear system with GHG forcing – and that there are no guarantees to how the climate may respond.\"\nIn their paper, Swanson et al. use climate models to hash out the role internal variability has played in average global temperature changes over the past century (Figure 1).\nFigure 1: Estimation of the observed signature of internal variability in the observed 20th century global mean temperature in climate model simulations\nAs you can see, over periods of a few decades, modeled internal variability does not cause surface temperatures to change by more than 0.3°C, and over longer periods, such as the entire 20th Century, its transient warming and cooling influences tend to average out, and internal variability does not cause long-term temperature trends.\nAdditional Studies\nA number of other scientific studies have also examined the impact of internal variability on global temperatures, and arrived at a very similar conclusion to Swanson et al. For example, Huber and Knutti (2011)take an approach which utilizes the principle of conservation of energy for the global energy budget, concluding (emphasis added):\n\"Our results show that it is extremely likely that at least 74% (+/- 12%, 1 sigma) of the observed warming since 1950 was caused by radiative forcings, and less than 26% (+/- 12%) by unforced internal variability.\"\nWhile Huber and Knutti find that internal variability could account for as much as ~0.15°C warming since 1950, it could also account for ~0.15°C cooling, or anything in between. What the authors have concluded is that natural variability can very likely account for no more than 26% of the warming since 1950, and no more than 18% since 1850 (and in both cases, the most likely value is close to zero).\nDelSole et al. (2011) similarly conclude (emphasis added):\n\"The amplitude and time scale of the IMP [internal multidecadal pattern] are such that its contribution to the trend dominates that of the forced component on time scales shorter than 16 yr, implying that the lack of warming trend during the past 10 yr is not statistically significant...While the IMP can contribute significantly to trends for periods of 30 yr or shorter, it cannot account for the 0.8°C warming that has been observed in the twentieth-century spatially averaged SST.\"\nThis conclusion directly contradicts the statement that natural variability can account for all of the recent warming. This is not a new finding, as it is consistent for example with Stouffer et al. (1994):\n\"throughout the simulated time series no temperature change as large as 0.5°C per century is sustained for more than a few decades. Assuming that the model is realistic, these results suggest that the observed trend is not a natural feature of the interaction between the atmosphere and oceans.\"\nand with Wigley and Raper (1990):\n\"Simulations with a simple climate model are used to determine the main controls on internally generated low-frequency variability, and show that natural trends of up to 0.3°C may occur over intervals of up to 100 years. Although the magnitude of such trends is unexpectedly large, it is insufficient to explain the observed global warming during the twentieth century.\"\nThese studies are also consistent with Bertrand and van Ypersele (2002), Rybski et al. (2006), and Zorita et al. (2008), among others. There is a strong consensus that natural variability cannot account for the observed global warming trend.\nSpencer's Hypothesis\nDr. Roy Spencer has proposed a hypothesis whereby some unknown internal mechanism causes cloud cover to change, which in turn changes the reflectivity (albedo) of the planet, thus causing warming or cooling. Spencer also attributes most of the global warming over the past century to this \"internal radiative forcing.\" There are some significant flaws in this hypothesis. For one thing, it fails to explain many of the observed \"fingerprints\" of human-caused global warming, such as the cooling upper atmosphere (stratosphere and above) and the higher rate of warming at night than during the day.\nIn order for internal variability to account for the global warming over the past century (especially over the past 40 years), it requires that the large greenhouse gas radiative forcing can't have much effect on global temperatures. For this to be true, climate sensitivity must be low. But as discussed in Swanson et al. (2009), if climate is more sensitive to internal variability than currently thought, this would also mean climate is more sensitive to external forcings, including CO2. This is a Catch-22 for Spencer's hypothesis; it effectively requires that climate sensitivity is simultaneously both low and high.\nDebunked by Dessler\nDr. Andrew Dessler published a study (Dessler 2010) which casts further doubt on Spencer's hypothesis, as detailed in an email exchange between the two scientists. In short, Dessler argues that cloud cover change is a feedback to a radiative forcing, for example increasing greenhouse gases, while Spencer argues that clouds are changing due to some other, unknown cause, and acting as a forcing themselves. Unlike Spencer, Dessler explains the mechanism and supporting evidence behind his cloud feedback research:\n\"My cloud feedback calculation is supported by a firm causal link: ENSO causes surface temperature variations which causes cloud changes. This is supported by the iron triangle of observations, theory, and climate models.\"\nDessler published a second study Dessler (2011), examining whether observational data behaved as expected by Spencer's internal variability hypothesis. Spencer & Braswell (2011) assumed that the change in top of the atmosphere (TOA) energy flux due to cloud cover changes from 2000 to 2010 was twice as large as the heating of the climate system through ocean circulation. Dessler (2011) used observational data (such as surface temperature measurements and ARGO ocean temperature) to estimate and corroborate these values, and found that the heating of the climate system through ocean heat transport was 20 times larger than TOA energy flux changes due to cloud cover over the period in question.\nThis empirical finding contradicts Spencer's hypothesis that cloud cover changes are driving global warming, but is consistent with our current understanding of the climate: ocean heat is exchanged with the atmosphere, which causes surface warming, which alters atmospheric circulation, which alters cloud cover, which impacts surface temperature. However, while Spencer hypothesizes that the changes in cloud cover are the main driver behind global warming, Dessler concludes that they're only responsible for a small percentage of the changes in surface temperature from 2000 to 2010.\nEl Niño Southern Oscillation (ENSO)\nAlthough he is very coy about the physical mechanisms behind his hypothesis, Spencer does seem to believe that his hypothesized internal radiative forcing will cause \"ENSO-type behavior,\" such as warming surface air temperatures. However, Trenberth et al. (2002) examined the role ENSO has played in the global warming over the past half-century, and their conclusions do not bode well for Spencer's hypothesis:\n\"For 1950–1998, ENSO linearly accounts for 0.06°C of global surface temperature increase.\"\nThis 0.06°C accounts for approximately 12% of the warming trend over the timeframe in question. Foster et al. (2010) also examined the effects of ENSO on global temperature and arrived at the same conclusion.\n\"It has been well known for many years that ENSO is associated with significant variability in global mean temperatures on interannual timescales. However, this relationship (which, contrary to the claim of MFC09, is simulated by global climate models, e.g. Santer et al. [2001]) cannot explain temperature trends on decadal and longer time scales.\"\nFoster et al. examine a number of previous studies which assessed and removed the effects of ENSO on the global surface temperature (emphasis added):\n\"In all of these previous analyses, ENSO has been found to describe between 15 and 30% of the interseasonal and longer-term variability in surface and/or lower tropospheric temperature, but little of the global mean warming trend of the past half century.\"\nPacific Decadal Oscillation (PDO)\nENSO is part of the PDO, which Spencer has also tried to blame for the current global warming. In a post on his blog following up on Spencer and Braswell (2008), Spencer claims:\n\"The evidence presented here suggests that most of that warming might well have been caused by cloud changes associated with a natural mode of climate variability: the Pacific Decadal Oscillation.\"\nHowever, as detailed here by Dr. Barry Bickmore in a three part series, and by Dr. Ray Pierrehumbert at RealClimate, Spencer's attribution of the recent global warming to PDO is no more than an example of how to cook a graph. As Dr. Bickmore put it,\n\"Spencer's curve-fitting enterprise could (and did!) give him essentially any answer he wanted, as long as he didn't mind using parameters that don't make any physical sense.\"\nFurther, as we have previously discussed, like ENSO, PDO physically cannot cause a long-term global warming trend. It is an oscillation which simply moves heat from oceans to air and vice-versa, so even if there were a period of predominantly positive PDO over the long-term, the oceans would cool as a consequence of the transfer of heat to the overlying air. That is not the case: the oceans are warming as well.\nIt's not Internal Variability\nIn conclusion, there is simply no supporting evidence or physics behind the claim that the global warming over the past century could simply be attributed to internal variability. Studies on the subject consistently show that internal variability does not account for more than ~0.3°C warming of global surface air temperatures over periods of several decades. Internal variability also tends to average out over longer periods of time, as has been the case over the past century, and cannot account for more than a small fraction of the observed warming over that period. Spencer's hypothesis cannot account for numerous observed changes in the global climate (which are consistent with an increased greenhouse effect), does not have a known physical mechanism, and there are simply better explanations for interactions between global temperature and cloud cover.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 19 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "It's land use",
   "paragraph": "The minor role of land use in global warming\nLink to this page\nWhat the science says...\nCorrelations between warming and economic activity are 0ften unreliable. They don't take into account local forcing agents such as tropospheric ozone or black carbon. Correlations are likely over-estimated since grid boxes in both economic and climate data are not independent. Lastly, there is significant independent evidence for warming in the oceans, snow cover and sea ice extent changes.\nClimate Myth...\nIt's land use\n\"Land-use modifications for urbanization and agriculture have affected climate change data more than previously thought, according to new research by a University of Guelph professor. In a paper published online this week in the Journal of Geophysical Research-Atmosphere, economics professor Ross McKitrick says the resulting discrepancies may be leading to an overstatement of the role of greenhouse gases in the atmosphere. In fact, the study concludes that skewed data could account for as much as half the post-1980 warming trend over land.\" (University of Guelph)\nSee Spurious correlations between recent warming and indices of local economic activity (Schmidt 2009) for more info.\nLast updated on 5 December 2013 by nancyk. View Archives"
  },
  {
   "title": "It's methane",
   "paragraph": "What is methane's contribution to global warming?\nLink to this page\nWhat the science says...\nWhile methane is a more potent greenhouse gas than CO2, there is over 200 times more CO2 in the atmosphere. Hence the amount of warming methane contributes is 28% of the warming CO2 contributes.\nClimate Myth...\nIt's methane\n\"A United Nations report has identified the world's rapidly growing herds of cattle as the greatest threat to the climate, forests and wildlife. ...\n...Livestock are responsible for 18 per cent of the greenhouse gases that cause global warming, more than cars, planes and all other forms of transport put together.\nBurning fuel to produce fertiliser to grow feed, to produce meat and to transport it - and clearing vegetation for grazing - produces 9 per cent of all emissions of carbon dioxide, the most common greenhouse gas. And their wind and manure emit more than one third of emissions of another, methane, which warms the world 20 times faster than carbon dioxide.\" (Geoffrey Lean)\nWhile methane is a more potent greenhouse gas than CO2, there is over 200 times more CO2 in the atmosphere. Eg - CO2 levels are 380 ppm (parts per million) while methane levels are 1.75ppm. Hence the amount of warming methane contributes is calculated at 28% of the warming CO2 contributes. Here is a graph of the various forcings that influence climate (methane is CH4, right above CO2).\nThis is not to say methane can be ignored - reducing methane levels is definitely a goal to pursue. The good news is since the early 1990's, the trend in increasing methane has slowed down and even leveled off in the last few years (Dlugokencky 2003).\nLast updated on 26 October 2016 by John Cook. View Archives"
  },
  {
   "title": "It's microsite influences",
   "paragraph": "Microsite influences on the temperature record are minimal\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nMicrosite influences on temperature changes are minimal; good and bad sites show the same trend.\nClimate Myth...\nIt's microsite influences\nU.S. weather stations have been located next to exhaust fans of air conditioning units, surrounded by asphalt parking lots, on blistering-hot rooftops, and near sidewalks and buildings that absorb and radiate heat. 89 percent of the stations fail to meet the National Weather Service’s own siting requirements that stations must be 30 metres away from an artificial heating or radiating/reflecting heat source. (Watts 2009)\nThe website surfacestations.org enlisted an army of volunteers to photograph US surface temperature measurement stations and document stations located near parking lots, air conditioners, or anything else that might impose a warming bias. They found that 89% of the stations did not meet the US weather service siting criteria in one way or another. That is not good. Does this prove that a US warming trend is just the artificial influence of parking lots and air conditioners on the temperature record coming from \"bad\" stations?\nNo. Actually, an analysis shows that \"good\" and \"bad\" stations show very similar trends for temperature over time. The chart below compares data from stations that surfacestations.org identified as \"good\", as well as \"bad\" stations. Notice that \"good\" stations track very closely to \"bad\" stations, and actually the \"good\" stations show more of a warming trend!\nFigure 1. Annual average maximum and minimum unadjusted temperature change calculated using (c) maximum and (d) minimum temperatures from good and poor exposure sites (Menne 2010).\nThe volunteers from surfacestations.org deserve credit for pointing out siting problems of the US Weather Service temperature measurement stations. Unfortunately the fact that \"good\" and \"bad\" stations show the same upward trend proves that warming in the US is not just a measurement problem. Temperatures are trending upward around the globe, not just in the US. Microsite influences on temperature measurements in the US can't explain the US temperature rise, much less the global rise.\nBasic rebuttal written by Jim Meador\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 15 July 2015 by pattimer. View Archives"
  },
  {
   "title": "It's not bad",
   "paragraph": "Positives and negatives of global warming\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nNegative impacts of global warming on agriculture, health & environment far outweigh any positives.\nClimate Myth...\nIt's not bad\n\"Two thousand years of published human histories say that warm periods were good for people. It was the harsh, unstable Dark Ages and Little Ice Age that brought bigger storms, untimely frost, widespread famine and plagues of disease.\" (Dennis Avery)\nHere’s a list of cause and effect relationships, showing that most climate change impacts will confer few or no benefits, but may do great harm at considerable cost.\nAgriculture\nWhile CO2 is essential for plant growth, all agriculture depends also on steady water supplies, and climate change is likely to disrupt those supplies through floods and droughts. It has been suggested that higher latitudes – Siberia, for example – may become productive due to global warming, but the soil in Arctic and bordering territories is very poor, and the amount of sunlight reaching the ground in summer will not change because it is governed by the tilt of the earth. Agriculture can also be disrupted by wildfires and changes in seasonal periodicity, which is already taking place, and changes to grasslands and water supplies could impact grazing and welfare of domestic livestock. Increased warming may also have a greater effect on countries whose climate is already near or at a temperature limit over which yields reduce or crops fail – in the tropics or sub-Sahara, for example.\nHealth\nWarmer winters would mean fewer deaths, particularly among vulnerable groups like the aged. However, the same groups are also vulnerable to additional heat, and deaths attributable to heatwaves are expected to be approximately five times as great as winter deaths prevented. It is widely believed that warmer climes will encourage migration of disease-bearing insects like mosquitoes and malaria is already appearing in places it hasn’t been seen before.\nPolar Melting\nWhile the opening of a year-round ice free Arctic passage between the Atlantic and Pacific oceans would confer some commercial benefits, these are considerably outweighed by the negatives. Detrimental effects include loss of polar bear habitat and increased mobile ice hazards to shipping. The loss of ice albedo (the reflection of heat), causing the ocean to absorb more heat, is also a positive feedback; the warming waters increase glacier and Greenland ice cap melt, as well as raising the temperature of Arctic tundra, which then releases methane, a very potent greenhouse gas (methane is also released from the sea-bed, where it is trapped in ice-crystals called clathrates). Melting of the Antarctic ice shelves is predicted to add further to sea-level rise with no benefits accruing.\nOcean Acidification\nA cause for considerable concern, there appear to be no benefits to the change in pH of the oceans. This process is caused by additional CO2 being absorbed in the water, and may have severe destabilising effects on the entire oceanic food-chain.\nMelting Glaciers\nThe effects of glaciers melting are largely detrimental, the principle impact being that many millions of people (one-sixth of the world’s population) depend on fresh water supplied each year by natural spring melt and regrowth cycles and those water supplies – drinking water, agriculture – may fail.\nSea Level Rise\nMany parts of the world are low-lying and will be severely affected by modest sea rises. Rice paddies are being inundated with salt water, which destroys the crops. Seawater is contaminating rivers as it mixes with fresh water further upstream, and aquifers are becoming polluted. Given that the IPCC did not include melt-water from the Greenland and Antarctic ice-caps due to uncertainties at that time, estimates of sea-level rise are feared to considerably underestimate the scale of the problem. There are no proposed benefits to sea-level rise.\nEnvironmental\nPositive effects of climate change may include greener rainforests and enhanced plant growth in the Amazon, increased vegitation in northern latitudes and possible increases in plankton biomass in some parts of the ocean. Negative responses may include further growth of oxygen poor ocean zones, contamination or exhaustion of fresh water, increased incidence of natural fires, extensive vegetation die-off due to droughts, increased risk of coral extinction, decline in global photoplankton, changes in migration patterns of birds and animals, changes in seasonal periodicity, disruption to food chains and species loss.\nEconomic\nThe economic impacts of climate change may be catastrophic, while there have been very few benefits projected at all. The Stern report made clear the overall pattern of economic distress, and while the specific numbers may be contested, the costs of climate change were far in excess of the costs of preventing it. Certain scenarios projected in the IPCC AR4 report would witness massive migration as low-lying countries were flooded. Disruptions to global trade, transport, energy supplies and labour markets, banking and finance, investment and insurance, would all wreak havoc on the stability of both developed and developing nations. Markets would endure increased volatility and institutional investors such as pension funds and insurance companies would experience considerable difficulty.\nDeveloping countries, some of which are already embroiled in military conflict, may be drawn into larger and more protracted disputes over water, energy supplies or food, all of which may disrupt economic growth at a time when developing countries are beset by more egregious manifestations of climate change. It is widely accepted that the detrimental effects of climate change will be visited largely on the countries least equipped to adapt, socially or economically.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 July 2015 by pattimer. View Archives"
  },
  {
   "title": "It's not happening",
   "paragraph": "Evidence for global warming\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThere are many lines of evidence indicating global warming is unequivocal.\nClimate Myth...\nIt's not happening\n\"...these global warming studies that now we're seeing (are) a bunch of snake oil science.\" (Sarah Palin)\nThe 2009 State of the Climate report of the US National Oceanic and Atmospheric Administration (NOAA), released in mid-2010, brings together many different series of data “from the top of the atmosphere to the depths of the ocean”. The conclusion? All of these independent lines of evidence tell us unequivocally that the Earth is warming.\nThe very accessible 10-page summary examines the trends for 10 key climate indicators using a total of 47 different sets of data. All of the indicators expected to increase in a warming world, are in fact increasing, and all that are expected to decrease, are decreasing:\nThe 10 indicators are:\nLand surface air temperature as measured by weather stations. You know all those skeptic arguments about how the temperature record is biased by the urban heat island effect, badly-sited weather stations, dropped stations, and so on? This is the only indicator which suffers from all those problems. So if you’re arguing with somebody who tries to frame the discussion as being about land surface air temperature, just remind them about the other nine indicators.\nSea surface temperature. As with land temperatures, the longest record goes back to 1850 and the last decade is warmest.\nAir temperature over the oceans.\nLower troposphere temperature as measured by satellites for around 50 years. By any of these measures, the 2000s was the warmest decade and each of the last three decades has been much warmer than the previous one.\nOcean heat content, for which records go back over half a century. More than 90% of the extra heat from global warming is going into the oceans – contributing to a rise in…\nSea level. Tide gauge records go back to 1870, and sea level has risen at an accelerating rate.\nSpecific humidity, which has risen in tandem with temperatures.\nGlaciers. 2009 was the 19th consecutive year in which there was a net loss of ice from glaciers worldwide.\nNorthern Hemisphere snow cover, which has also decreased in recent decades.\nPerhaps the most dramatic change of all has been in Arctic sea ice. Satellite measurements are available back to 1979 and reliable shipping records back to 1953. September sea ice extent has shrunk by 35% since 1979.\nScience isn’t like a house of cards, in that removing one line of evidence (eg. land surface air temperature) wouldn’t cause the whole edifice of anthropogenic global warming to collapse. Rather, “land surface warming” is one of more than ten bricks supporting “global warming”; and with global warming established, there is a whole other set of bricks supporting “anthropogenic global warming”. To undermine these conclusions, you’d need to remove most or all of the bricks supporting them – but as the evidence continues to pile up, that is becoming less and less likely.\nBasic rebuttal written by James Wight\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "It's not urgent",
   "paragraph": "Why it's urgent we act now on climate change\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nA large amount of warming is delayed, and if we don’t act now we could pass tipping points.\nClimate Myth...\nIt's not urgent\n\"There are many urgent priorities that need the attention of Congress, and it is not for me as an invited guest in your country to say what they are. Yet I can say this much: on any view, “global warming” is not one of them.\" (Christopher Monckton in testimony to the US Congress)\nGlobal warming is an increasingly urgent problem. The urgency isn’t obvious because a large amount of warming is being delayed. But some of the latest research says if we want to keep the Earth’s climate within the range humans have experienced, we must leave nearly all the remaining fossil fuels in the ground. If we do not act now we could push the climate beyond tipping points, where the situation spirals out of our control. How do we know this? And what should we do about it? Read on.\nJames Hansen, NASA’s top climatologist and one of the first to warn greenhouse warming had been detected, set out to define dangerous human interference with climate. In 2008, his team came to the startling conclusion that the current level of atmospheric carbon dioxide (CO2) is already in the danger zone.\nSince the Industrial Revolution, atmospheric CO2 has increased from 280 to 390 parts per million (ppm). Don’t be fooled by the small number – 390 ppm is higher than CO2 has been in millions of years. CO2 is rising by 2 ppm per year as we continue to burn fossil fuels. To stabilise the Earth’s climate, we must reduce CO2 to the relatively safe level of 350 ppm. And we must hurry, because the task will soon be an impossible one.\nThe 350 target is based not on climate modeling, but on past climate change (“paleoclimate”). Hansen looked at the highly accurate ice core record of the last few hundred thousand years, sediment core data going back 65 million years, and the changes currently unfolding. He discovered that, in the long term, climate is twice as sensitive in the real world as it is in the models used by the IPCC.\nThe key question in climate modeling is how much global warming you get from doubling CO2, once all climate feedbacks are taken into account. A feedback is something that amplifies or cancels out the initial effect (eg. interest is a feedback on a loan). The models include “fast feedbacks” like water vapor, clouds, and sea ice, but exclude longer-term “slow feedbacks” like melting ice sheets (an icy surface reflects more heat than a dark surface).\nBoth models and paleoclimate studies agree the warming after fast feedbacks is around 3°C per doubling of CO2. Slow feedbacks have received far less attention. Paleoclimate is the only available tool to estimate them. To cut a long story short, Hansen found the slow ice sheet feedback doubles the warming predicted by climate models (ie. 6°C per CO2 doubling).\nThe global climate has warmed only 0.7°C, but has not yet fully responded to our past emissions. We know this because the Earth is still gaining more heat than it is losing. There is further warming in the pipeline, and Hansen’s results imply there’s a lot more than in the models. If CO2 remains at 390 ppm long enough for the ice sheet feedback to kick in, the delayed warming would eventually reach 2°C. That would result in an Earth unlike the one on which humans evolved and a sea level rise of not one metre, not two metres, but 25 metres. Imagine waves crashing over an eight-storey building.\nIt’s hard to dispute this would be “dangerous” climate change. But how quickly could it happen? In the past, ice sheets took millennia to respond, though once they got moving sea level rose several metres per century. But maybe ice sheets can melt faster if CO2 rises faster, as it is now doing. The IPCC predicted they would grow by 2100, but instead they are starting to shrink “100 years ahead of schedule”. Once an ice sheet begins to collapse there is no way to stop it sliding into the ocean. We would suffer centuries of encroaching shorelines. The climate change we started would proceed out of our control.\nIf ice sheets can melt significantly this century, then Hansen’s long-term warming has near-term policy implications. The tragedy we have set in motion can still be prevented, if we get the Earth to stop accumulating heat before slow feedbacks can kick in. To do so we must target the greatest, fastest-growing, and longest-lived climate driver: CO2.\nUnder business as usual, we are heading for up to 1,000 ppm by 2100, or nearly two doublings (and that’s not including possible carbon feedbacks). This would surely be an unimaginable catastrophe on any timescale. Even the mitigation scenarios governments are quarreling over are based on IPCC assessments now several years out of date. The lowest CO2 target being considered is 450 ppm, which Hansen concluded would eventually melt all ice on the planet, raising sea level by 75 metres. The Earth has not been ice-free since around the time our distant ancestors split off from monkeys.\nInstead of stepping on or easing off the accelerator, we need to be slamming on the brakes. We must not only slow the rise of CO2 in the atmosphere, but reverse it. We must reduce CO2 from 390 to 350 ppm as soon as possible. That should stop the planet’s accumulation of heat. Stabilizing the CO2 level will require rapidly reducing CO2 emissions until nature can absorb carbon faster than we emit it – in practical terms, cutting emissions to near zero.\nThe only realistic way of getting back to 350 ppm is leaving most of the remaining fossil fuels in the ground. We must:\n1) phase out coal by 2030. It is not enough to slow down coal-burning by converting it to liquid fuels, because CO2 stays in the atmosphere for a very long time. The fundamental problem is with the coal being burned at all.\n2) not burn tar sands or oil shale. Their reserves are virtually untapped but thought to contain even more carbon than coal. Canada cannot keep burning them.\n3) not burn the last drops of oil and gas if their reserves are on the high side. If it turns out we have already used about half, then we can safely burn the rest.\n4) turn deforestation into reforestation. We’d still be left with the gargantuan task of removing CO2 from the atmosphere. Nature can absorb some carbon, but it has limits.\nIt won’t be easy, but with these actions CO2 could peak around 400 ppm as early as 2025 and return to 350 ppm by century’s end. I believe we can achieve this; it’s primarily a question of political will. But our window of opportunity is rapidly slamming shut. Even one more decade of business as usual, and CO2 can be expected to remain in the danger zone for a very long time.\nI should point out estimating a CO2 target from paleoclimate is fraught with uncertainties. I’ve had to simplify for this short article. I explain in more detail on Skeptical Science, or you can read Hansen’s paper free here. If there is one lesson recent climate research should teach us, it is that it’s a mistake to call uncertainty our friend. Arguably the most important aspect Hansen ignores, carbon feedbacks, is likely to make things even worse. There is more than enough reason to heed Hansen’s warning.\nRight now we stand at an intersection. What we do in this decade is crucial. If we choose one path, by the end of the decade the world could be well on its way to phasing out coal. If we choose the other, we face an uncertain future in which the only certainty is a continually shifting climate. I’ll leave the final word to Hansen et al, whose concluding statements were pretty strongly worded coming from a dense, technical, peer-reviewed paper:\nPresent policies, with continued construction of coal-fired power plants without CO2 capture, suggest that decision-makers do not appreciate the gravity of the situation. We must begin to move now toward the era beyond fossil fuels. […] The most difficult task, phase-out over the next 20-25 years of coal use that does not capture CO2, is Herculean, yet feasible when compared with the efforts that went into World War II. The stakes, for all life on the planet, surpass those of any previous crisis. The greatest danger is continued ignorance and denial, which could make tragic consequences unavoidable.\nBasic rebuttal written by James Wight\nUpdate August 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 August 2015 by MichaelK. View Archives"
  },
  {
   "title": "It's not us",
   "paragraph": "The human fingerprint in global warming\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nMultiple sets of independent observations find a human fingerprint on climate change.\nClimate Myth...\nIt's not us\n'What do the skeptics believe? First, they concur with the believers that the Earth has been warming since the end of a Little Ice Age around 1850. The cause of this warming is the question. Believers think the warming is man-made, while the skeptics believe the warming is natural and contributions from man are minimal and certainly not potentially catastrophic à la Al Gore.' (Neil Frank)\nWhen presented with the overwhelming evidence that the planet is warming, many people react by asking \"but how can we be sure that we’re causing the warming?\" It turns out that the observed global warming has a distinct human fingerprint on it.\nIn climatology, as in any other science, establishing causation is more complicated than merely establishing an effect. However, there are a number of lines of evidence that have helped to convince climate scientists that the current global warming can be attributed to human greenhouse gas emissions (in particular CO2). Here are just some of them:\nThe first four pieces of evidence show that humans are raising CO2 levels:\nHumans are currently emitting around 30 billion tonnes of CO2 into the atmosphere.\nOxygen levels are falling as if carbon is being burned to create carbon dioxide.\nFossil carbon is building up in the atmosphere. (We know this because the two types of carbon have different chemical properties.)\nCorals show that fossil carbon has recently risen sharply.\nAnother two observations show that CO2 is trapping more heat:\nSatellites measure less heat escaping to space at the precise wavelengths which CO2 absorbs.\nSurface measurements find this heat is returning to Earth to warm the surface.\nThe last four indicators show that the observed pattern of warming is consistent with what is predicted to occur during greenhouse warming:\nAn increased greenhouse effect would make nights warm faster than days, and this is what has been observed.\nIf the warming is due to solar activity, then the upper atmosphere (the stratosphere) should warm along with the rest of the atmosphere. But if the warming is due to the greenhouse effect, the stratosphere should cool because of the heat being trapped in the lower atmosphere (the troposphere). Satellite measurements show that the stratosphere is cooling.\nThis combination of a warming troposphere and cooling stratosphere should cause the tropopause, which separates them, to rise. This has also been observed.\nIt was predicted that the ionosphere would shrink, and it is indeed shrinking.\n(References for all of these findings can be found here.)\nOften one hears claims that the attribution of climate change is based on modeling, and that nobody can really know its causes. But here we have a series of empirical observations, all of which point to the conclusion that humans are causing the planet to warm.\nBasic rebuttal written by James Wight\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "It's only a few degrees",
   "paragraph": "Why a few degrees of global warmings matter\nLink to this page\nWhat the science says...\nA few degrees of global warming has a huge impact on ice sheets, sea levels and other aspects of climate.\nClimate Myth...\nIt's only a few degrees\n\"There might be some adverse outcomes from that eight tenths of a degree of temperature rise threatening my Grandchildren in 2050, but neither I nor anyone else knows what those outcomes might be. We’ll assuredly get an extra flood over here, and one less flood over there, it’s very likely to be drier somewhere and wetter somewhere else, in other words, the climate will do what climate has done since forever — change.\" (Willis Eschenbach)\nThere are 3 problems with even small sounding global warming. Firstly, 2 °C is a very optimistic assessment: if the skeptical Dr Roy Spencer is correct here then we’re on course to get more like 3.5 °C. If most climate science is correct then we’ll get 6 °C by doubling CO2 twice.\nSecondly, if we cause a ~2 °C warming, some scientists think feedbacks such as melting permafrost releasing more greenhouse gases might kick in. Ice and sediment cores suggest we haven’t been this warm in at least 600,000 years so we’re not sure – but this could trigger a lot more warming.\nFinally, 6 °C, the actual “best estimate” for eventual global warming from current CO2 trends still sounds small. But heating isn’t distributed evenly: as we came out of the last ice age, the temperature in northern countries rose by more than at the equator. When you average over the entire world it turns out to have only been about 6 °C global warming: for people living in Northern Europe and Canada it’s the difference between walking around in a t-shirt and a mile of ice over your head.\nThe graph below is the temperature calculated over the past 400,000 years in Antarctica from the Vostok ice core. The tiny peaks are a bit like today and the tiny troughs would force hundreds of millions from their homes. A few degrees of warming might sound small, but it can mean a lot and this is why scientists look at what the impacts of warming will be, rather than just saying “it doesn’t look like much so it can’t matter”.\nBasic rebuttal written by MarkR\nUpdate August 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 4 August 2015 by MichaelK. View Archives"
  },
  {
   "title": "It's Pacific Decadal Oscillation",
   "paragraph": "The Pacific Decadal Oscillation (PDO) is not causing global warming\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nThe PDO shows no trend, and therefore the PDO is not responsible for the trend of global warming.\nClimate Myth...\nIt's Pacific Decadal Oscillation\n\"The Pacific Decadal Oscillation (PDO) is a temperature pattern in the Pacific Ocean that spends roughly 20-30 years in the cool phase or the warm phase. In 1905, PDO switched to a warm phase. In 1946, PDO switched to a cool phase. In 1977, PDO switched to a warm phase. In 1998, PDO showed a few cool years. Note that the cool phases seem to coincide with the periods of cooling (1946-1977) and the warm phases seem to coincide with periods of warming (1905-1946, 1977-1998).\" (The Reference Frame)\nThe Pacific Decadal Oscillation (PDO) is a climate phenomenon that occurs primarily in the North Pacific Ocean. The “oscillation” happens between warm phases (positive values) and cool phases (negative values) that last anywhere from 10 to 40 years. The phases are associated with changes in sea surface temperatures (SST). While the causes of the PDO are still unknown, the primary effects seem to be changes in northeast Pacific marine ecosystems and a changing jet stream path.\nImportant to note, however, is that the phases are not set in stone; there are frequently short sets of 1-5 warm years during a cool phase and vice-versa. Secondly, the “warm” and “cool” phases are less descriptive than they would appear. The cool period, for instance, is actually associated with extremely high sea surface temperatures in the Northern Pacific (see image below).\nFigure 1: PDO warm phase (left) and cool phase (right). Image courtesy of JISAO.\nOne way to test this skeptic theory is to plot the Global Temperature Anomaly alongside the PDO Index (shown below). What we find is that although the PDO index appears to influence short-term temperature changes, global temperatures have a distinct upward trend, while the PDO Index does not.\nFigure 2: Pacific Decadal Oscillation index (blue, University of Washington) versus Global Temperature Anomaly (Red - GISS Temp). Smoothed data (thicker blue and red lines) and trend lines (thick straight line) are added.\nNatural oscillations like PDO simply move heat around from oceans to air and vice-versa. They don't have the ability to either create or retain heat, therefore they're not capable of causing a long-term warming trend, just short-term temperature variations. Basically they're an example of internal variability, not an external radiative forcing. If PDO were responsible for warming the surface, the oceans would be cooling, which is not the case.\nThese results are expected. The long term warming trend is a result of an energy imbalance caused primarily by an increase of greenhouse gases in the atmosphere. In contrast, the PDO is an internal process and does not increase or decrease the total energy in the climate system.\nLast updated on 16 September 2010 by Nicholas Berini."
  },
  {
   "title": "It's planetary movements",
   "paragraph": "Climastrology - curve fitting planetary movements\nLink to this page\nWhat the science says...\nTrying to blame global warming on planetary movements is little more than 'climastrology' and curve fitting. There is no physical evidence that planetary movements influence the Earth's temperature.\nClimate Myth...\nIt's planetary movements\n\"The solar system oscillates with a 60-year cycle due to the Jupiter/Saturn three-synodic cycle and to a Jupiter/Saturn beat tidal cycle...About 60% of the warming observed from 1970 to 2000 was very likely caused by the above natural 60-year climatic cycle during its warming phase\" (Loehle and Scafetta)\nIn a paper published in The Bentham Open Atmospheric Science Journal, two \"skeptics\", Loehle and Scafetta (L&S), perform a curve fitting exercise, trying to match global temperature changes using a very simple model involving cycles which they claim are caused by planetary movements. L&S describe their methodology in the paper:\n\"The model was fit by nonlinear least squares estimation using Mathematica functions, with phase and amplitude free but period fixed as above.\"\nIn other words, they let the parameters vary freely without any physical constraints, and fit the curve as best they could. They suggest the first two terms represent astronomical cycles:\n\"The solar system oscillates with a 60-year cycle due to the Jupiter/Saturn three-synodic cycle and to a Jupiter/Saturn beat tidal cycle\"\nHow exactly these Jupiter and Saturn orbital cycles are supposed to impact the Earth's surface temperature is left unexplained, and thus their model has no physical basis in reality. A fundamental problem with the L&S model is the assumption of significant effects of 20 and 60 year astronomical cycles on the Earth's temperature. Aside from failing to provide a physical mechanism through which Jupiter and Saturn could impact the Earth's temperature, as Riccardo has previously noted, the mere existence of a 60 year cycle in the Earth's surface temperature depends on the choice of trend to begin with. By fitting certain polynomials to the global temperature data, we can find a residual with a 60 year cycle, but only with certain curve fitting choices. Those curve fitting choices must first be justified.\nHowever, instead of first subtracting off the underlying trend and seeing what's left, L&S start their exercise under the assumption that the 20 and 60 year cycles are present, and then fit the temperature data as best they can with those cycles (with no physical constraints). After they conduct this curve fitting, they see what's left and attribute the remainder to human effects. This is not a scientific approach, it's simply curve fitting (a.k.a. \"graph cooking\" and \"mathturbation\") at its worst.\nThere may be a 60 year cycle in the global climate, perhaps related to an oceanic cycle like the Pacific Decadal Oscillation. However, in order to explain the warming of both the oceans and atmosphere, there must be an external forcing at work, which may be why L&S invoke these mysterious astronomical cycles. But the fact remains that they have failed to provide a physical mechanism through which these cycles could impact the Earth's climate.\nL&S use a model with a very simple formula:\nYou may be able to guess how their model will perform just by examining this formula. The first two terms are their proposed plantary oscillations of 60 and 20 year periods, respectively. The third term will produce a linear warming trend, and the fourth is simply a constant. So this model will produce a linear warming trend with two natural oscillations superimposed on top of it.\nL&S tweaked the parameters to fit the temperature curve without doing any further testing, but we can test it for them by running their model backwards in time and comparing to reconstructed temperatures. Because of the linear term, you might expect the model to match the observations back to the Little Ice Age (LIA) and then rapidly diverge from observations. If so, you would essentially be right (Figure 1).\nFigure 1: The L&S Case 2 model projected backwards in time (red), compared to the Moberg et al. (2005) millennial northern hemisphere temperature reconstruction (blue) and the Loehle (2008) millennial global temperature reconstruction (green).\nTalk about a divergence problem!\nLoehle is generally most well-known for his millennial global temperature reconstruction using non-tree ring proxies. Although the paper was published in Energy & Environment, which is not considered a peer-reviewed journal, considering that Loehle was the lead author on L&S, we felt it would be worthwhile to see if Loehle's model matches his own temperature reconstruction.\nThe L&S model matches the reconstructed temperature trends reasonably well back to the 17th century, but fails miserably to match prior temperatures. Moreover, the 60 year cycle in the L&S model matches up extremely poorly with the Moberg reconstruction (Figure 2), and even with Loehle's own reconstruction (Figure 3).\nFigure 2: The L&S Case 2 model projected backwards to the period 1500 to 1900, compared to the Moberg et al. (2005) milennial northern hemisphere temperature reconstruction.\nFigure 3: The L&S Case 2 model projected backwards to the period 1500 to 1900, compared to the Loehle (2008) milennial global temperature reconstruction.\nSeveral times between 1500 and 1900, the L&S model is anti-phase with both reconstructions, with the peak of the 60 year cycle matching a trough in temperature. Thus we see that although L&S have gotten lucky and matched the temperature trend a few centuries into the past, the 60 year cycle which is the basis of their paper is nowhere to be found in the temperature data.\nFitting a curve with a simple model using physically unconstrained parameters is simply not a scientific process, as illustrated in its failure when put to the hindcasting test. In fact, the assumed 60 year cycle which was the basis of the L&S model doesn't even show up in Loehle's own millennial global temperature reconstruction - a glaring contradiction.\nYou could just as easily conduct this sort of curve fitting exercise using the number of pirates, canoes, and pantaloons in southern Spain. L&S are guilty of a major error in the abstract of their paper:\n\"About 60% of the warming observed from 1970 to 2000 was very likely caused by the above natural 60-year climatic cycle during its warming phase.\"\nCorrelation is not causation; all L&S have demonstrated in their unphysical curve fitting exercise is a correlation between their cycles and global temperature. If I find a correlation between Spanish pirates and pantaloons and global temperature, that doesn't mean these variables are causing global warming. If you want to draw a conclusion like L&S have, you need to identify the physical mechanism through which your variables are causing a global temperature change, identify a realistic physical range that this effect can have on the temperature, and then run your model.\nWithout a realistic physical basis, like Spencer before them, all L&S are doing is playing pointless curve fitting games, and using their results to draw unsubstantiated conclusions. Climastrology is not real science, and planetary movements are not causing global warming.\nLast updated on 20 November 2011 by dana1981. View Archives"
  },
  {
   "title": "It's soot",
   "paragraph": "Soot and global warming\nLink to this page\nWhat the science says...\nSoot stays in the atmosphere for days to weeks; carbon dioxide causes warming for centuries.\nClimate Myth...\nIt's soot\n\"The Intergovernmental Panel On Climate Change (IPCC) drastically understates the warming potential of soot (black carbon) in its report to policy makers. The IPCC has an agenda and that agenda is to blame manmade carbon dioxide emission for climate change. Europe and Asia emit most of the soot from burning coal, wood, dung, and diesel in open fires or without particulate filters in stoves, chimneys, smokestacks, and exhaust pipes. The United States has been restricting soot emissions in Draconian fashion since the Clean Air Act of 1963. The IPCC agenda is really about blaming the United States.\" (Uncommon Descent)\nSoot, also called black carbon (BC), contributes to climate warming in two ways. First, black soot particles in the air absorb sunlight and directly heat the surrounding air. Second, soot falling on snow or ice changes those reflecting surfaces into absorbing ones, that is, soot decreases the albedo. Therefore, soot deposits increase the melting rate of snow and ice, including glaciers and the arctic ice.\nBlack carbon is a “short-term” climate forcer. Over the short term it is an important contributor to warming; so reducing soot will have immediate benefits in slowing warming over the next 40 years, perhaps by 0.1-0.2°C globally. Decreasing black carbon deposits in the arctic may also slow amplification of feedbacks from melting arctic snow and ice.\nBlack carbon does not accumulate in the atmosphere like CO2. So, reductions in BC have immediate, but not long-term effects on global warming. CO2 is certainly the “biggest control knob” on climate, and climate change cannot be prevented without reducing carbon emissions. Reductions in BC and CO2 and methane and ozone will be necessary to keep global temperatures from rising more than 2°C above preindustrial levels in the next 50 years.\n“It is important to emphasize that BC reduction can only help delay and not prevent unprecedented climate changes due to CO2 emissions.” (Ramanathan and Carmichael. Global and regional climate changes due to black carbon. Nature Geoscience (2008) vol. 1 (4) pp. 221-227)\n“Short-lived climate forcers – methane, black carbon and ozone – are fundamentally different from longer-lived greenhouse gases, remaining in the atmosphere for only a relatively short time. Deep and immediate carbon dioxide reductions are required to protect long-term climate, as this cannot be achieved by addressing short-lived climate forcers.” (Integrated Assessment of Black Carbon and Tropospheric Ozone; United Nations Environment Programme, http://www.unep.org/dewa/Portals/67/pdf/Black_Carbon.pdf, 2011)\nBecause of its short lifetime in the atmosphere the effects of BC are most important regionally, especially in South and East Asia. Other hotspots occur in Mexico, Brazil, Peru, and parts of Africa. In Asia, BC contributes to regional heating and disrupts rainfall patterns. BC is of great concern in the Himalayas where it accelerates melting of glaciers, which supply water to millions.\nThe largest sources of BC are incomplete burning of biomass and unfiltered diesel exhaust. Major reductions could be achieved by replacing traditional cook and heat stoves in developing countries with clean-burning biomass stoves or alternative fuel systems. Installation of filters on diesel vehicles reduces BC. Industrial coke ovens and brick kilns should also be updated to employ newer, cleaner technologies. Finally, open field burning of agricultural waste should be eliminated. These old technologies are primarily used in developing countries.\nIn the industrialized northern hemisphere, residential wood stoves are the primary source of BC. Emissions from North America and Europe are the major controllable sources of BC to the Arctic, contributing significantly to northern warming and loss of ice.\nThe major sources of BC (biomass burning for cooking and heating, and diesel engines) are not the biggest sources of CO2 (coal and fossil fuel burning). Therefore, both problems can and must be addressed independently and simultaneously. Immediate reductions in BC can buy a little time as we convert to low carbon energy sources.\nThere are reasons besides climate change to reduce BC emissions. Black carbon has serious and well documented health effects; worldwide reductions in soot emissions would prevent an estimated 2.4 million premature deaths. Emissions of BC are accompanied by CO, and volatile organic compounds (VOCs), which have additional adverse health effects.\nReferences\nIntegrated Assessment of Black Carbon and Tropospheric Ozone: Summary for Decision Makers. United Nations Environment Programme and World Meteorological Organization (2011) pp. 1-36.\nLacis et al. Atmospheric CO2: Principal Control Knob Governing Earth's Temperature. Science (2010) vol. 330 (6002) pp. 356-359.\nRamanathan and Carmichael. Global and regional climate changes due to black carbon. Nature Geoscience (2008) vol. 1 (4) pp. 221-227.\nLast updated on 28 March 2011 by Sarah."
  },
  {
   "title": "It's the ocean",
   "paragraph": "Why ocean heat can’t drive climate change, only chase it\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe oceans are warming and moreover are becoming more acidic, threatening the food chain.\nClimate Myth...\nIt's the ocean\n\"These small global temperature increases of the last 25 years and over the last century are likely natural changes that the globe has seen many times in the past. This small warming is likely a result of the natural alterations in global ocean currents which are driven by ocean salinity variations. Ocean circulation variations are as yet little understood. Human kind has little or nothing to do with the recent temperature changes. We are not that influential.\" (William Gray)\nThe argument attributing the warming of the Earth to heat being released by the oceans was clearly articulated by William M Gray, one of the world’s foremost experts on tropical storms. Unfortunately, his views on oceans and their part in global warming appear to contradict the published science. Gray believes that the increased atmospheric heat – which he calls a ‘small warming’ – is “...likely a result of the natural alterations in global ocean currents which are driven by ocean salinity variations.\" (BBC Interview 2000)\nThe Science\nThe problem with Gray’s argument is that unless more heat was being poured into the oceans, they would be obliged by the laws of physics to cool when heat was transferred to the atmosphere.\n80% of the heat in the planet's ecosystem is stored in the oceans, and they have been getting consistently warmer over time (Ocean cooling: skeptic arguments drowned by data). There would also be other indicators e.g. sea levels, which would be static or go down by some small amount as a result of thermal contraction. There are no indicators of ocean heat driving temperature changes that are supported by the evidence. It should also be noted that Gray has never published, nor offered any proof, of these theories, so his views are purely speculative.\nClaims that the warming of the planet is due to heat being released from the oceans into the atmosphere are not supported by any empirical evidence or peer-reviewed science.\nLast updated on 24 October 2010 by gpwayne."
  },
  {
   "title": "It's the sun",
   "paragraph": "Sun & climate: moving in opposite directions\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nThe sun's energy has decreased since the 1980s but the Earth keeps warming faster than before.\nClimate Myth...\nIt's the sun\n\"Over the past few hundred years, there has been a steady increase in the numbers of sunspots, at the time when the Earth has been getting warmer. The data suggests solar activity is influencing the global climate causing the world to get warmer.\" (BBC)\nOver the last 35 years the sun has shown a cooling trend. However global temperatures continue to increase. If the sun's energy is decreasing while the Earth is warming, then the sun can't be the main control of the temperature.\nFigure 1 shows the trend in global temperature compared to changes in the amount of solar energy that hits the Earth. The sun's energy fluctuates on a cycle that's about 11 years long. The energy changes by about 0.1% on each cycle. If the Earth's temperature was controlled mainly by the sun, then it should have cooled between 2000 and 2008.\nFigure 1: Annual global temperature change (thin light red) with 11 year moving average of temperature (thick dark red). Temperature from NASA GISS. Annual Total Solar Irradiance (thin light blue) with 11 year moving average of TSI (thick dark blue). TSI from 1880 to 1978 from Krivova et al 2007. TSI from 1979 to 2015 from the World Radiation Center (see their PMOD index page for data updates). Plots of the most recent solar irradiance can be found at the Laboratory for Atmospheric and Space Physics LISIRD site.\nThe solar fluctuations since 1870 have contributed a maximum of 0.1 °C to temperature changes. In recent times the biggest solar fluctuation happened around 1960. But the fastest global warming started in 1980.\nFigure 2 shows how much different factors have contributed recent warming. It compares the contributions from the sun, volcanoes, El Niño and greenhouse gases. The sun adds 0.02 to 0.1 °C. Volcanoes cool the Earth by 0.1-0.2 °C. Natural variability (like El Niño) heats or cools by about 0.1-0.2 °C. Greenhouse gases have heated the climate by over 0.8 °C.\nFigure 2 Global surface temperature anomalies from 1870 to 2010, and the natural (solar, volcanic, and internal) and anthropogenic factors that influence them. (a) Global surface temperature record (1870–2010) relative to the average global surface temperature for 1961–1990 (black line). A model of global surface temperature change (a: red line) produced using the sum of the impacts on temperature of natural (b, c, d) and anthropogenic factors (e). (b) Estimated temperature response to solar forcing. (c) Estimated temperature response to volcanic eruptions. (d) Estimated temperature variability due to internal variability, here related to the El Niño-Southern Oscillation. (e) Estimated temperature response to anthropogenic forcing, consisting of a warming component from greenhouse gases, and a cooling component from most aerosols. (IPCC AR5, Chap 5)\nSome people try to blame the sun for the current rise in temperatures by cherry picking the data. They only show data from periods when sun and climate data track together. They draw a false conclusion by ignoring the last few decades when the data shows the opposite result.\nBasic rebuttal written by Larry M, updated by Sarah\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 2 April 2017 by Sarah. View Archives"
  },
  {
   "title": "It's too hard",
   "paragraph": "Can we fix global warming?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nScientific studies have determined that current technology is sufficient to reduce greenhouse gas emissions enough to avoid dangerous climate change.\nClimate Myth...\nIt's too hard\n\"The fact is that there is no one in the world who can explain how we could cut our emissions by four fifths without shutting down virtually all our existing economy. What carries this even further into the higher realms of lunacy is that such a Quixotic gesture would do nothing to halt the world’s fast-rising CO2 emissions, already up 40 per cent since 1990. There is no way for us to prevent the world’s CO2 emissions from doubling by 2100\" (Christopher Booker)\nIn order to avoid dangerous global warming, we need to reduce global greenhouse gas (GHG) emissions by about 50% by the year 2050. Skeptics often make the argument that we simply don't have the technology necessary to reduce emissions this much, this quickly.\nPacala and Socolow (2004) investigated this claim by examining the various technologies available to reduce GHG emissions. Every technology they examined \"has passed beyond the laboratory bench and demonstration project; many are already implemented somewhere at full industrial scale.\" The study used the concept of a \"stabilization wedge\", in which \"a wedge represents an activity that reduces emissions to the atmosphere by a certain amount. The study identifies 15 current options which could be scaled up to produce at least one wedge:\nImproved fuel economy\nReduced reliance on cars\nMore efficient buildings\nImproved power plant efficiency\nSubstituting natural gas for coal\nStorage of carbon captured in power plants\nStorage of carbon captured in hydrogen plants\nStorage of carbon captured in synthetic fuels plants\nNuclear power\nWind power\nSolar photovoltaic power\nRenewable hydrogen\nBiofuels\nForest management\nAgricultural soils management\nThis is not an exhaustive list, and there are other possible wedges, such as other renewable energy technologies they did not consider. The study notes that \"Every one of these options is already implemented at an industrial scale and could be scaled up further over 50 years to provide at least one wedge.\" Implementing somewhere between 7 and 14 wedges would be necessary to avoid dangerous climate change.\nThe bottom line is that while achieving the necessary GHG emissions reductions and stabilization wedges will be difficult, it is possible. And there are many solutions and combinations of wedges to choose from.\nLast updated on 8 November 2010 by dana1981."
  },
  {
   "title": "It's waste heat",
   "paragraph": "Greenhouse warming 100 times greater than waste heat\nLink to this page\nWhat the science says...\nThe contribution of waste heat to the global climate is 0.028 W/m2. In contrast, the contribution from human greenhouse gases is 2.9 W/m2. Greenhouse warming is adding about 100 times more heat to our climate than waste heat.\nClimate Myth...\nIt's waste heat\n\"Global warming is mostly due to heat production by human industry since the 1800s, from nuclear power and fossil fuels, better termed hydrocarbons, – coal, oil, natural gas. Greenhouse gases such as carbon dioxide (CO2 play a minor role even though they are widely claimed the cause.\" (Morton Skorodin)\nWhen humans use energy, it gives off heat. Whenever we burn fossil fuels, heat is emitted. This heat doesn't just disappear - it dissipates into our environment. How much does waste heat contribute to global warming? This has been calculated in Flanner 2009 (if you want to read the full paper, access details are posted here). Flanner contributes that the contribution of waste heat to the global climate is 0.028 W/m2. In contrast, the contribution from human greenhouse gases is 2.9 W/m2 (IPCC AR4 Section 2.1). Waste heat is about 1% of greenhouse warming.\nWhat does these numbers mean? They refer to radiative forcing, the change in energy flux at the top of the atmosphere. Or putting it in plain English, the amount of heat being added to our climate. Greenhouse warming is currently adding about 100 times more heat to our climate than waste heat.\nLast updated on 27 July 2010 by John Cook."
  },
  {
   "title": "Jupiter is warming",
   "paragraph": "Global warming on Jupiter\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nJupiter is not warming, and anyway the sun is cooling.\nClimate Myth...\nJupiter is warming\n\"A new storm and a new red spot on Jupiter hints at climate change, USA TODAY and dozens of other sources explained yesterday. The temperatures are expected to change by as much as 10 Fahrenheit degrees at different places of the globe. At least close to the new spot and to the equator, nothing less than global warming is expected\" (source: The Reference Frame).\nThis argument is part of a greater one that other planets are warming. If this is happening throughout the solar system, clearly it must be the sun causing the rise in temperatures – including here on Earth.\nIt is curious that the theory depends so much on sparse information – what we know about the climates on other planets and their history – yet its proponents resolutely ignore the most compelling evidence against the notion. Over the last fifty years, the sun’s output has decreased slightly: it is radiating less heat. We can measure the various activities of the sun pretty accurately from here on Earth, or from orbit above it, so it is hard to ignore the discrepancy between the facts and the sceptical argument that the sun is causing the rise in temperatures.\nTSI from 1880 to 1978 from Solanki. TSI from 1979 to 2009 from PMOD.\nBut if the sun’s output has levelled off or even diminished, then what is causing other planets to warm up? Are they warming at all?\nThe planets and moons that are claimed to be warming total roughly eight out of dozens of large bodies in the solar system. Some, like Uranus, may be cooling. All the outer planets have vastly longer orbital periods than Earth, so any climate change on them may be seasonal. Saturn and its moons take 30 Earth years to orbit the Sun, so three decades of observations equates to only 1 Saturnian year. Uranus has an 84-year orbit and 98° axial tilt, so its seasons are extreme. Neptune has not yet completed a single orbit since its discovery in 1846.\nThis is a round-up of the planets said by sceptics to be experiencing climate change:\nJupiter: the notion that Jupiter is warming is actually based on predictions, since no warming has actually been observed. Climate models predict temperature increases along the equator and cooling at the poles. It is believed these changes will be catalysed by storms that merge into one super-storm, inhibiting the planet’s ability to mix heat. Sceptical arguments have ignored the fact this is not a phenomenon we have observed, and that the modelled forcing is storm and dust movements, not changes in solar radiation.\nMars: the notion that Mars is warming came from an unfortunate conflation of weather and climate. Based on two pictures taken 22 years apart, assumptions were made that have not proved to be reliable. There is currently no evidence to support claims that Mars is warming at all.\nNeptune: observations of changes in luminosity on the surface of both Neptune and its largest moon, Triton, have been taken to indicate warming caused by increased solar activity. In fact, the brightening is due to the planet’s seasons changing, but very slowly. Summer is coming to Neptune’s southern hemisphere, bringing more sunlight, as it does every 164 years.\nPluto: the warming exhibited by Pluto is not really understood. Pluto’s seasons are the least understood of all: its existence has only been known for a third of its 248 -year orbit, and it has never been visited by a space probe. The ‘evidence’ for climate change consists of just two observations made in 1988 and 2002. That’s equivalent to observing the Earth’s weather for just three weeks out of the year. Various theories suggest its highly elliptical orbit may play a part, as could the large angle of its rotational axis. One recent paper suggests the length of Pluto’s orbit is a key factor, as with Neptune. Sunlight at Pluto is 900 times weaker than it is at the Earth.\nClaims that solar system bodies are heating up due to increased solar activity are clearly wrong. The sun’s output has declined in recent decades. Only Pluto and Neptune are exhibiting increased brightness. Heating attributed to other solar bodies remains unproven.\nBasic rebuttal written by GPWayne\nLast updated on 1 August 2013 by gpwayne. View Archives"
  },
  {
   "title": "Less than half of published scientists endorse global warming",
   "paragraph": "Attempts to cast doubt on scientific consensus on climate change despite 97% agreement\nLink to this page\nWhat the science says...\nSchulte's paper makes much of the fact that 48% of the papers they surveyed are neutral papers, refusing to either accept or reject anthropogenic global warming. The fact that so many studies on climate change don't bother to endorse the consensus position is significant because scientists have largely moved from what's causing global warming onto discussing details of the problem (eg - how fast, how soon, impacts, etc).\nClimate Myth...\nLess than half of published scientists endorse global warming\nKlaus-Martin Schulte examined all papers published from 2004 to February 2007. Of 528 total papers on climate change, only 38 (7%) gave an explicit endorsement of the consensus. While only 32 papers (6%) reject the consensus outright, the largest category (48%) are neutral papers, refusing to either accept or reject the hypothesis. Only a single one makes any reference to climate change leading to catastrophic results. (Source: DailyTech)\nSchulte's paper (going on DailyTech's account) places great emphasis on the fact that only one paper endorses 'catastrophic climate change'. This is a classic straw man argument. Oreskes' 2004 paper never refers to an imminent catastrophe. Neither do the IPCC nor do the Academies of Science from 11 countries that endorse the consensus position that most of the warming over the last 50 years is likely due to the increase in greenhouse gas concentrations.\nEven more fuss is made over the large percentage of neutral studies. Ironically, Oreskes emphasised the same point in 2004 when she published The Scientific Consensus on Climate Change. Nowadays, earth science papers are rarely found explicitly endorsing plate tectonics as the theory is established and taken for granted. The fact that so many studies on climate change don't bother to endorse the consensus position is significant because scientists have largely moved from what's causing global warming onto discussing details of the problem (eg - how fast, how soon, impacts, etc).\nWhat of the 6% of papers that reject AGW? The most appropriate approach would be to see what these papers actually say. Schulte's paper is yet to be published so the full list is not available (please contact me if you have more info). Monckton does mention several studies which one assumes are the \"cream of the crop\". Deltoid also has its readers categorising peer review studies since 2003. The papers purported to reject the consensus can be divided into several categories:\nNon-scientific papers\nTwo of the papers conduct no actual scientific research but merely review social aspects of climate science. I'm baffled as to why they would be included other than to \"boost the numbers\":\nLeiserowitz 2005 asks the question \"Is Climate Change Dangerous?\" It then proceeds to \"describes results from a national study that examined the risk perceptions and connotative meanings of global warming in\nthe American mind\". In other words, it doesn't answer the question \"is climate change dangerous\" - instead it answers \"does the public think climate change is dangerous?\"\nGerhard 2004 (published in the American Association of Petroleum Geologists Bulletin) \"summarizes recent scientific progress in climate science and arguments about human influence on climate\".\nPapers that don't actually reject the consensus\nThree papers focus on specific aspects of climate change but don't actually reject the consensus:\nCao 2005 recommends multi-scale modelling techniques to better understand and quantify the carbon cycle. It mentions uncertainties in our understanding of the carbon cycle but doesn't refute the consensus position at all.\nLai 2004 suggests internal processes in the ocean may be causing global warming. Paradoxically, it concludes by recommending we \"reduce carbon dioxide emissions to the atmosphere, thus reduce global warming\". More on the ocean...\nMoser 2005 studies the uncertainties of the impact of rising sea levels in 3 US states. The emphasis is on society's ability to adapt to rising sea levels and contributes no research on the cause of global warming.\nBonafide scientific papers rejecting the consensus\nThere are some papers that conduct original research and reject the consensus. It's useful to look at the actual arguments they present to reject AGW:\nShaviv 2005 claims cosmic rays are causing global warming. While the link between cosmic rays and clouds are still under question, the more serious problem is that the correlation between cosmic rays and temperature ended in the 1970's when the modern global warming trend began. More on cosmic rays...\nZhen-Shan 2006, performs statistical analysis on the temperature record and finds temperature doesn't linearly follow CO2. Looking at global cooling from 1940 to 1970, they conclude \"The global climate warming is not solely affected by the CO2 greenhouse effect\". Ignoring aerosol cooling and solar forcing while failing to recognise that temperature's relationship with CO2 is logarithmic, not linear, are serious failings. More on mid-century global cooling...\nUPDATE 20 Sep 2007: paper not to be published. Apparently, the news that Schulte's paper would be published was grossly exagerated as editor Sonja Boehmer-Christiansen has confirmed Energy and Environment will not be publishing the paper:\n\"His survey of papers critical of the consensus was a bit patchy and nothing new, as you point out. it was not what was of interest to me; nothing has been published.\"\nUPDATE 24 Mar 2008: Apparently Energy and Environment have reversed their policy and published the Schulte paper.\nUPDATE 25 Mar 2008: Chris Monckton posts his side of the story on DeSmogBlog. in response to John Mashey's critique of Schulte's paper.\nLast updated on 29 October 2016 by John Cook. View Archives"
  },
  {
   "title": "Lindzen and Choi find low climate sensitivity",
   "paragraph": "Working out climate sensitivity from satellite measurements\nLink to this page\nWhat the science says...\nLindzen's analysis has several flaws, such as only looking at data in the tropics. A number of independent studies using near-global satellite data find positive feedback and high climate sensitivity.\nClimate Myth...\nLindzen and Choi find low climate sensitivity\nClimate feedbacks are estimated from fluctuations in the outgoing radiation budget from the latest version of Earth Radiation Budget Experiment (ERBE) nonscanner data. It appears, for the entire tropics, the observed outgoing radiation fluxes increase with the increase in sea surface temperatures (SSTs). The observed behavior of radiation fluxes implies negative feedback processes associated with relatively low climate sensitivity. This is the opposite of the behavior of 11 atmospheric models forced by the same SSTs. (Lindzen 2009)\nClimate sensitivity is a measure of how much our climate responds to an energy imbalance. The most common definition is the change in global temperature if the amount of atmospheric CO2 was doubled. If there were no feedbacks, climate sensitivity would be around 1°C. But we know there are a number of feedbacks, both positive and negative. So how do we determine the net feedback? An empirical solution is to observe how our climate responds to temperature change. We have satellite measurements of the radiation budget and surface measurements of temperature. Putting the two together should give us an indication of net feedback.\nOne paper that attempts to do this is On the determination of climate feedbacks from ERBE data (Lindzen et al 2009). It looks at sea surface temperature in the tropics (20° South to 20° North) from 1986 to 2000. Specifically, it looked at periods where the change in temperature was greater than 0.2°C, marked by red and blue colors (Figure 1).\nFigure 1: Monthly sea surface temperature for 20° South to 20° North. Periods of temperature change greater than 0.2°C marked by red and blue (Lindzen et al 2009).\nLindzen et al also analysed satellite measurements of outgoing radiation over these periods. As short-term tropical sea surface temperatures are largely driven by the El Nino Southern Oscillation, the change in outward radiation offers an insight into how climate responds to changing temperature. Their analysis found that when it gets warmer, there was more outgoing radiation escaping to space. They concluded that net feedback is negative and our planet has a low climate sensitivity of about 0.5°C.\nDebunked by Trenberth\nHowever, a response to this paper, Relationships between tropical sea surface temperature and top-of-atmosphere radiation (Trenberth et al 2010) revealed a number of flaws in Lindzen's analysis. It turns out the low climate sensitivity result is heavily dependent on the choice of start and end points in the periods they analyse. Small changes in their choice of dates entirely change the result. Essentially, one could tweak the start and end points to obtain any feedback one wishes.\nFigure 2: Warming (red) and cooling (blue) intervals of tropical SST (20°N – 20°S) used by Lindzen et al 2009 (solid circles) and an alternative selection proposed derived from an objective approach (open circles) (Trenberth et al 2010).\nDebunked by Murphy\nAnother major flaw in Lindzen's analysis is that they attempt to calculate global climate sensitivity from tropical data. The tropics are not a closed system - a great deal of energy is exchanged between the tropics and subtropics. To properly calculate global climate sensitivity, global observations are required.\nThis is confirmed by another paper published in early May (Murphy 2010). This paper finds that small changes in the heat transport between the tropics and subtropics can swamp the tropical signal. They conclude that climate sensitivity must be calculated from global data.\nDebunked by Chung\nIn addition, another paper reproduced the analysis from Lindzen et al 2009 and compared it to results using near-global data (Chung et al 2010). The near-global data find net positive feedback and the authors conclude that the tropical ocean is not an adequate region for determining global climate sensitivity.\nDebunked by Dessler\nDessler (2011) found a number of errors in Lindzen and Choi (2009) (slightly revised as Lindzen & Choi [2011]). First, Lindzen and Choi's mathematical formula to calculate the Earth's energy budget may violate the laws of thermodynamics - allowing for the impossible situation where ocean warming is able to cause ocean warming. Secondly, Dessler finds that the heating of the climate system through ocean heat transport is approximately 20 times larger than the change in top of the atmosphere (TOA) energy flux due to cloud cover changes. Lindzen and Choi assumed the ratio was close to 2 - an order of magnitude too small.\nThirdly, Lindzen and Choi plot a time regression of change in TOA energy flux due to cloud cover changes vs. sea surface temperature changes. They find larger negative slopes in their regression when cloud changes happen before surface temperature changes, vs. positive slopes when temperature changes happen first, and thus conclude that clouds must be causing global warming.\nHowever, Dessler also plots climate model results and finds that they also simulate negative time regression slopes when cloud changes lead temperature changes. Crucially, sea surface temperatures are specified by the models. This means that in these models, clouds respond to sea surface temperature changes, but not vice-versa. This suggests that the lagged result first found by Lindzen and Choi is actually a result of variations in atmospheric circulation driven by changes in sea surface temperature, and contrary to Lindzen's claims, is not evidence that clouds are causing climate change, because in the models which successfully replicate the cloud-temperature lag, temperatures cannot be driven by cloud changes.\n2011 Repeat\nLindzen and Choi tried to address some of the criticisms of their 2009 paper in a new version which they submitted in 2011 (LC11), after Lindzen himself went as far as to admit that their 2009 paper contained \"some stupid mistakes...It was just embarrassing.\" However, LC11 did not address most of the main comments and contradictory results from their 2009 paper.\nLindzen and Choi first submitted LC11 to the Proceedings of the National Academy of Sciences (PNAS) after adding some data from the Clouds and the Earth’s Radiant Energy System (CERES).\nPNAS editors sent LC11 out to four reviewers, who provided comments available here. Two of the reviewers were selected by Lindzen, and two others by the PNAS Board. All four reviewers were unanimous that while the subject matter of the paper was of sufficient general interest to warrant publication in PNAS, the paper was not of suitable quality, and its conclusions were not justified. Only one of the four reviewers felt that the procedures in the paper were adequately described.\nAs PNAS Reviewer 1 commented,\n\"The paper is based on...basic untested and fundamentally flawed assumptions about global climate sensitivity\"\nThese remaining flaws in LC11 included:\nAssuming that that correlations observed in the tropics reflect global climate feedbacks.\nFocusing on short-term local tropical changes which might not be representative of equilibrium climate sensitivity, because for example the albedo feedback from melting ice at the poles is obviously not reflected in the tropics.\nInadequately explaining methodology in the paper in sufficient detail to reproduce their analysis and results.\nFailing to explain the many contradictory results using the same or similar data (Trenberth, Chung, Murphy, and Dessler).\nTreating clouds as an internal initiator of climate change, as opposed to treating cloud changes solely as a climate feedback (as most climate scientists do) without any real justification for doing so.\nAs a result of these fundamental problems, PNAS rejected the paper, which Lindzen and Choi subsequently got published in a rather obscure Korean journal, the Asia-Pacific Journal of Atmospheric Science.\nWholly Debunked\nA full understanding of climate requires we take into account the full body of evidence. In the case of climate sensitivity and satellite data, it requires a global dataset, not just the tropics. Stepping back to take a broader view, a single paper must also be seen in the context of the full body of peer-reviewed research. A multitude of papers looking at different periods in Earth's history independently and empirically converge on a consistent answer - climate sensitivity is around 3°C implying net positive feedback.\nLast updated on 6 July 2012 by dana1981. View Archives"
  },
  {
   "title": "Ljungqvist broke the hockey stick",
   "paragraph": "How does Ljungqvist's reconstruction compare to others?\nLink to this page\nWhat the science says...\nLjungqvist's millennial temperature reconstruction was very similar to Moberg et al. (2005) and Mann et al. (2008). It also concludes that current northern hemisphere surface air temperatures are significantly higher than during the peak of the Medieval Warm Period (MWP). Further, arguing for a hot MWP is also arguing that climate sensitivity is not low - which undermines a critical argument for \"skeptics\".\nClimate Myth...\nLjungqvist broke the hockey stick\n\"[Ljungqvist 2010 shows that] there is nothing unusual, nothing unnatural or nothing unprecedented about the planet's current level of warmth, seeing it was just as warm as, or even warmer than, it has been recently during both the Roman and Medieval Warm Periods, when the atmosphere's CO2 concentration was more than 100 ppm less than it is today. And this latter observation, together with the realization that earth's climate naturally transits back and forth between cooler and warmer conditions on a millennial timescale, demonstrates that there is absolutely no need to associate the planet's current level of warmth with its current higher atmospheric CO2 concentration, in clear contradiction of the worn-out IPCC and climate-alarmist claim that the only way to explain earth's current warmth is to associate it with the greenhouse effect of CO2\" (NIPCC)\nFredrik Ljungqvist created a 2000-year temperature history of the extra-tropical portion of the Northern Hemisphere (30-90°N) based on 30 proxy records. Certain \"skeptics\" have argued that his reconstruction shows greater natural variability than previous reconstructions, and that it shows the peak of the Medieval Warm Period (MWP) hotter than today's surface air temperatures.\nLjungqvist Compared to other Reconstructions\nHowever, Ljungqvist's reconstruction is not substantially different from the many other millennial northern hemisphere temperature reconstructions, as the author himself states in his paper:\n“Our temperature reconstruction agrees well with the reconstructions by Moberg et al. (2005) and Mann et al. (2008) with regard to the amplitude of the variability as well as the timing of warm and cold periods, except for the period c. AD 300–800, despite significant differences in both data coverage and methodology.”\nIndeed by plotting Ljungqvist's data along with Moberg et al. (2005), Mann et al. (2008), and the surface temperature record, we can confirm that the three reconstructions are very similar (Figure 1).\nFigure 1: Moberg et al. 2005 NH (blue), Mann et al. 2008 EIV NH (red), and Ljungqvist 2010 NH (green). Courtesy of Robert Way and John Cook.\nMWP Peak vs. Current Temperature\nContrary to \"skeptic\" claims that his reconstruction shows the peak of the MWP as hotter than today's temperatures, Ljungqvist says the following when combining his proxy reconstruction with recent instrumental temperature data:\n“Since AD 1990, though, average temperatures in the extra-tropical Northern Hemisphere exceed those of any other warm decades the last two millennia, even the peak of the Medieval Warm Period”\nFigure 2: Ljungqvist (2010) 30-90°N decadal averages (black) vs. HadCRUT land-ocean 30-90°N decadal averages (red). Courtesy of Robert Way.\nWhat Reconstructions Tell Us\nThe NIPCC also claims that if the MWP was as hot as today (which it wasn't), that means that current global warming and climate change could be natural. It's true, hypothetically, the current warming could be natural, if there were a natural mechanism causing it. However, there is no such known mechanism. There is a measured energy imbalance caused by the increase in atmospheric greenhouse gases. We know that this energy must cause the planet to warm, and how much it warms depends on the climate sensitivity to the energy imbalance.\nIn fact, the hotter the MWP, the more sensitive the climate is to these energy imbalances. So arguing for a hot MWP is actually arguing that greenhouse gases must be causing significant global warming - the NIPCC has it exactly backwards.\nSummary\nDespite the different methodologies and data coverage used in Ljungqvist (2010), his reconstruction is consistent with previous peer-reviewed northern hemisphere temperature reconstructions, and like all previous peer-reviewed reconstructions, concludes that current temperatures are higher than the peak of the MWP. Claiming that the MWP was hotter than today is also counter-productive for \"skeptics\", because a hotter MWP means climate sensitivity is high.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 9 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Loehle and Scafetta find a 60 year cycle causing global warming",
   "paragraph": "How curve-fitting can ignore physics\nLink to this page\nWhat the science says...\nLoehle and Scafetta's paper is nothing more than a curve fitting exercise with no physical basis using an overly simplistic model.\nClimate Myth...\nLoehle and Scafetta find a 60 year cycle causing global warming\nAbout 60% of the warming observed from 1970 to 2000 was very likely caused by this natural 60-year climatic cycle during its warming phase...climate may remain approximately steady until 2030-2040, and may at most warm 0.5-1.0°C by 2100 at the estimated 0.66°C/century anthropogenic warming rate, which is about 3.5 times smaller than the average 2.3°C/century anthropogenic warming rate projected by the IPCC during the first decades of the 21st century (Craig Loehle)\nIn a paper published in The Bentham Open Atmospheric Science Journal, which has received a significant amount of blogosphere attention (for example, with posts published on the \"skeptic\" blogs of Judith Curry and Anthony Watts), two \"skeptics\", Loehle and Scafetta (L&S), performed the same sort of curve fitting exercise using a simple model of the climate.\nUsually in climate modeling, scientists will set the allowable range for each input parameter based on observational data. For example, the ocean mixed layer, in which the density is approximately the same as the ocean surface, is at most between 25 and 200 meters (generally more like 50-100 meters). Once they have established these ranges, climate scientists can run the climate models and compare their results to recent observational data (a.k.a. \"hindcasting\"), and tweak the models (within the physically allowable ranges) to match the observational data.\nThis is not how the \"skeptics\" do modeling. For example, Spencer has previously used a mixed ocean layer depth of 700 meters, because although this is physically unjustifiable, it allowed his model to fit the data with a low climate sensitivity. In this case, L&S use a model with a very simple formula:\nYou may be able to guess how their model will perform just by examining this formula. The first two terms are oscillations of 60 and 20 year periods, respectively. The third term will produce a linear warming trend, and the fourth is simply a constant. So this model will produce a linear warming trend with two natural oscillations superimposed on top of it. L&S tweaked the parameters to fit the temperature curve without doing any further testing, but we can test it for them by running their model backwards in time and comparing to reconstructed temperatures. Because of the linear term, you might expect the model to match the observations back to the Little Ice Age (LIA) and then rapidly diverge from observations. If so, you would essentially be right (Figure 1).\nFigure 1: The L&S Case 2 model projected backwards in time (red), compared to the Moberg et al. (2005) millennial northern hemisphere temperature reconstruction (blue) and the Loehle (2008) millennial global temperature reconstruction (green).\nTalk about a divergence problem!\nLoehle is generally most well-known for his millennial global temperature reconstruction using non-tree ring proxies. Although the paper was published in Energy & Environment, which is not considered a peer-reviewed journal, considering that Loehle was the lead author on L&S, we felt it would be worthwhile to see if Loehle's model matches his own temperature reconstruction.\nThe L&S model matches the reconstructed temperature trends reasonably well back to the 17th century, but fails miserably to match prior temperatures. Moreover, the 60 year cycle in the L&S model matches up extremely poorly with the Moberg reconstruction (Figure 2), and even with Loehle's own reconstruction (Figure 3).\nFigure 2: The L&S Case 2 model projected backwards to the period 1500 to 1900, compared to the Moberg et al. (2005) milennial northern hemisphere temperature reconstruction.\nFigure 3: The L&S Case 2 model projected backwards to the period 1500 to 1900, compared to the Loehle (2008) milennial global temperature reconstruction.\nSeveral times between 1500 and 1900, the L&S model is anti-phase with both reconstructions, with the peak of the 60 year cycle matching a trough in temperature. Thus we see that although L&S have gotten lucky and matched the temperature trend a few centuries into the past, the 60 year cycle which is the basis of their paper is nowhere to be found in the temperature data.\nWhy Does the L&S Model Fail the Hindcasting Test?\nSo why the hindcasting failure? The basic reason is simple - this is not a physical model. L&S describe their methodology in the paper:\n\"The model was fit by nonlinear least squares estimation using Mathematica functions, with phase and amplitude free but period fixed as above.\"\nIn other words, they let the parameters vary freely without any physical constraints, and fit the curve as best they could. They suggest the first two terms represent astronomical cycles:\n\"The solar system oscillates with a 60-year cycle due to the Jupiter/Saturn three-synodic cycle and to a Jupiter/Saturn beat tidal cycle\"\nHow exactly these Jupiter and Saturn orbital cycles are supposed to impact the Earth's surface temperature is left unexplained, and thus there are no realistic physical constraints on their parameters 'A' and 'B', which correspond to the amplitude of the cyclical effects on global surface temperature. Is it realistic for a Jupiter/Saturn cycle to have an effect on the Earth's average temperature with an peak-to-trough amplitude of 0.24°C? It seems exceptionally unlikely, but this question is left unanswered, and thus the model has no physical basis (more on these astronomical cycles below).\nLinear Trends and Bad Assumptions\nBut the real L&S model failure is in the linear trend, since these mysterious astronomical cycles are simply oscillations on top of that trend. Actually, in the paper, L&S apply two different cases to their model. In Case 1, they simply apply it to the observational data from 1850 to 2010. L&S are able to achieve a decent fit with Case 1, but conclude that the residual (subtracting model output from observations) is non-random, and thus they're missing some key components.\nThey then try \"Case 2\", in which they fit their model from 1850 to 1950 (but apply it through 2010), and then apply an additional linear trend from 1950 to 2010, which they claim is the man-made global warming signal. They assume that humans have zero influence on the global surface temperature prior to about 1950, which is a poor assumption.\nA second problem is that L&S do not evaluate whether the remaining residual after they apply the second (anthropogenic) linear trend is random - it appears not to be (see their Figure 2b, the lower graph in our Figure 4):\nFigure 4: L&S Case 2 Model vs. HadCRUT global surface temperature (upper frame), residual plus a linear trend fitting data from 1950 to 2010, representing the man-made warming effect (lower frame).\nQuite possibly the biggest flaw in the paper (and there are many to choose from), is in their \"natural\" linear trend, which L&S describe thusly:\n\"The linear trend would approximately extrapolate a natural warming trend due to solar and volcano effects that is known to have occurred since the Little Ice Age\"\nOther than the supposed man-made linear trend L&S apply in Case 2 starting in 1950, and the two cyclical factors which cause zero long-term trend, this \"natural\" linear trend is the only factor included in the L&S model. In other words, it must account for every non-cyclical \"natural\" effect (forcing) on the average global temperature.\nSo why do L&S assume natural forcings have had a constant warming effect over the past 160 years? They provide no justification for this assumption, and it is clearly faulty. For example, solar activity increased significantly in the early 20th century, but has remained essentially flat (even declining slightly) since 1950. Meehl et al. (2004) provide a graph of the natural forcings over the period in question (Figure 5).\nFigure 5: Natural radiative forcings from Meehl et al. (2004)\nQuite clearly the natural radiative forcing does not increase at a constant linear rate from 1850 to 2010, and thus the assumption which is the backbone of the L&S model is faulty. By overestimating the \"natural\" contribution to the 1950-2010 warming, they necessarily underestimate the human contribution, and thus underestimate the climate sensitivity to increasing CO2 (more on this later).\nSince the L&S model is so oversimplified, failing to account for changes in the natural forcings, they fail to accurately model past climate changes. For the same reason, they will fail to accurately model future climate changes as well.\nAstronomical Cycles\nAnother fundamental problem with the L&S model is the assumption of significant effects of 20 and 60 year astronomical cycles on the Earth's temperature. Aside from failing to provide a physical mechanism through which Jupiter and Saturn could impact the Earth's temperature, as Riccardo has previously noted, the mere existence of a 60 year cycle in the Earth's surface temperature depends on the choice of trend to begin with. By fitting certain polynomials to the global temperature data, we can find a residual with a 60 year cycle, but only with certain curve fitting choices. Those curve fitting choices must first be justified.\nHowever, instead of first subtracting off the underlying trend and seeing what's left, L&S start their exercise under the assumption that the 20 and 60 year cycles are present, and then fit the temperature data as best they can with those cycles (with no physical constraints). After they conduct this curve fitting, they see what's left and attribute the remainder to human effects. This is not a scientific approach, it's simply curve fitting (a.k.a. \"graph cooking\" and \"mathturbation\") at its worst.\nThere may be a 60 year cycle in the global climate, perhaps related to an oceanic cycle like the Pacific Decadal Oscillation. However, in order to explain the warming of both the oceans and atmosphere, there must be an external forcing at work, which may be why L&S invoke these mysterious astronomical cycles. But the fact remains that they have failed to provide a physical mechanism through which these cycles could impact the Earth's climate. And as we saw in Figures 2 and 3, their assumed 60 year cycle does not show up in millennial temperature reconstructions - not even in Loehle's own previous work! This is a rather glaring contradiction.\nAerosols\nThe effects of aerosols on the climate are a problem for the L&S model, as the authors almost admit in the paper:\n\"Residual analysis does not provide any evidence for a substantial cooling effect due to sulfate aerosols from 1940 to 1970....sulfate aerosols produced by volcanoes or industrial emissions no doubt have a cooling effect\"\nWe have recently discussed several papers which have found substantial global dimming as a result of increased human aerosol emissions from 1950 to 1980 and 2000 to 2010. As L&S admit, this global dimming due to aerosols \"no doubt [has] a cooling effect\", yet it doesn't show up in their model. This is further evidence of the unconstrained overfitting of their 20 and 60 year cycles.\nFuture Warming Prediction Flaws\nAs noted above, since the L&S model fails the hindcasting test, there is no reason to believe it can accurately predict future global temperature changes. However, that's not the only problem with their prediction. The authors also assume that the linear man-made warming trend which they've thrown into their model will continue at the same linear rate into the future. We have previously addressed this misconception in Monckton Myth #3: Linear Warming. The only way we will prevent man-made warming from accelerating is if we take significant action to reduce our greenhouse gas emissions, which \"skeptics\" generally oppose. L&S don't even provide the emissions scenario for their future warming prediction - they simply assume that the linear man-made warming trend will continue without any justification. Once again, this is a glaring lack of physics in their paper.\nClimate Sensitivity Estimate Flaws\nL&S estimate the equilibrium climate sensitivity to doubled CO2 from their model at \"about 1-1.5°C or less\". However, there are two major problems with this estimate:\nAs noted above, L&S have overestimated the 'natural' warming effects from 1950 to 2010, and as a result, have underestimated the man-made warming effects over this period. Their climate sensitivity estimate is based entirely on this underestimated man-made warming trend, and thus is also too low.\nL&S are estimating an equilibrium climate sensitivity for a climate which is not currently in equilibrium. What they are actually trying to estimate here is the transient climate sensitivity, but they erroneously compare it to the IPCC equilibrium sensitivity range.\nThe Bentham Open Journals\nAt this point, you may be wondering how such a tremendously flawed paper made it through the peer review process. The paper was published in the Bentham Open Atmospheric Science journal. The Bentham Open journals appear to have a tendency to publish 'controversial' papers which most peer-reviewed journals would not publish. There has also been a case of another Bentham Open journal accepting a nonsensical hoax submission paper, which calls the peer-review standards of these journals into question. Additionally, the ISI Master Journal list does not recognize the Bentham Open journals as legitimately peer-reviewed.\nThe paper should be evaluated on its own merits, of course (which are basically non-existent), but the publication of this paper probably had a lot to do with the low standards and quality of peer-review of the journal in which it was published.\nNothing to See Here\nUnfortunately, there is little we can learn from the L&S paper. Fitting a curve with a simple model using physically unconstrained parameters is simply not a scientific process, as illustrated in its failure when put to the hindcasting test. You could just as easily conduct this sort of curve fitting exercise using the number of pirates, canoes, and pantaloons in southern Spain. L&S are guilty of a major error in the abstract of their paper:\n\"About 60% of the warming observed from 1970 to 2000 was very likely caused by the above natural 60-year climatic cycle during its warming phase.\"\nCorrelation is not causation; all L&S have demonstrated in their unphysical curve fitting exercise is a correlation between their cycles and global temperature. If I find a correlation between Spanish pirates and pantaloons and global temperature, that doesn't mean these variables are causing global warming. If you want to draw a conclusion like L&S have, you need to identify the physical mechanism through which your variables are causing a global temperature change, identify a realistic physical range that this effect can have on the temperature, and then run your model.\nWithout a realistic physical basis, like Spencer before them, all L&S are doing is playing pointless curve fitting games, and using their results to draw unsubstantiated conclusions.\nLast updated on 1 August 2011 by dana1981."
  },
  {
   "title": "Mars is warming",
   "paragraph": "Global warming on Mars, ice caps melting\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nMars is not warming globally.\nClimate Myth...\nMars is warming\n\"Some people think that our planet is suffering from a fever. Now scientists are telling us that Mars is experiencing its own planetary warming: Martian warming. It seems scientists have noticed recently that quite a few planets in our solar system seem to be heating up a bit, including Pluto.\nNASA says the Martian South Pole’s “ice cap” has been shrinking for three summers in a row. Maybe Mars got its fever from earth. If so, I guess Jupiter’s caught the same cold, because it’s warming up too, like Pluto.\" (Fred Thompson).\nIt is hard to understand how anyone could claim global warming is happening on Mars when we can’t even agree what’s happening on the planet we live on. Yet they do, and the alleged reasoning is this; if other planets are warming up, then there is some solar system-wide phenomena at work – and therefore that it isn’t human activity causing climate change here on Earth.\nThe broadest counter argument depends on a simple premise: we know so little about Mars that it's impossible to say what trends in climate the planet is experiencing, or why changes occur. We do have information from various orbiting missions and the few lander explorations to date, yet even this small amount of data has been misunderstood, in terms of causal complexity and significance.\nThere are a few basic points about the climate on Mars that are worth reviewing:\nPlanets do not orbit the sun in perfect circles, sometimes they are slightly closer to the sun, sometimes further away. This is called orbital eccentricity and it contributes far greater changes to Martian climate than to that of the Earth because variations in Mars' orbit are five times greater than the Earth.\nMars has no oceans and only a very thin atmosphere, which means there is very little thermal inertia – the climate is much more susceptible to change caused by external influences.\nThe whole planet is subject to massive dust storms, and these have many causal effects on the planet’s climate, very little of which we understand yet.\nWe have virtually no historical data about the climate of Mars prior to the 1970s, except for drawings (and latterly, photographs) that reveal changes in gross surface features (i.e. features that can be seen from Earth through telescopes). It is not possible to tell if current observations reveal frequent or infrequent events, trends or outliers.\nA picture is worth a thousand words, but only if you understand what it is saying\nThe global warming argument was strongly influenced by a paper written by a team led by NASA scientist Lori Fenton, who observed that changes in albedo – the property of light surfaces to reflect sunlight e.g. ice and snow – were shown when comparing 1977pictures of the Martian surface taken by the Viking spacecraft, to a 1999 image compiled by the Mars Global Surveyor. The pictures revealed that in 1977 the surface was brighter than in 1999, and from this Fenton used a general circulation model to suggest that between 1977 and 1999 the planet had experienced a warming trend of 0.65 degrees C. Fenton attributed the warming to surface dust causing a change in the planet's albedo.\nUnfortunately, Fenton’s conclusions were undermined by the failure to distinguish between climate (trends) and weather (single events). Taking two end points – pictures from 1977 and 1999 – did not reveal any kind of trend, merely the weather on two specific Martian days. Without the intervening data – which was not available – it is impossible to say whether there was a trend in albedo reduction, or what part the prodigious dust storms played in the intervening period between the first and second photographs. Indeed, when you look at all the available data – sparse though it is – there is no discernable long term trend in albedo.\nAt this time, there is little empirical evidence that Mars is warming. Mars' climate is primarily driven by dust and albedo, not solar variations, and we know the sun is not heating up all the planets in our solar system because we can accurately measure the sun’s output here on Earth.\nBasic rebuttal written by GPWayne\nLast updated on 1 August 2013 by gpwayne. View Archives"
  },
  {
   "title": "Mauna Loa is a volcano",
   "paragraph": "Mauna Loa and global network of CO2 measurements\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nThe measurements of the amount of CO2 made at the Mauna Loa Observatory are accurate and uncontaminated by any emissions from the volcano. The measurements show a steadily increasing tend of CO2 concentrations in the air, a trend that is confirmed by many measurements made elsewhere.\nClimate Myth...\nMauna Loa is a volcano\n'Mauna Loa has been producing a readout which supports Manning's predetermined goal by showing steady growth in atmospheric CO2 concentrations since 1959. Just thirty miles from the observatory, Kilauea's Pu`u O`o vent sends 3.3 million metric tons of CO2 into the atmosphere every year. Pu`u O`o sends into \"the undisturbed air\" near \"the remote location\" the equivalent to yearly CO2 production from an average city of 660,000 people.' (Andrew Walden)\nThe observatory near the summit of the Mauna Loa volcano in Hawaii has been recording the amount of carbon dioxide in the air since 1958. This is the longest continuous record of direct measurements of CO2 and it shows a steadily increasing trend from year to year; combined with a saw-tooth effect that is caused by changes in the rate of plant growth through the seasons. This curve is commonly known as the Keeling Curve, named after Charles Keeling, the American scientist who started the project.\nWhy Mauna Loa? Early attempts to measure CO2 in the USA and Scandinavia found that the readings varied a lot due to the influence of growing plants and the exhaust from motors. Mauna Loa is ideal because it is so remote from big population centres. Also, on tropical islands at night, the prevailing winds blow from the land out to sea, which effect brings clean, well-mixed Central Pacific air from high in the atmosphere to the observatory. This removes any interference coming from the vegetation lower down on the island.\nBut how about gas from the volcano? It is true that volcanoes blow out CO2 from time to time and that this can interfere with the readings. Most of the time, though, the prevailing winds blow the volcanic gasses away from the observatory. But when the winds do sometimes blow from active vents towards the observatory, the influence from the volcano is obvious on the normally consistent records and any dubious readings can be easily spotted and edited out (Ryan, 1995).\n(Illustration updated on 9/5/2015)\nImportantly, Mauna Loa is not the only atmospheric measuring station in the world. As the graph from NOAA shows, other stations show the same year-after-year increasing trend. The seasonal saw-tooth varies from place to place, of course, but the background trend remains steadily upwards. The Keeling Curve is one of the best-defined results in climatology and there really are no valid scientific reasons for doubting it.\nFurther reading: Spencer Weart's The Discovery of Global Warming describes Charles Keeling's research efforts in more detail. Weart also has a separate article on Keeling's struggle to fund his research.\nLast updated on 6 September 2015 by Andy Skuce. View Archives"
  },
  {
   "title": "Medieval Warm Period was warmer",
   "paragraph": "How does the Medieval Warm Period compare to current global temperatures?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nWhile the Medieval Warm Period saw unusually warm temperatures in some regions, globally the planet was cooler than current conditions.\nClimate Myth...\nMedieval Warm Period was warmer\n\"For now, though, it is enough just to see the Medieval WARM Period shown to be global, and warmer than today.\" (Musings from the Chiefio)\nOne of the most often cited arguments of those skeptical of global warming is that the Medieval Warm Period (800-1400 AD) was as warm as or warmer than today. Using this as proof to say that we cannot be causing current warming is a faulty notion based upon rhetoric rather than science. So what are the holes in this line of thinking?\nFirstly, evidence suggests that the Medieval Warm Period may have been warmer than today in many parts of the globe such as in the North Atlantic. This warming thereby allowed Vikings to travel further north than had been previously possible because of reductions in sea ice and land ice in the Arctic. However, evidence also suggests that some places were very much cooler than today including the tropical pacific. All in all, when the warm places are averaged out with the cool places, it becomes clear that the overall warmth was likely similar to early to mid 20th century warming.\nSince that early century warming, temperatures have risen well-beyond those achieved during the Medieval Warm Period across most of the globe. The National Academy of Sciences Report on Climate Reconstructions in 2006 found it plausible that current temperatures are hotter than during the Medieval Warm Period. Further evidence obtained since 2006 suggests that even in the Northern Hemisphere where the Medieval Warm Period was the most visible, temperatures are now beyond those experienced during Medieval times (Figure 1). This was also confirmed by a major paper from 78 scientists representing 60 scientific institutions around the world in 2013.\nSecondly, the Medieval Warm Period has known causes which explain both the scale of the warmth and the pattern. It has now become clear to scientists that the Medieval Warm Period occurred during a time which had higher than average solar radiation and less volcanic activity (both resulting in warming). New evidence is also suggesting that changes in ocean circulation patterns played a very important role in bringing warmer seawater into the North Atlantic. This explains much of the extraordinary warmth in that region. These causes of warming contrast significantly with today's warming, which we know cannot be caused by the same mechanisms.\nOverall, our conclusions are:\na) Globally temperatures are warmer than they have been during the last 2,000 years, and\nb) the causes of Medieval warming are not the same as those causing late 20th century warming.\nFigure 1: Northern Hemisphere Temperature Reconstruction by Moberg et al. (2005) shown in blue, Instrumental Temperatures from NASA shown in Red.\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Models are unreliable",
   "paragraph": "How reliable are climate models?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nModels successfully reproduce temperatures since 1900 globally, by land, in the air and the ocean.\nClimate Myth...\nModels are unreliable\n\"[Models] are full of fudge factors that are fitted to the existing climate, so the models more or less agree with the observed data. But there is no reason to believe that the same fudge factors would give the right behaviour in a world with different chemistry, for example in a world with increased CO2 in the atmosphere.\" (Freeman Dyson)\nClimate models are mathematical representations of the interactions between the atmosphere, oceans, land surface, ice – and the sun. This is clearly a very complex task, so models are built to estimate trends rather than events. For example, a climate model can tell you it will be cold in winter, but it can’t tell you what the temperature will be on a specific day – that’s weather forecasting. Climate trends are weather, averaged out over time - usually 30 years. Trends are important because they eliminate - or \"smooth out\" - single events that may be extreme, but quite rare.\nClimate models have to be tested to find out if they work. We can’t wait for 30 years to see if a model is any good or not; models are tested against the past, against what we know happened. If a model can correctly predict trends from a starting point somewhere in the past, we could expect it to predict with reasonable certainty what might happen in the future.\nSo all models are first tested in a process called Hindcasting. The models used to predict future global warming can accurately map past climate changes. If they get the past right, there is no reason to think their predictions would be wrong. Testing models against the existing instrumental record suggested CO2 must cause global warming, because the models could not simulate what had already happened unless the extra CO2 was added to the model. All other known forcings are adequate in explaining temperature variations prior to the rise in temperature over the last thirty years, while none of them are capable of explaining the rise in the past thirty years. CO2 does explain that rise, and explains it completely without any need for additional, as yet unknown forcings.\nWhere models have been running for sufficient time, they have also been proved to make accurate predictions. For example, the eruption of Mt. Pinatubo allowed modellers to test the accuracy of models by feeding in the data about the eruption. The models successfully predicted the climatic response after the eruption. Models also correctly predicted other effects subsequently confirmed by observation, including greater warming in the Arctic and over land, greater warming at night, and stratospheric cooling.\nThe climate models, far from being melodramatic, may be conservative in the predictions they produce. For example, here’s a graph of sea level rise:\nObserved sea level rise since 1970 from tide gauge data (red) and satellite measurements (blue) compared to model projections for 1990-2010 from the IPCC Third Assessment Report (grey band). (Source: The Copenhagen Diagnosis, 2009)\nHere, the models have understated the problem. In reality, observed sea level is tracking at the upper range of the model projections. There are other examples of models being too conservative, rather than alarmist as some portray them. All models have limits - uncertainties - for they are modelling complex systems. However, all models improve over time, and with increasing sources of real-world information such as satellites, the output of climate models can be constantly refined to increase their power and usefulness.\nClimate models have already predicted many of the phenomena for which we now have empirical evidence. Climate models form a reliable guide to potential climate change.\nMainstream climate models have also accurately projected global surface temperature changes. Climate contrarians have not.\nVarious global temperature projections by mainstream climate scientists and models, and by climate contrarians, compared to observations by NASA GISS. Created by Dana Nuccitelli.\nA 2019 study led by Zeke Hausfather evaluated 17 global surface temperature projections from climate models in studies published between 1970 and 2007. The authors found \"14 out of the 17 model projections indistinguishable from what actually occurred.\"\nThere's one chart often used to argue to the contrary, but it's got some serious problems, and ignores most of the data.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nAdditional video from the MOOC\nDana Nuccitelli: Principles that models are built on.\nLast updated on 9 September 2019 by pattimer. View Archives"
  },
  {
   "title": "Most of the last 10,000 years were warmer",
   "paragraph": "Confusing Greenland warming vs global warming\nLink to this page\nWhat the science says...\nThis argument uses temperatures from the top of the Greenland ice sheet. This data ends in 1855, long before modern global warming began. It also reflects regional Greenland warming, not global warming.\nClimate Myth...\nMost of the last 10,000 years were warmer\nEven if the warming were as big as the IPCC imagines, it would not be as dangerous as Mr. Brown suggests. After all, recent research suggests that some 9,100 of the past 10,500 years were warmer than the present by up to 3 Celsius degrees: yet here we all are. (Christopher Monckton)\nThis argument is based on the work of Don Easterbrook who relies on temperatures at the top of the Greenland ice sheet as a proxy for global temperatures. That’s a fatal flaw, before we even begin to examine the use of the ice core data. A single regional record cannot stand in for the global record — local variability will be higher than the global, plus we have evidence that Antarctic temperatures swing in the opposite direction to Arctic changes. Richard Alley discussed that in some detail at Dot Earth last year, and it’s well worth reading his comments. Easterbrook, however, is content to ignore someone who has worked in this field, and relies entirely on Greenland data to make his case.\nMost of the past 10,000 [years] have been warmer than the present. Figure 4 shows temperatures from the GISP2 Greenland ice core. With the exception of a brief warm period about 8,200 years ago, the entire period from 1,500 to 10,500 years ago was significantly warmer than present.\nThis is Easterbrook’s Fig 4:\nIt’s a graph he’s used before, in various forms, almost certainly copied and altered from the original (click image below to see source: the NOAA web page for Richard Alley’s 2000 paper The Younger Dryas cold interval as viewed from central Greenland, though DE credits it as “Modified from Cuffy and Clow, 1997″, misspelling Kurt Cuffey’s name in the process:\nEasterbrook continues:\nAnother graph of temperatures from the Greenland ice core for the past 10,000 years is shown in Figure 5. It shows essentially the same temperatures as Cuffy and Clow (1997) but with somewhat greater detail. What both of these temperature curves show is that virtually all of the past 10,000 years has been warmer than the present.\nThis is his Fig 5:\nEasterbrook plots the temperature data from the GISP2 core, as archived here. Easterbrook defines “present” as the year 2000. However, the GISP2 “present” follows a common paleoclimate convention and is actually 1950. The first data point in the file is at 95 years BP. This would make 95 years BP 1855 — a full 155 years ago, long before any other global temperature record shows any modern warming. In order to make absolutely sure of my dates, I emailed Richard Alley, and he confirmed that the GISP2 “present” is 1950, and that the most recent temperature in the GISP2 series is therefore 1855.\nThis is Easterbrook’s main sleight of hand. He wants to present a regional proxy for temperature from 155 years ago as somehow indicative of present global temperatures. The depths of his misunderstanding are made clear in a response he gave to a request from the German EIKE forum to clarify why he was representing 1905 (wrongly, in two senses) as the present. Here’s what he had to say:\nThe contention that the ice core only reaches 1905 is a complete lie (not unusual for AGW people). The top of the core is accurately dated by annual dust layers at 1987. There has been no significant warming from 1987 to the present, so the top of the core is representative of the present day climate in Greenland.\nUnfortunately for Don, the first data point in the temperature series he’s relying on is not from the “top of the core”, it’s from layers dated to 1855. The reason is straightforward enough — it takes decades for snow to consolidate into ice.\nAnd so to an interesting question. What has happened to temperatures at the top of Greenland ice sheet since 1855? Jason Box is one of the most prominent scientists working on Greenland and he has a recent paper reconstructing Greenland temperatures for the period 1840-2007 (Box, Jason E., Lei Yang, David H. Bromwich, Le-Sheng Bai, 2009: Greenland Ice Sheet Surface Air Temperature Variability: 1840–2007. J. Climate, 22, 4029–4049. doi: 10.1175/2009JCLI2816.1). He was kind enough to supply me with a temperature reconstruction for the GRIP drilling site — 28 km from GISP2. This is what the annual average temperature record looks like (click for bigger version):\nI’ve added lines showing the average temperatures for the 1850s (blue) and the last 10 years (red), and the difference between those is a warming of 1.44ºC. I’ve also added the two most recent GISP2 temperature data points (for 1847 and 1855, red crosses). It’s obvious that the GRIP site is warmer than GISP2 (at Summit Camp). The difference is estimated to be 0.9ºC on the annual average (Box, pers comm).\nLet’s have ago at reconstructing Easterbrook’s Fig 5, covering the last 10,000 years of GISP2 data. It looks like this (click for bigger version):\nThe GISP2 series — the red line — appears to be identical to Easterbrook’s version. The bottom black line shows his 1855 “present”, and it intersects the red line in the same places as his chart. I’ve added a grey line based on the +1.44ºC quantum calculated from the GRIP temperature data, and two blue crosses, which show the GISP2 site temperatures inferred from adjusted GRIP data for 1855 and 2009.\nTwo things are immediately apparent. If we make allowance for local warming over the last 155 years, Easterbrook’s claim that “most of the past 10,000 [years] have been warmer than the present” is not true for central Greenland, let alone the global record. It’s also clear that there is a mismatch between the temperature reconstructions and the ice core record. The two blue crosses on the chart show the GISP site temperatures (adjusted from GRIP data) for 1855 and 2009. It’s clear there is a calibration issue between the long term proxy (based on ∂18O measurement) and recent direct measurement of temperatures on the Greenland ice sheet. How that might be resolved is an interesting question, but not directly relevant to the point at issue — which is what Don Easterbrook is trying to show. Here’s his conclusion:\nSo where do the 1934/1998/2010 warm years rank in the long-term list of warm years? Of the past 10,500 years, 9,100 were warmer than 1934/1998/2010. Thus, regardless of which year ( 1934, 1998, or 2010) turns out to be the warmest of the past century, that year will rank number 9,099 in the long-term list. The climate has been warming slowly since the Little Ice Age (Fig. 5), but it has quite a ways to go yet before reaching the temperature levels that persisted for nearly all of the past 10,500 years. It’s really much to do about nothing.\n1855 — Easterbrook’s “present” — was not warmer than 1934, 1998 or 2010 in Greenland, let alone around the world. His claim that 9,100 out of the last 10,500 years were warmer than recent peak years is false, based on a misunderstanding or misrepresentation of data.\nThe last word goes to Richard Alley, who points out that however interesting the study of past climate may be, it doesn’t help us where we’re heading:\n\"Whether temperatures have been warmer or colder in the past is largely irrelevant to the impacts of the ongoing warming. If you don’t care about humans and the other species here, global warming may not be all that important; nature has caused warmer and colder times in the past, and life survived. But, those warmer and colder times did not come when there were almost seven billion people living as we do. The best science says that if our warming becomes large, its influences on us will be primarily negative, and the temperature of the Holocene or the Cretaceous has no bearing on that. Furthermore, the existence of warmer and colder times in the past does not remove our fingerprints from the current warming, any more than the existence of natural fires would remove an arsonist’s fingerprints from a can of flammable liquid. If anything, nature has been pushing to cool the climate over the last few decades, but warming has occurred.\nSee also: MT at Only In It For The Gold. My thanks to Richard Alley and Jason Box for their rapid response to my questions.\nNOTE: This rebuttal is an edited version of a blog post first published by Gareth Renowden at Hot Topic.\nIntermediate rebuttal written by Gareth\nUpdate August 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 4 August 2015 by MichaelK. View Archives"
  },
  {
   "title": "Murry Salby finds CO2 rise is natural",
   "paragraph": "The lines of evidence that humans are raising CO2 levels\nLink to this page\nWhat the science says...\nMultiple lines of evidence make it very clear that the rise in atmospheric CO2 is due to human emissions.\nClimate Myth...\nMurry Salby finds CO2 rise is natural\n\"Salby’s argument is that the usual evidence given for the rise in CO2 being man-made is mistaken. It’s usually taken to be the fact that as carbon dioxide concentrations in the atmosphere increase, the 1 per cent of CO2 that’s the heavier carbon isotope ratio c13 declines in proportion. Plants, which produced our coal and oil, prefer the lighter c12 isotope. Hence, it must be our gasses that caused this relative decline. But that conclusion holds true only if there are no other sources of c12 increases which are not human caused. Salby says there are - the huge increases in carbon dioxide concentrations caused by such things as spells of warming and El Ninos, which cause concentration levels to increase independently of human emissions. He suggests that its warmth which tends to produce more CO2, rather than vice versa - which, incidentally is the story of the past recoveries from ice ages.\" (Andrew Bolt)\nEvery year humans release about 30 billion tons of carbon dioxide (CO2) into the atmosphere through the burning of fossil fuels, such as coal and oil. This is causing the Earth to warm by disrupting the biological (fast) carbon cycle, and is therefore increasing the Greenhouse Effect. Although there are large annual fluctuations in carbon dioxide, as it is exchanged back-and-forth between the atmosphere, oceans, soils, and forests, just under half of human emissions (the airborne fraction) remain in the air because the oceans, soils and forests are unable to absorb all of it. As a result, carbon dioxide has been steadily accumulating in the atmosphere.\nFigure 1 - Fraction of the total human emissions (fossil fuel burning & land use change) that remain in the: a) atmosphere, b) land vegetation and soil, c) the oceans. From Canadell (2007)\nMurry Salby, a professor at Macquarie University in Sydney, Australia, has an upcoming paper that attempts to pin the current rise in carbon dioxide on rising temperatures. Having listened to a podcast of a talk Salby gave at the Sydney Institute earlier this week, he demonstrates a remarkably poor understanding of the carbon cycle, and his hypothesis seems to stem from this fundamental misunderstanding.\nSalby's carbon cycle confusion\nProfessor Salby refers to a number of graphs in his talk, but I have been unable to track down copies of these, therefore we'll have to rely on what I'm able to glean from the podcast, and given it's length, I'll only address some of the more obvious mistakes. At the beginning of the talk Salby states:\n\"current CO2 values are 380pmmv\"(parts per million by volume)\nNot an encouraging start that he cites the atmospheric CO2 concentration as it was in 2005, rather than the 393 parts per million by volume (ppmv) it currently is in 2011. Not a fatal flaw of course, but not encouraging either.\n\"Net annual emission has an average increase of about 1.5ppmv per year. We're on the right planet. That's the annual average increase you just saw. But it varies between years, dramatically by over 100%. From nearly zero in some years to 3ppmv in others. Net global emission of CO2 changes independently of of the human contribution\"\nAt this point the accentuation and drama in Salby's voice make it sound as though he has stumbled onto something momentous, something no one else has noticed before. On the face of it, it seems preposterous that the army of scientists that have worked on carbon cycling over the years could have missed something so glaringly obvious. No, of course they haven't.\nAs discussed in the first paragraph of this post (and evident in Figure 1), the natural flux of CO2 in and out of natural systems varies from year-to-year. This flux is 20-30 times larger than the annual contribution by humans, but this balances out in the long-term. This variability is driven largely by El Nino and La Nina in the tropical Pacific, which shifts rainfall patterns over much of the world and is associated with warming and cooling of equatorial waters in the Pacific. The change in seawater temperature, and episodic upwelling of carbon-rich deep water, significantly affects the uptake and outgassing of CO2 from the oceans, and of course rainfall variation greatly affects plant growth.\nThe upshot is that land vegetation takes up more CO2 during La Nina, and expels more CO2 during El Nino. In the ocean, the opposite trend occurs - El Nino leads to more CO2 absorption, and La Nina is when the oceans give up more CO2 (Figure 2).\nFigure 2 - (a) time trend in the exchange of CO2 by land-based vegetation (& soil microbes) with the atmosphere. (b) same - but for exchange of CO2 by ocean with atmosphere. Red indicates El Nino and blue La Nina phase. See Keeling (1995).\nThere is simply no reason why the annual fluctuation should match the human contribution. At least Salby doesn't explain why he expects this to be the case.\nHaving now convinced himself that short-term net CO2 has nothing to do with the human contribution, Salby therefore deduces long-term net CO2 must also be unrelated to human emissions. He goes on to derive a formula for CO2 rise associated with temperature. Salby claims a good match back to 1960 but therefafter it deviates from actual CO2 measurements by 10ppmv. By 1880, prior to atmospheric CO2 sampling, he estimates atmospheric CO2 at 275ppmv with a whopping uncertainty of 220 to 330ppmv!\nIn order to explain the deviation between the surface temperature record and his calculated atmospheric CO2 level, Salby blames the surface temperature record as being unreliable. As for his calculated trend disagreeing with the ice core record for the year 1880 (i.e the CO2 in air, from that period, trapped in ice cores) he 'disses' the ice core record claiming it to be only a 'proxy'. Which is news, I'm sure, to respected ice core experts like Dr Richard Alley.\nYou will note that every time the data disagrees with Salby's 'model', he trusts his 'model' over the data. Which contravenes the 'skeptic lore' that models are worthless and must be bashed, and only data should be trusted.\nQ&A time - try not to shoot yourself in the foot!\nThe question & answer session at the end of Salby's talk throws up a few more comments that just reinforce that he has strayed into a field of science which he just simply doesn't understand. Witness:\n\"I think it's a pitfall that people look at the ice proxy of CO2 and take it literally. It's not atmospheric CO2, and I don't believe it's CO2 that was even in the atmosphere when that piece of snow was layed down\"\nThis is nonsense. Perhaps Professor Salby should have acquainted himself with glaciology research before making such comments, because CO2 from ancient air trapped in the ice cores is precisely what is measured, albeit with some uncertainty in dating some sections.\n\"CO2 after the turn of the (21st) century continued to increase, in fact if anything slightly faster, but global temperature didn't. If anything it decreased in the first decade of the 21st century. Now I'm confident the IPCC (Intergovernmental Panel on Climate Change) will come up with an explanation, in fact they've come up with several\"\nIt's here we need to back the truck up a bit. Salby's entire premise is that CO2 in the air directly dependent upon temperature - increase temperature and you increase CO2. Yet here he argues that CO2 can increase without an accompanying increase in temperature. Which contradicts his 'model'. By this time Salby is too focused on 'dissing' the IPCC to notice his own incoherency, and none of the audience picks up on this either.\nNote that SkS recently discussed the 'noughties slow-down' in global temperature here and here.\nIf the curve fits\nSeasoned readers will notice similarities between this Salby claim and a Lon Hocker rebuttal here at SkS last year. But the whole premise seems to follow along the lines of other recent flawed works tendered by Roy Spencer and Craig Loehle & Nicola Scafetta. That is: find some tenuous statistical relationship between two sets of data, and use these to assert the mainstream scientific establishment is wrong. The fact that there is no physical basis for the statistical relationship, or it doesn't fit within the well-established scientific framework, or is contrary to numerous other sets of data, never seems to warrant attention by \"skeptic\" scientists. It should, because of the implications one can draw.\nSo what does this work by Salby imply, if it were true? From what I can gather from Salby's podcast, a 0.8°C change in average surface temperature is supposed to lead to about 120ppmv change in CO2. Therefore we can work backward in time to estimate what he reckons atmospheric CO2 would be at the time of the last Ice Age (glacial maximum), a time when global temperatures were about 4-6°C cooler than now . Today atmospheric CO2 is about 393ppm, so with 4°C cooling you already have a negative value for CO2 when we re-trace our steps back to the last ice age. Therefore all plant-based life on Earth must have died (and all the animals that depended on them) according to Professor Salby. And the Earth froze solid too.\nFigure 3 - the last Ice Age according to Murry Salby? Fictional image from celestiamotherlode.net\nScience - a description of reality, but YMMV\nWithout viewing Salby's calculations on the temperature/net global CO2 relationship, it's not possible to provide the 'killer blow' to his assertions; however, I don't believe that's necessary, considering the many flaws in Salby's work and fundamental reasoning.\nThe gradual increase in atmospheric CO2 is less than the total emissions of CO2 from human sources, so by elementary deduction, the excess must be going into the oceans, forests and soils, the other components of the fast carbon cycle.\nA tell-tale signature of human fossil fuel emissions is the large fraction of CO2 being driven into the oceans. According to Henry's Law, we would expect the oceans to absorb more CO2 as the air above it becomes increasingly saturated with CO2. In other words the CO2 must be coming from a source external to the fast carbon cycle. This is supported by measurements showing that CO2 is accumulating in the ocean, and is reflected in the declining oceanic pH, showing the ocean is actually gaining CO2 over the long-term, not losing it, as Salby seems to believe.\nWe also know that the world's land vegetation has increased in mass - through re-growth in forests in the Northern Hemisphere, and CO2 fertilization of tropical forests. So that is gaining carbon too, and the areas affected are so large, we would expect them to have an effect on atmospheric CO2 levels at a global scale.\nThere are a host of other problems with Salby's 'model', such as the ice core record, and where the warming came from in the first place, but there's no need to go into these details when the fundamental premise of Salby's argument is so clearly wrong.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\n>\nLast updated on 11 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "Naomi Oreskes' study on consensus was flawed",
   "paragraph": "What does Naomi Oreskes' study on consensus show?\nLink to this page\nWhat the science says...\nAn examination of the papers that critics claim refute the consensus are found to actually endorse the consensus or are review papers (eg - they don't offer any new research but merely review other papers). This led the original critic Benny Peiser to retract his criticism of Oreskes' study.\nClimate Myth...\nNaomi Oreskes' study on consensus was flawed\nThe claim of “consensus” rests almost entirely on an inaccurate and now-outdated single page comment in the journal Science entitled The Scientific Consensus on Climate Change (Oreskes 2004). Benny Peiser conducted a search of peer-reviewed literature on the ISI Web of Science database between 1993 and 2003. Dr. Peiser’s research demonstrated that several of the abstracts confounded Oreskes’ assertion of unanimity by explicitly rejecting or casting doubt upon the notion that human activities are the main drivers of the observed warming over the last 50 years. (source: Consensus? What Consensus?)\nIn 2004, Naomi Oreskes performed a survey of all peer reviewed abstracts on the subject \"global climate change\" published between 1993 and 2003. She surveyed the ISI Web of Science database, looking only at peer reviewed, scientific articles. The survey failed to find a single paper that rejected the consensus position that global warming over the past 50 years is predominantly anthropogenic. 75% of the papers agreed with the consensus position while 25% made no comment either way (eg - focused on methods or paleoclimate analysis).\nBenny Peiser's rebuttal\nBenny Peiser repeated Oreskes survey and claimed to have found 34 peer reviewed studies rejecting the consensus. However, an inspection of each of the 34 studies reveals most of them don't reject the consensus at all. The remaining articles in Peiser's list are editorials or letters, not peer-reviewed studies. Peiser has since retracted his criticism of Oreskes survey:\n\"Only [a] few abstracts explicitly reject or doubt the AGW (anthropogenic global warming) consensus which is why I have publicly withdrawn this point of my critique. [snip] I do not think anyone is questioning that we are in a period of global warming. Neither do I doubt that the overwhelming majority of climatologists is agreed that the current warming period is mostly due to human impact.\"\nThe Viscount Monckton of Benchley's rebuttal\nDespite Peiser's retraction, the same argument was repeated by the Viscount Monckton of Benchley (and plagiarised by Schulte). Here are the five studies Monckton claims Oreskes should've included in her survey as rejecting the consensus position:\nMulti-resolution time series analysis applied to solar irradiance and climate reconstructions (Ammann 2003) finds a correlation between solar activity and temperature. However, the temperature reconstructions used end in the mid-20th century before the modern global warming trend and don't address the consensus position that warming over the past 50 years is primarily anthropogenic. However, Amman has published a more recent study examining more up-to-date temperature records, concluding \"although solar and volcanic effects appear to dominate most of the slow climate variations within the past thousand years, the impacts of greenhouse gases have dominated since the second half of the last century\" (Ammann 2007).\nSolar Forcing of Global Climate Change Since The Mid-17th Century (Reid 1997) finds a link between solar variability and climate change, concluding that \"solar forcing and anthropogenic greenhouse-gas forcing made roughly equal contributions to the rise in global temperature that took place between 1900 and 1955\". Considering CO2 forcing before 1955 was much lower while solar forcing was much greater due to increasing solar activity, this conclusion only serves to reinforce the consensus position. More on the sun...\nAd Hoc Committee on Global Climate Issues: Annual Report (Gerhard 2000) is non-peer reviewed. Oreske's survey only included peer reviewed studies. This is even conceded by Schulte.\nAtmospheric Greenhouse-Effect in the Context of Global Climate-Change (Kondratyev 1995) is a review, not an article - it doesn't actually include any research but reviews other studies. Oreskes' survey only included articles, not reviews.\nReview and impacts of climate change uncertainties (Fernau 1993) is another review, not an article, and is found in the Social Science Citation Index. Oreskes sampled articles only from the Science Citation Index.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 4 November 2016 by MichaelK. View Archives"
  },
  {
   "title": "Neptune is warming",
   "paragraph": "What does Neptune's brightening mean for global warming?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe sun has been cooling while Neptune was warming.\nClimate Myth...\nNeptune is warming\nIn April 2007, Heidi Hammel published a study comparing Neptune's brightening to Earth's warming and solar variations. She concluded that while they don't correlate statistically, the patterns are visually compelling and planetary climate changes may be due to solar variations.\nThis argument is part of a greater one that other planets are warming. If this is happening throughout the solar system, clearly it must be the sun causing the rise in temperatures – including here on Earth.\nIt is curious that the theory depends so much on sparse information – what we know about the climates on other planets and their history – yet its proponents resolutely ignore the most compelling evidence against the notion. Over the last fifty years, the sun’s output has decreased slightly: it is radiating less heat. We can measure the various activities of the sun pretty accurately from here on Earth, or from orbit above it, so it is hard to ignore the discrepancy between the facts and the sceptical argument that the sun is causing the rise in temperatures.\nTSI from 1880 to 1978 from Solanki. TSI from 1979 to 2009 from PMOD.\nBut if the sun’s output has levelled off or even diminished, then what is causing other planets to warm up? Are they warming at all?\nThe planets and moons that are claimed to be warming total roughly eight out of dozens of large bodies in the solar system. Some, like Uranus, may be cooling. All the outer planets have vastly longer orbital periods than Earth, so any climate change on them may be seasonal. Saturn and its moons take 30 Earth years to orbit the Sun, so three decades of observations equates to only 1 Saturnian year. Uranus has an 84-year orbit and 98° axial tilt, so its seasons are extreme. Neptune has not yet completed a single orbit since its discovery in 1846.\nThis is a round-up of the planets said by sceptics to be experiencing climate change:\nNeptune: observations of changes in luminosity on the surface of both Neptune and its largest moon, Triton, have been taken to indicate warming caused by increased solar activity. In fact, the brightening is due to the planet’s seasons changing, but very slowly. Summer is coming to Neptune’s southern hemisphere, bringing more sunlight, as it does every 164 years.\nJupiter: the notion that Jupiter is warming is actually based on predictions, since no warming has actually been observed. Climate models predict temperature increases along the equator and cooling at the poles. It is believed these changes will be catalysed by storms that merge into one super-storm, inhibiting the planet’s ability to mix heat. Sceptical arguments have ignored the fact this is not a phenomenon we have observed, and that the modelled forcing is storm and dust movements, not changes in solar radiation.\nMars: the notion that Mars is warming came from an unfortunate conflation of weather and climate. Based on two pictures taken 22 years apart, assumptions were made that have not proved to be reliable. There is currently no evidence to support claims that Mars is warming at all.\nPluto: the warming exhibited by Pluto is not really understood. Pluto’s seasons are the least understood of all: its existence has only been known for a third of its 248 -year orbit, and it has never been visited by a space probe. The ‘evidence’ for climate change consists of just two observations made in 1988 and 2002. That’s equivalent to observing the Earth’s weather for just three weeks out of the year. Various theories suggest its highly elliptical orbit may play a part, as could the large angle of its rotational axis. One recent paper suggests the length of Pluto’s orbit is a key factor, as with Neptune. Sunlight at Pluto is 900 times weaker than it is at the Earth.\nClaims that solar system bodies are heating up due to increased solar activity are clearly wrong. The sun’s output has declined in recent decades. Only Pluto and Neptune are exhibiting increased brightness. Heating attributed to other solar bodies remains unproven.\nLast updated on 15 September 2010 by gpwayne."
  },
  {
   "title": "No long tail means climate sensitivity is low",
   "paragraph": "What is the significance of climate sensitivity's long tail?\nLink to this page\nWhat the science says...\nRecent studies concluding that climate sensitivity is unlikely to be very high (above 4°C global surface warming in response to doubled atmospheric CO2) are often fundamentally flawed because they ignore the accelerated warming of the deep oceans. However, these studies still agree that climate sensitivity is within the IPCC expected range.\nClimate Myth...\nNo long tail means climate sensitivity is low\n\"this is what many have been saying now and for some time, that the climate sensitivity has been overestimated. Kudos to Annan for realizing the likelihood of a lower climate sensitivity\" (Anthony Watts)\nComments by a few climate scientists that climate sensitivity (the total amount of global surface warming in response to the increased greenhouse effect from a doubling of atmospheric CO2, including amplifying and dampening effects) is unlikely to be very high have been widely misinterpreted. For example, in a blog post and in comments on Andrew Revkin's Dot Earth blog, climate scientist James Annan said:\n\"...a high climate sensitivity [is] increasingly untenable. A value (slightly) under 2 is certainly looking a whole lot more plausible than anything above 4.5.\"\nAnnan is mostly critical of the Intergovernmental Panel on Climate Change (IPCC) for maintaining the \"long tail\" of high possible climate sensitivity values, for example as shown in Figure 1.\nFigure 1: Probability distribution of climate sensitivity to a doubling of atmospheric CO2, from Roe and Baker (2007)\nUncontroversial Comments\nAnnan's own work has focused on constraining the range of climate sensitivity values. For example, Annan and Hargreaves (2009) investigated the question using a Bayesian statistical approach, and concluded that\n\"the long fat tail that is characteristic of all recent estimates of climate sensitivity simply disappears, with an upper 95% probability limit ... easily shown to lie close to 4°C, and certainly well below 6°C.\"\nAnnan appears to feel that the IPCC has been too slow to let go of the sensitivity 'long tail' and incorporate a more tightly-constrained probability distribution into their reports. However, from a purely policy standpoint, it is important to consider all possible scenarios, and a very high climate sensitivity cannot yet be ruled out, as Chris Colose explains (via personal communication):\n\"From an IPCC/policy perspective, however, I'm not convinced the longer tails should be completely disregarded, even if they have very low probability of being consistent with the present-day evidence. Ray Pierrehumbert pointed to the Pliocene-case where a much different climate prevailed at a time with very similar climatic boundary conditions as present. The issue runs deeper than this, since such a regime shift could indicate the low-but-finite-probability of some sort of bifurcation point in the system ... I have seen this behavior in GCM's, not necessarily dor a doubling of CO2, but for higher concentrations, and as others have noted, the world doesn't end once we double CO2.\"\nNevertheless, in general, Annan's comments are consistent with the body of mainstream climate science research, and most of his colleagues believe that climate sensitivity is most likely close to 3°C surface warming in response to doubled CO2; unlikely to be more than 4.5°C or less than 2°C. One good example of this was Hansen et al. (2008), which concluded that equilibrium climate sensitivity is\n\"3 ± 1°C for the 4 W/m2 forcing of doubled CO2.\"\nAnd the probable range of equilibrium climate sensitivity cited by Annan is fully consistent with the body of scientific literature (Figure 2).\nFigure 2: Distributions and ranges for climate sensitivity from different lines of evidence. The circle indicates the most likely value. The thin colored bars indicate very likely value (more than 90% probability). The thicker colored bars indicate likely values (more than 66% probability). Dashed lines indicate no robust constraint on an upper bound. The IPCC likely range (2 to 4.5°C) is indicated by the vertical light blue bar. Adapted from Knutti and Hegerl (2008).\nReally all Annan is disputing is the 'long tail' of possible climate sensitivity values above 4.5°C, which Annan believes are more improbable than the IPCC report has stated. Nevertheless, the 'long tail' represents very low probability scenarios even in the IPCC report.\nMemo to Contrarians: \"High\" Isn't What You Think it is\nIt's something of a mystery why the climate contrarian blogosphere lit up in response to Annan's fairly mainstream, uncontroversial comments, other than the fact that he was rather critical of the IPCC. Annan said equilibrium climate sensitivity is unlikely to be higher than 4.5°C - there are few if any mainstream climate scientists who would disagree with this. He also said that sensitivity is unlikely to be much less than 2°C. This rules out the beliefs of many prominent climate scientist contrarians, like Roy Spencer (who believes equilibrium sensitivity is around 1.3°C) and Richard Lindzen (who believes it's less than 1°C).\nIn short, the comments Annan made which were celebrated by climate contrarians are incompatible with the beliefs of the most prominent climate contrarian scientists, but entirely compatible with mainstream climate scientists like James Hansen. Is this really an argument you want to get behind, contrarians?\nAnother issue highlighted by Joe Romm – climate sensitivity is not the same thing as future projected warming, unless we limit ourselves to a doubling of atmospheric CO2. At the moment, our emissions are tracking along some of the worst case scenarios (Figure 3), and if this continues, we will blow well past a doubling of atmospheric CO2.\nFigure 3: IEA fossil fuel CO2 emissions estimates vs. IPCC SRES emissions scenarios.\nThis highlights a mistake that climate contrarians make frequently, for example claiming that we will only see 1°C warming over the next century. The amount of future warming depends on two factors - climate sensitivity, and human CO2 emissions. Even if climate sensitivity is on the lower end, if we don't get our emissions under control, we will still see a dangerous amount of global warming (more details on this to come in a future blog post).\nDo We Have \"A Bit More Time\"?\nAnnan has also made the case that the most likely equilibrium climate sensitivity value may be closer to 2.5°C than 3°C. This case appears to be based on recent research taking two different approaches: looking at recent climate changes, and changes during the Last Glacial Maximum (LGM) about 20,000 years ago. As with the press release and media attention surrounding the Norwegian climate sensitivity project we recently examined, this has resulted in some suggestions that perhaps climate sensitivity is toward the lower end of possible values, which might buy us a bit more time to reduce human greenhouse gas emissions.\nIf true this would be good news, because our current efforts to reduce global human CO2 emissions have been woefully inadequte. They continue to climb with no international climate agreement in sight.\nHowever, caution is advisable here. As we discussed regarding the Norwegian paper, studies estimating climate sensitivity based on recent data may be biased low due to a failure to account for increased heat transfer to the 700–2000 meter ocean layer (Figure 3).\nFigure 3: Comparison of Global Heat Content 0-700 meters layer vs. 0-2000 meters layer, from the National Oceanographic Data Center.\nKevin Trenberth similarly notes (via personal communication),\n\"Global surface temperature is but one manifestation of warming and not a very good one as it is subject to a lot of natural variability. The increasing evidence for much more heat going deeper into the ocean has major implications and that pattern can easily be reversed. I think that any assessment of climate sensitivity based on the short term temperatures record is fraught with major difficulties and the implied assumptions do not stand up. Simple box models that keep mixing into deep ocean fixed are wrong!\nAnother interpretation is that given a certain energy imbalance at the top of atmosphere, if the heat is not manifested as surface temperature rise then it goes elsewhere. Another place it goes is into the more vigorous hydrological cycle which has a whole new set of implications.\"\nThere are also significant uncertainties associated with some radiative forcings (aerosols in particular), and the possibility that climate feedbacks are not linear (e.g. discussed in Long and Collins 2013).\nAs for the LGM, equilibrium climate sensitivity estimates depend strongly on the temperature data used. Research by Schmittner et al. (2011) and Annan and Hargreaves (2012) found most likely equilibrium sensitivity values close to 2.5°C based on LGM changes, whereas as noted above, Hansen et al. (2008) estimated 3 ± 1°C sensitivity based on the LGM. The Schmittner and Annan studies used ocean temperature data from the Multiproxy Approach for the Reconstruction of the Glacial Ocean (MARGO) project, about which Richard Alley noted:\n\"MARGO made a solid effort, which indicates very small temperature changes. But, there are other ways to do it, and indeed, [Schmittner et al.] coauthor Alan Mix has published independent papers indicating that the temperature changes were larger in some regions than indicated by MARGO. David Lea and others have also obtained larger temperature shifts….\nIn short, the MARGO data for the ocean show very small temperature change from the ice age to today, and thus lead to the low climate sensitivity, but they disagree with some independent estimates showing larger temperature change. They also lead to disagreement with the pollen-based land temperature data. Furthermore, they lead to an answer that disagrees with many other lines of evidence for climate sensitivity.\"\nA smaller temperature change estimate will result in a lower climate sensitivity estimate, so if MARGO data are biased low, that could result in a too-low climate sensitivity estimate.\nNevertheless, 2.5°C equilibrium sensitivity is certainly a possibility, well within the IPCC range. And it would essentially give us an extra 10 to 15 years of greenhouse gas emissions before we become committed to 2°C warming above pre-industrial levels, for example – in roughly 2038 as opposed to 2027 in Representative Concentrations Pathway (RCP) 4.5, which represents a scenario in which we slowly reduce our greenhouse gas emissions. This also depends on how our aerosol and soot emissions change in the future.\nRealism is Important\nUltimately while 2.5°C equilibrium sensitivity would certainly be better news than 3°C, it's not something we can bank on. In either scenario we need to take serious action to reduce human greenhouse gas emissions in order to avoid dangerous and potentially catastrophic climate change.\nJames Annan's comments and research are also incompatible with an equilibrium climate sensitivity much less than that – certainly nowhere near as low as the most prominent climate contrarians would have us believe.\nThe danger with acting as though we have plenty of time to reduce our emissions is that if this turns out not to be the case, we may find ourselves beyond the point where potentially catastrophic climate change is avoidable. At the moment, the body of scientific research points to 3°C as the most likely equilibrium climate sensitivity value. It's possible that it's 2.5°C, or even 2°C, but there's also evidence that it may be closer to 4°C, and it's certainly not much below 2°C, contrary to contrarian beliefs. In any realistic scenario, we need to take serious and immediate action to reduce human greenhouse gas emissions.\nHopefully sensitivity is on the lower end of possible values, but in any case, we are running out of time to solve the problem. We will have a blog post examining the various possible climate change scenarios (from best case to worst case) in the near future.\nLast updated on 24 March 2013 by dana1981. View Archives"
  },
  {
   "title": "No warming in 16 years",
   "paragraph": "Human activity continues to warm the planet over the past 16 years\nLink to this page\nWhat the science says...\nSelect a level... Intermediate Advanced\nOnce natural influences, in particular the impact of El Niño and La Niña, are removed from the recent termperature record, there is no evidence of a significant change in the human contribution to climate change.\nClimate Myth...\nNo warming in 16 years\n\"...there has been no increase in the global average surface temperature for the past 16 years\" (Judith Curry and David Rose)\nUpdate 26/05/2013: The '16 years' video, originally linked from this article, is not representative of the scientific consensus. In fact the short term trends are rather more complicated. The problem is explained in more detail in this article.\nHumans have continued to contribute to the greenhouse warming of the planet over the past 16 years. The myth arises from two misconceptions. Firstly, it ignores the fact that short term temperature trends are strongly influenced by a variety of natural factors and observational limitations which must be analyzed to isolate the human contribution. Secondly it focuses on one small part of the climate system (the atmosphere) while ignoring the largest part (the oceans). We will address each of these errors in turn.\nWhat factors influence the 16 year trend?\nClimate scientists have traditionally looked at climate over long periods - 30 years or more. However the media obsession with short term trends has focussed attention on the past 15-16 years. Short term trends are much more complex because they can be affected by many factors which cancel out over longer periods. In a recent interview James Hansen noted \"If you look over a 30-40 year period the expected warming is two-tenths of a degree per decade, but that doesn't mean each decade is going to warm two-tenths of a degree: there is too much natural variability\".\nThe list of factors which can affect short term temperature trends is extensive, and some of them can rival the global warming signal in magnitude over short periods. The following table identifies a range of influences on the recent temperature trend:\nInfluence Effect Notes\nHuman GHG emissions Warming\nHuman sulphate emissions Cooling Recent emissions from China\nCoverage bias Cooling HadCRUT4 and NOAA only\nSea surface temperature bias Cooling GISTEMP and NOAA only\nThe El Niño oscillation Cooling The recent run of La Niñas\nVolcanic erruptions Warming Recovery from Pinatubo erruption\nSolar cycle Cooling Recent solar minimum\nLonger term oscillations Unknown AMO and PDO\nChange in ocean heat uptake Cooling Balmaseda et al (2013), Guemas et al (2013)\nMost of the short term influences, with the exception of greenhouse gas emissions and probably volcanoes (but see Neely et al 2013), have had a cooling influence. As a result it is unsurprising that we have seen a reduced rate of warming over the past 16 years. The fact that there has been any warming at all is strongly supportive of the warming effect of greenhouse gas emissions.\nThe fundamental mechanism of global warming is a change in the top-of-atmosphere energy balance, and as a result the energy content of the climate system provides a more direct measure of global warming which avoids many of these problems, although the observational record is shorter and less complete (e.g. Church et al 2011).\nThe rest of the climate system\nFocusing on surface air temperatures also misses more than 90% of the overall warming of the planet (Figure 2).\nFigure 2: Components of global warming for the period 1993 to 2003 calculated from IPCC AR4 5.2.2.3.\nNuccitelli et al. (2012) considered the warming of the oceans (both shallow and deep), land, atmosphere, and ice, and showed that global warming has not slowed in recent years (Figure 3).\nFigure 3: Land, atmosphere, and ice heating (red), 0-700 meter OHC increase (light blue), 700-2,000 meter OHC increase (dark blue). From Nuccitelli et al. (2012).\nReferences\nFoster and Rahmstorf (2011), Global temperature evolution 1979–2010 doi:10.1088/1748-9326/6/4/044022\nNuccitelli et al. (2012) Comment on Ocean heat content and Earth's radiation imbalance. II. Relation to climate shifts doi:10.1016/j.physleta.2012.10.010\nCredits: Calculations and video: Kevin C. Voiceover: Daniel Bailey. Advice: The SkS team.\nIntermediate rebuttal written by Kevin C\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 9 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Ocean acidification isn't serious",
   "paragraph": "Ocean acidification: global warming's evil twin\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nOcean acidification threatens entire marine food chains.\nClimate Myth...\nOcean acidification isn't serious\n'Our harmless emissions of trifling quantities of carbon dioxide cannot possibly acidify the oceans. Paper after paper after learned paper in the peer-reviewed literature makes that quite plain. Idso cites some 150 scientific sources, nearly all of them providing hard evidence, by measurement and experiment, that there is no basis for imagining that we can acidify the oceans to any extent large enough to be measured even by the most sensitive instruments.' (Christopher Monckton)\nNot all of the CO2 emitted by human industrial activities remains in the atmosphere. Between 25% and 50% of these emissions over the industrial period have been absorbed by the world’s oceans, preventing atmospheric CO2 buildup from being much, much worse.\nBut this atmospheric benefit comes at a considerable price.\nAs ocean waters absorb CO2 they become more acidic. This does not mean the oceans will become acid. Ocean life can be sensitive to slight changes in pH levels, and any drop in pH is an increase in acidity, even in an alkaline environment.\nThe acidity of global surface waters has increased by 30% in just the last 200 years. This rate of acidification is projected through the end of the century to accelerate even further with potentially catastrophic impacts to marine ecosystems.\nEndorsed by seventy academies of science from around the world, a June 2009 statement from the InterAcademy Panel on International Issues (IAP) stated the following.\n\"The current rate of change is much more rapid than during any event over the last 65 million years. These changes in ocean chemistry are irreversible for many thousands of years, and the biological consequences could last much longer.\"\n- The InterAcademy Panel, June 1, 2009\nAs surface waters become more acidic, it becomes more difficult for marine life like corals and shellfish to form the hard shells necessary for their survival, and coral reefs provide a home for more than 25% of all oceanic species. Tiny creatures called pteropods located at the base of many oceanic food chains can also be seriously impacted. The degradation of these species at the foundation of marine ecosystems could lead to the collapse of these environments with devastating implications to millions of people in the human populations that rely on them.\nThe IAP also stated that, if atmospheric CO2 were to reach 550 parts per million (ppm) along its current rapid ascent from its pre-industrial level of 280 ppm, coral reefs around the globe could be dissolving.\nBasic rebuttal written by Michael Searcy\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Oceans are cooling",
   "paragraph": "Does ocean cooling prove global warming has ended?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe most recent ocean measurements show consistent warming.\nClimate Myth...\nOceans are cooling\n“Ocean heat touches on the very core of the AGW hypothesis: When all is said and done, if the climate system is not accumulating heat, the hypothesis is invalid.\n[…]Now that heat accumulation has stopped (and perhaps even reversed), the tables have turned. The same criteria used to support their hypothesis, is now being used to falsify it.” (William DiPuccio)\nIn 2008, climate change sceptic Roger Pielke Sr said this: “Global warming, as diagnosed by upper ocean heat content has not been occurring since 2004”. It is a fine example of denialist spin, making several extraordinary leaps:\nthat one symptom is indicative of the state of an entire malaise (e.g. not being short of breath one day means your lung cancer is cured).\nthat one can claim significance about a four year period when it’s too short to draw any kind of conclusion\nthat global warming has not been occurring on the basis of ocean temperatures alone\nSo much for the hype. What does the science say about the temperature of the oceans – which, after all, constitute about 70% of the Earth’s surface? The oceans store approximately 80% of all the energy in the Earth’s climate, so ocean temperatures are a key indicator for global warming.\nNo straight lines\nClaims that the ocean has been cooling are correct. Claims that global warming has stopped are not. It is an illogical position: the climate is subject to a lot of natural variability, so the premise that changes should be ‘monotonic’ – temperatures rising in straight lines – ignores the fact that nature doesn’t work like that. This is why scientists normally discuss trends – 30 years or more – so that short term fluctuations can be seen as part of a greater pattern. (Other well-known cyclic phenomena like El Nino and La Nina play a part in these complex interactions).\nLooking at the trend in ocean heat, this is what we find:\nSource: Levitus 2009\nThere are, however, disputes about the accuracy of Argo buoys and expendable measuring devices dropped into the sea, and the reporting of temperatures down to only 700 metres. How do scientists resolve these kind of disputes – bearing in mind that such disputes are the very stuff of science, the essence of true scepticism? One way is to find more data sources – different ways of measuring the phenomenon in dispute. By using results from seven different teams of scientists, all using different tools and methods, we are able to see a clear trend. And while there is variation between team results due to the differences in technique and measurement methods, one thing they all agree on: long term, temperatures are going up.\nSource: Lyman 2010\nThe reaction of the oceans to climate change are some of the most profound across the entire environment, including disruption of the ocean food chain through chemical changes caused by CO2, the ability of the sea to absorb CO2 being limited by temperature increases, (and the potential to expel sequestered CO2 back into the atmosphere as the water gets hotter), sea-level rise due to thermal expansion, and the amount of water vapour in the atmosphere.\nWhile there is a great deal we don’t know about how the oceans behave, we do however know that it’s safer to discuss all aspects of climate change using multiple sets of data, rather than just one, as Pielke Sr did. If ocean heat is a guide, then global warming is still on track to cause great disruption if we don’t modify our actions to reduce the release of anthropogenic CO2.\nClaims that global warming is not happening on the basis of short-term ocean temperatures are not supported by the evidence.\nBasic rebuttal written by GPWayne\nLast updated on 1 August 2013 by gpwayne. View Archives"
  },
  {
   "title": "Other planets are warming",
   "paragraph": "What climate change is happening to other planets in the solar system?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nMars and Jupiter are not warming, and anyway the sun has recently been cooling slightly.\nClimate Myth...\nOther planets are warming\n\"[E]vidence that CO2 is not the principle driver of warming on this planet is provided by the simultaneous warming of other planets and moons in our solar system, despite the fact that they obviously have no anthropogenic emissions of greenhouse gases.\nMars, Triton, Pluto and Jupiter all show global warming, pointing to the Sun as the dominating influence in determining climate throughout the solar system.\" (Ian McClintock)\nThis argument is part of a greater one that other planets are warming. If this is happening throughout the solar system, clearly it must be the sun causing the rise in temperatures – including here on Earth.\nIt is curious that the theory depends so much on sparse information – what we know about the climates on other planets and their history – yet its proponents resolutely ignore the most compelling evidence against the notion. Over the last fifty years, the sun’s output has decreased slightly: it is radiating less heat. We can measure the various activities of the sun pretty accurately from here on Earth, or from orbit above it, so it is hard to ignore the discrepancy between the facts and the sceptical argument that the sun is causing the rise in temperatures.\nTSI from 1880 to 1978 from Solanki. TSI from 1979 to 2009 from PMOD.\nBut if the sun’s output has levelled off or even diminished, then what is causing other planets to warm up? Are they warming at all?\nThe planets and moons that are claimed to be warming total roughly eight out of dozens of large bodies in the solar system. Some, like Uranus, may be cooling. All the outer planets have vastly longer orbital periods than Earth, so any climate change on them may be seasonal. Saturn and its moons take 30 Earth years to orbit the Sun, so three decades of observations equates to only 1 Saturnian year. Uranus has an 84-year orbit and 98° axial tilt, so its seasons are extreme. Neptune has not yet completed a single orbit since its discovery in 1846.\nThis is a round-up of the planets said by sceptics to be experiencing climate change:\nMars: the notion that Mars is warming came from an unfortunate conflation of weather and climate. Based on two pictures taken 22 years apart, assumptions were made that have not proved to be reliable. There is currently no evidence to support claims that Mars is warming at all. More on Mars...\nJupiter: the notion that Jupiter is warming is actually based on predictions, since no warming has actually been observed. Climate models predict temperature increases along the equator and cooling at the poles. It is believed these changes will be catalysed by storms that merge into one super-storm, inhibiting the planet’s ability to mix heat. Sceptical arguments have ignored the fact this is not a phenomenon we have observed, and that the modelled forcing is storm and dust movements, not changes in solar radiation.\nNeptune: observations of changes in luminosity on the surface of both Neptune and its largest moon, Triton, have been taken to indicate warming caused by increased solar activity. In fact, the brightening is due to the planet’s seasons changing, but very slowly. Summer is coming to Neptune’s southern hemisphere, bringing more sunlight, as it does every 164 years.\nPluto: the warming exhibited by Pluto is not really understood. Pluto’s seasons are the least understood of all: its existence has only been known for a third of its 248 -year orbit, and it has never been visited by a space probe. The ‘evidence’ for climate change consists of just two observations made in 1988 and 2002. That’s equivalent to observing the Earth’s weather for just three weeks out of the year. Various theories suggest its highly elliptical orbit may play a part, as could the large angle of its rotational axis. One recent paper suggests the length of Pluto’s orbit is a key factor, as with Neptune. Sunlight at Pluto is 900 times weaker than it is at the Earth.\nClaims that solar system bodies are heating up due to increased solar activity are clearly wrong. The sun’s output has declined in recent decades. Only Pluto and Neptune are exhibiting increased brightness. Heating attributed to other solar bodies remains unproven.\nLast updated on 15 September 2010 by gpwayne."
  },
  {
   "title": "Over 31,000 scientists signed the OISM Petition Project",
   "paragraph": "How the OISM Petition Project casts doubt on the scientific consensus on climate change\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe 'OISM petition' was signed by only a few climatologists.\nClimate Myth...\nOver 31,000 scientists signed the OISM Petition Project\nThe Petition Project features over 31,000 scientists signing the petition stating \"there is no convincing scientific evidence that human release of carbon dioxide will, in the forseeable future, cause catastrophic heating of the Earth's atmosphere\". (OISM)\nThere are several claims that large numbers of scientists do not agree with the theory of climate change, the best known of which is a petition organised by the Oregon Institute of Science and Medicine (the OISM petition). This petition now appears to be signed by over 32,000 people with a BSc or higher qualification. The signatories agree with these statements:\nThe proposed limits on greenhouse gases would harm the environment, hinder the advance of science and technology, and damage the health and welfare of mankind.\nThere is no convincing scientific evidence that human release of carbon dioxide, methane, or other greenhouse gasses is causing or will, in the foreseeable future, cause catastrophic heating of the Earth's atmosphere and disruption of the Earth's climate.\nNo evidence has ever been offered to support the first statement, and the second statement is in flat contradiction with the scientists who study climate change. There are also valid issues regarding the methodology:\nThe organisers have never revealed how many people they canvassed (so the response rate is unknown) nor have they revealed the sampling methodology, an ironic omission considering how much fuss is made about scientists being candid and making public their methods and data.\nThe petition is, in terms of climate change science, rather out of date.\nIn the professional field of climate science, the consensus is unequivocal: human activities are causing climate change and additional anthropogenic CO2 may cause great disruption to the climate.\n32,000 Sounds Like A Lot\nIn fact, OISM signatories represent a tiny fraction (~0.3%) of all US science graduates (petition cards were only sent to individuals within the U.S)\nAccording to figures from the US Department of Education Digest of Education Statistics: 2008, 10.6 million science graduates have gained qualifications consistent with the OISM polling criteria since the 1970-71 school year. 32,000 out of 10 million is not a very compelling figure, but a tiny minority - approximately 0.3 per cent.\nThere are many issues casting doubt on the validity of this petition. On investigation, attempts to undermine the scientific consensus on climate change often appear to have ideological roots, vested business interests or political sponsors. The claims made for the OISM petition do not withstand objective scrutiny, and the assertions made in the petition are not supported by evidence, data or scientific research.\nSeveral studies conducted independently (Oreskes 2004, Oreskes 2007, Doran and Zimmerman (2009), Anderegg et al. (2010), Cook et. al., 2013) have shown that 97% of climate scientists agree that humans are causing the climate to change, and that anthropogenic greenhouse gases are causing global changes to the climate. These views form the scientific consensus on climate change.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "Peer review process was corrupted",
   "paragraph": "Climategate and the peer-review process\nLink to this page\nWhat the science says...\nThe Independent Climate Change Email Review investigated the CRU scientists' actions relating to peer review. In one case, it judged their strong reaction to a controversial paper was not unusual. In another, it turned out the alleged victim had actually been spreading malicious rumours about CRU. In a third, the allegation of collusion fell apart when the full email exchange was examined. The Review concluded that CRU's actions were normal and did not threaten the integrity of peer review.\nClimate Myth...\nPeer review process was corrupted\n\"They had interfered with the process of peer-review itself by leaning on journals to get their friends rather than independent scientists to review their papers. They had successfully leaned on friendly journal editors to reject papers reporting results inconsistent with their political viewpoint. They had campaigned for the removal of a learned journal’s editor, solely because he did not share their willingness to debase and corrupt science for political purposes.\" (Christopher Monckton)\nExhibit No. 1 of the climate conspiracy theory is a collection of emails stolen from the Climatic Research Unit (CRU) of the University of East Anglia (UEA), which appeared on the internet in November 2009. Though some of these \"Climategate\" emails can sound damning when quoted out of context, several inquiries have cleared the scientists. The most comprehensive inquiry, the Independent Climate Change Email Review, did something the media completely failed to do: it put the emails into context by investigating the main allegations. Its general findings (summarised here) were that the scientists' rigour and honesty are not in doubt, and their behaviour did not prejudice the advice given to policymakers, though they did fail to display the proper degree of openness.\nOne set of allegations examined by the Review is the potential corruption of the peer review process. Contrarians claim that a small group of scientists, including those at CRU, attempted to hijack the peer review process, pressuring journals to reject papers whose conclusions contradicted their own. There are three main instances in which this is alleged to have happened.\nThe first involved a paper by Soon and Baliunas published in Climate Research in 2003, reviewing the literature on temperature change during the recent millennium. It concluded that late 20th century Northern Hemisphere temperatures were not unprecedented, contradicting the majority of the other analyses which came before and after it. The paper was approved by four reviewers and one of the journal’s ten review editors, Chris de Freitas, but received a hostile reception from the climate science community, as is reflected in the CRU emails. For example, Jones wrote in an email dated 11/3/2003:\nI think the skeptics will use this paper to their own ends and it will set paleo back a number of years if it goes unchallenged. I will be emailing the journal to tell them I’m having nothing more to do with it until they rid themselves of this troublesome editor, a well-known skeptic in NZ. A CRU person is on the board but papers get dealt with by the editor assigned by Hans von Storch.\nMichael Mann replied:\nThis was the danger of always criticising the skeptics for not publishing in the “peer-reviewed literature”. Obviously, they found a solution to that — take over a journal! So what do we do about this? I think we have to stop considering “Climate Research” as a legitimate peer-reviewed journal. Perhaps we should encourage our colleagues in the climate research community to no longer submit to, or cite papers in, this journal. We would also need to consider what we tell or request of our more reasonable colleagues who currently sit on the editorial board…\nContrarians have used these email quotes to argue that a group of scientists including Jones and Mann deliberately hijacked the peer review process to promote a favoured conclusion.\nThe second incident involved the editor of Energy and Environment, Dr Boehmer-Christiansen, who claims “[t]he hacked emails revealed attempts to manipulate peer review to E&E’s disadvantage, and showed that libel threats were considered against its editorial team. Dr Jones even tried to put pressure on my university department.”\nThe third involved Briffa’s actions as the editor of Holocene. In an email dated 4/6/2003, Briffa wrote:\nI am really sorry but I have to nag about that review — Confidentially I now need a hard and if required extensive case for rejecting — to support Dave Stahle’s and really as soon as you can. Please\nBased on this email, contrarians accuse Briffa of colluding with the reviewer to reject a contradictory paper.\nAs well as investigating these individual cases, the Review also commissioned Dr Richard Horton, editor of distinguished medical journal The Lancet, to write an essay about the context of peer review, published as an appendix to the inquiry report.\nDr Horton told the Review that some of the questions raised by the CRU emails “may be based on a misinformed view of the peer review process”. Peer review is quality control, not censorship. Although it is obviously impossible for reviewers to be purely objective, the decision to accept or reject is the editor’s responsibility alone; what an editor seeks from a reviewer is “a powerful critique of the manuscript”. Peer review has an important role to play: it prevents over-interpretation and ensures discussion of uncertainty and context — things which contrarians claim to be in favour of. However peer review is not infallible: “Many well-founded concepts are rejected and many erroneous ideas accepted.”\nHorton wrote:\nAuthors and reviewers are frequently passionate in their intellectual combat over a piece of research. The tone of their exchanges and communications with editors can be attacking, accusatory, aggressive, and even personal. If a research paper is especially controversial and word of it is circulating in a particular scientific community, third-party scientists or critics with an interest in the work may get to hear of it and decide to contact the journal. They might wish to warn or encourage editors. This kind of intervention is entirely normal. It is the task of editors to weigh up the passionate opinions of authors and reviewers, and to reflect on the comments (and motivations) of third parties. To an onlooker, these debates may appear as if improper pressure is being exerted on an editor. In fact, this is the ordinary to and fro of scientific debate going on behind the public screen of science. Occasionally, a line might be crossed. [Appendix 5]\nSo the question becomes: did the CRU scientists cross that line? It turns out the answer is probably not. Let’s look at the three individual cases named above.\nIn the case of Soon & Baliunas 2003, it was not only CRU which reacted strongly to the paper. The Review recounts:\nA number of review editors resigned as a reaction against the publication of what they regarded as a seriously flawed paper. The journal’s publisher admitted that the journal should have requested appropriate revisions of the manuscript prior to publication. The Editor in Chief resigned on being refused permission by the publisher to write an editorial about what he regarded as a failure of the peer review system. [8.3]\nAlthough de Freitas described this reaction as “a mix of a witch-hunt and the Spanish Inquisition”, the Review pointed out that there were scientific grounds given (namely, the paper “conflated qualitative data on temperature and precipitation from many sources that could not be combined into a consistent proxy record”). These counter-arguments “are strongly put, and suggest that the reaction was based on a belief, for which evidence was adduced, that the science was poor. In light of the reaction of the Journal’s publisher, we do not believe that any criticism of Jones can be justified in this regard.” [8.3]\nConsidering this in the context provided by Richard Horton’s paper, the Review concluded that “this scale of reaction is not unusual in contested areas […] The Review makes no judgement or otherwise about the correctness or otherwise of the Soon and Baliunas paper, but we conclude that the strong reaction to it was understandable, and did not amount to undue pressure on Climate Research.” [8.3]\nIn the case of Energy and Environment, the Review Team “see nothing [in] Boehmer-Christiansen’s evidence that supports any allegation that CRU has directly and improperly attempted to influence the journal that she edits.” Furthermore, the emails actually show that Boehmer-Christiansen had been accusing CRU of scientific fraud, and “Jones’ response to her accusation of scientific fraud was appropriate, measured and restrained.” [8.4]\nIn the case of Briffa’s actions, when the Review examined the full email exchange they found nothing to support the interpretation of collusion in rejecting contradictory ideas:\nIt appears to reflect an Editor with a strongly negative review in hand, and who presumably had read the paper, asking for confirmation that the paper should be rejected, possibly to reduce one of the many complications that assail an editor; and in view of the delay in communicating to authors, hoping for a strong decision from the referee. On receiving a second, more equivocal review, he offers the authors the opportunity to re-submit. [8.5]\nThe Review’s conclusion on the peer review allegations was as follows (its emphasis):\nOn the allegations that there was subversion of the peer review or editorial process we find no evidence to substantiate this in the three instances examined in detail. On the basis of the independent work we commissioned (see Appendix 5) on the nature of peer review, we conclude that it is not uncommon for strongly opposed and robustly expressed positions to be taken up in heavily contested areas of science. We take the view that such behaviour does not in general threaten the integrity of peer review or publication. [1.3.3]\nDespite being heralded as “the final nail in the coffin of anthropogenic global warming”, Climategate did not even demonstrate small-scale corruption of the peer review process, let alone on the scale of the climate science community. In any case, the CRU scientists reviewed only a small part of the large body of evidence for anthropogenic global warming. That mountain of evidence cannot be explained away by the behaviour of a few individuals.\nLast updated on 24 December 2010 by James Wight."
  },
  {
   "title": "Phil Jones says no global warming since 1995",
   "paragraph": "Phil Jones and the meaning of 'statistically significant warming'\nLink to this page\nWhat the science says...\nWhen you read Phil Jones' actual words, you see he's saying there is a warming trend but it's not statistically significant. He's not talking about whether warming is actually happening. He's discussing our ability to detect that warming trend in a noisy signal over a short period.\nClimate Myth...\nPhil Jones says no global warming since 1995\n'Phil Jones said that for the past 15 years there has been no \"statistically significant\" warming. The admissions will be seized on by sceptics as fresh evidence that there are serious flaws at the heart of the science of climate change and the orthodoxy that recent rises in temperature are largely man-made.' (Daily Mail)\nA headline in the Daily Mail claims that Phil Jones, ex-director of the University of East Anglia’s Climatic Research Unit, said 'there has been no global warming since 1995'. Not only did Phil Jones not say these words, this interpretation shows a poor understanding of the scientific concepts behind his words. To fully understand what Phil Jones was saying, one needs to read his actual words and understand the science discussed. Here is the relevant excerpt from the BBC interview:\nBBC: Do you agree that from 1995 to the present there has been no statistically-significant global warming\nPhil Jones: Yes, but only just. I also calculated the trend for the period 1995 to 2009. This trend (0.12C per decade) is positive, but not significant at the 95% significance level. The positive trend is quite close to the significance level. Achieving statistical significance in scientific terms is much more likely for longer periods, and much less likely for shorter periods.\nBBC: How confident are you that warming has taken place and that humans are mainly responsible?\nPhil Jones: I'm 100% confident that the climate has warmed. As to the second question, I would go along with IPCC Chapter 9 - there's evidence that most of the warming since the 1950s is due to human activity.\nPhil Jones is saying there is a warming trend but it's not statistically significant. He's not talking about whether warming is actually happening. He's discussing our ability to detect that warming trend in a noisy signal over a short period. To demonstrate this, look at the HadCRUT temperature record from 1995 to 2009. The linear trend is that of warming. However, the temperature record is very noisy with lots of short term variability. The noisy signal means that over a short period, the uncertainty of the warming trend is almost as large as the actual trend. Hence it's considered statistically insignificant. Over longer time periods, the uncertainty is less and the trend is more statistically significant.\nFigure 1: HadCRUT global temperature change in degrees Celsius. Blue is yearly average. Red is linear trend (HadCRUT).\nIt bears remembering that the HadCRUT record only covers around 80% of the globe. Analysis by European Centre for Medium-Range Weather Forecasts (ECMWF) and NASA GISS (Hansen 2006) find that the areas omitted by HadCRUT are some of the fastest warming regions in the world. Consequently, the HadCRUT record underestimates the warming trend, as demonstrated by the NASA GISS record which covers the whole globe:\nFigure 2: NASA GISS Global temperature change in degrees Celsius. Blue is yearly average. Red is linear trend (NASA GISS).\nHowever, even this doesn't give you the full picture. Surface temperature is only a small fraction of our climate with most of global warming going into the oceans. When all the heat accumulating in the oceans, warming the land and atmosphere and melting ice is tallied up, we see that global warming is still happening.\nFigure 3: Change in total Earth heat content from 1950 (Murphy 2009).\nLast updated on 26 June 2010 by John Cook."
  },
  {
   "title": "Planting a trillion trees will solve global warming",
   "paragraph": "How much will planting a trillion trees slow global warming?\nLink to this page\nWhat the science says...\nResearch has shown that maximum afforestation and reforestation (close to a trillion new trees) would sequester around 75 billion tons of carbon, which is 7–8 years of annual human emissions at current rates and enough to slow global warming by less than a quarter degree Celsius.\nClimate Myth...\nPlanting a trillion trees will solve global warming\n\"There is no limit to how much carbon we can store in wood\" [Rep. Bruce Westerman (R-AR)]\nDuring his 2019 State of the Union address, Donald Trump announced that the United States will join the Trillion Trees Initiative. House Republicans plan to introduce legislation to plant 3.3 billion trees per year domestically over the next 30 years (an 800 million increase over the 2.5 billion per year that are already planted in the U.S.) as the lynchpin of their party's climate plan. This poses the question – how much impact would the tree planting initiative have on atmospheric carbon dioxide (CO2) levels and global warming?\nBastin et al. (2019) sought to quantify the potential global tree restoration potential and the carbon sequestration associated with that reforestation and afforestation. The study concluded, “there is room for an extra 0.9 billion hectares of canopy cover, which could store 205 gigatonnes of carbon [GtC] in areas that would naturally support woodlands and forests.” For comparison, humans have emitted approximately 640 GtC, so this would represent a significant chunk of human emissions to date. 900 million hectares of land is approximately the size of the United States.\nHowever, several comments identified flaws in the Bastin et al. estimate. Friedlingstein et al. (2019) noted that their estimate of the potential carbon storage of trees in each biome did not account for the carbon already stored in those regions, and thus concluded:\nthe potential carbon storage would be substantially lower than reported … Moreover, forests affect climate through biophysical feedbacks, such as changes in albedo or evapotranspiration, which can counteract the cooling effect from CO2 uptake … These biophysical feedbacks were not discussed in the article and could substantially reduce the potential of forest reforestation in some of the considered regions.\nVeldman et al. (2019) in concluding that the true maximum tree carbon sequestration potential is closer to 42 GtC, noted:\nTheir analysis inflated soil organic carbon gains, failed to safeguard against warming from trees at high latitudes and elevations, and considered afforestation of savannas, grasslands, and shrublands to be restoration.\nLewis et al. (2019) noted that the carbon sequestration rate used by Bastin et al. (0.22 GtC per million hectares) was twice that in previously published estimates. Using a variety of methods to roughly approximate the global tree carbon sequestration potential, Lewis et al. estimated the value between 89 and 108 GtC. Moreover, Lewis et al. note:\n25% of the new tree cover [would be] in tundra and boreal regions, where warming from forests’ lower surface albedo can offset the cooling from new carbon uptake.\nLet’s examine the carbon sequestration and global warming mitigation potential of planting trees in all available areas outside of tundra and boreal regions, where as Lewis et al. note, replacing relatively reflective land surface with dark tree canopy would offset the cooling from carbon uptake. Using the above referenced studies, as an approximation, let’s estimate that doing so could sequester 75 GtC, once the nearly trillion trees have reached maturity.\nHumans currently release 10 GtC annually from fossil fuel combustion and other activities; therefore, continuing at current emissions rates would offset the carbon sequestration potential of this maximal tree restoration effort within 7 to 8 years. Thus it’s important to note that planting trees cannot replace the phasing out of fossil fuels, but it can complement it.\nUnder policies and commitments currently in place, global average surface temperatures are on a path for approximately 3°C in 2100 (though headed north of 4.5°C by the time a new equilibrium is reached, unless future policies bring emissions down to zero). This translates into approximately 1,650 GtC of human emissions between 2020 and 2100, or an atmospheric carbon dioxide concentration around 620ppm in 2100. Removing 75 GtC through tree planting would lower that CO2 concentration to around 585 ppm (see this post for useful conversions). To translate these numbers into global surface temperature changes, we can use the following formula related to the radiative forcing of CO2:\nWhere the left side of the formula represents the global surface temperature change once a new equilibrium is reached, RF is the radiative forcing (in this case, from increased CO2), lambda is the climate sensitivity parameter (approximately 0.8), C is the atmospheric CO2 concentration, and Co is the initial CO2 concentration (280 ppm pre-industrial).\nAt equilibrium, the temperature change associated with 620 ppm CO2 is 3.4°C, and for 585 ppm is 3.15°C (this doesn’t account for non-CO2 greenhouse gases or other forcings). By 2100 when the climate system will not yet be in equilibrium, the CO2-caused temperature changes would be closer to 2°C and 1.85°C, respectively. In short, maximal tree planting would offset around 0.15°C warming by 2100 and a quarter degree Celsius at equilibrum.\nDomestically, the US emits about 5.3 Gt CO2 (1.4 GtC) per year. Planting another 24 billion trees (an additional 800 million per year over 30 years), at around 1,000 trees per hectare and 0.12 GtC sequestered per hectare, per Lewis et al. (2019), corresponds to about 5.3 GtC sequestered once those trees reach maturity. In short, the tree planting proposal would offset about 3–4 year’s worth of US carbon emissions at current rates. That’s a start, but only a start.\nIt’s worth noting that reforestation and afforestation occupy a number of high slots on the Project Drawdown list of top climate solutions, including #5 and #12. However, adding them all up accounts for just around 40 GtC. This is probably a more realistic number than our 75 GtC, since we can’t plant trees on every available hectare of land. Doing so would decrease agricultural production and thus increase food prices, for example. This highlights why Project Drawdown lists 100 different individual solutions.\nPlanting trees won't be enough to solve climate change unless they're Ents and do to our fossil fuel infrastructure what they did to Isengard in Lord of the Rings.\nThere is no climate change silver bullet; planting trees helps, but it’s just one piece of silver buckshot among the many solutions needed to avert a climate crisis.\nLast updated on by dana1981. View Archives"
  },
  {
   "title": "Pluto is warming",
   "paragraph": "Pluto warms while the sun cools\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nAnd the sun has been recently cooling.\nClimate Myth...\nPluto is warming\n\"Some people think that our planet is suffering from a fever. Now scientists are telling us that Mars is experiencing its own planetary warming: Martian warming. It seems scientists have noticed recently that quite a few planets in our solar system seem to be heating up a bit, including Pluto.\" (Fred Thompson)\nThis argument is part of a greater one that other planets are warming. If this is happening throughout the solar system, clearly it must be the sun causing the rise in temperatures – including here on Earth.\nIt is curious that the theory depends so much on sparse information – what we know about the climates on other planets and their history – yet its proponents resolutely ignore the most compelling evidence against the notion. Over the last fifty years, the sun’s output has decreased slightly: it is radiating less heat. We can measure the various activities of the sun pretty accurately from here on Earth, or from orbit above it, so it is hard to ignore the discrepancy between the facts and the sceptical argument that the sun is causing the rise in temperatures.\nTSI from 1880 to 1978 from Solanki. TSI from 1979 to 2009 from PMOD.\nBut if the sun’s output has levelled off or even diminished, then what is causing other planets to warm up? Are they warming at all?\nThe planets and moons that are claimed to be warming total roughly eight out of dozens of large bodies in the solar system. Some, like Uranus, may be cooling. All the outer planets have vastly longer orbital periods than Earth, so any climate change on them may be seasonal. Saturn and its moons take 30 Earth years to orbit the Sun, so three decades of observations equates to only 1 Saturnian year. Uranus has an 84-year orbit and 98° axial tilt, so its seasons are extreme. Neptune has not yet completed a single orbit since its discovery in 1846.\nThis is a round-up of the planets said by sceptics to be experiencing climate change:\nPluto: the warming exhibited by Pluto is not really understood. Pluto’s seasons are the least understood of all: its existence has only been known for a third of its 248 -year orbit, and it has never been visited by a space probe. The ‘evidence’ for climate change consists of just two observations made in 1988 and 2002. That’s equivalent to observing the Earth’s weather for just three weeks out of the year. Various theories suggest its highly elliptical orbit may play a part, as could the large angle of its rotational axis. One recent paper suggests the length of Pluto’s orbit is a key factor, as with Neptune. Sunlight at Pluto is 900 times weaker than it is at the Earth.\nJupiter: the notion that Jupiter is warming is actually based on predictions, since no warming has actually been observed. Climate models predict temperature increases along the equator and cooling at the poles. It is believed these changes will be catalysed by storms that merge into one super-storm, inhibiting the planet’s ability to mix heat. Sceptical arguments have ignored the fact this is not a phenomenon we have observed, and that the modelled forcing is storm and dust movements, not changes in solar radiation.\nMars: the notion that Mars is warming came from an unfortunate conflation of weather and climate. Based on two pictures taken 22 years apart, assumptions were made that have not proved to be reliable. There is currently no evidence to support claims that Mars is warming at all.\nNeptune: observations of changes in luminosity on the surface of both Neptune and its largest moon, Triton, have been taken to indicate warming caused by increased solar activity. In fact, the brightening is due to the planet’s seasons changing, but very slowly. Summer is coming to Neptune’s southern hemisphere, bringing more sunlight, as it does every 164 years.\nClaims that solar system bodies are heating up due to increased solar activity are clearly wrong. The sun’s output has declined in recent decades. Only Pluto and Neptune are exhibiting increased brightness. Heating attributed to other solar bodies remains unproven.\nLast updated on 15 September 2010 by gpwayne."
  },
  {
   "title": "Polar bear numbers are increasing",
   "paragraph": "How will global warming affect polar bears?\nLink to this page\nWhat the science says...\nPolar bears are in danger of extinction as well as many other species.\nClimate Myth...\nPolar bear numbers are increasing\n“A leading Canadian authority on polar bears, Mitch Taylor, said: ‘We’re seeing an increase in bears that’s really unprecedented, and in places where we’re seeing a decrease in the population it’s from hunting, not from climate change.'” (Scotsman.com)\nPolar bears are found in the Arctic circle and surrounding land masses. There are 19 recognised subpopulations, and estimates place their numbers at about 20,000 to 25,000. Polar bears are classed as vulnerable by the World Conservation Union (IUCN) and listed as a threatened species under the US Endangered Species Act. Yet some claim that polar bear numbers have increased since the 1950s and are now stable. So what is the situation for this species?\nFirst of all, a few points need to be made about polar bear numbers:\nNobody really knows how many bears there were in the 1950s and 1960s. Estimates then were based on anecdotal evidence provided by hunters or explorers and not by scientific surveys.\nPolar bears are affected by several factors, including hunting, pollution and oil extraction. Most notably, hunting, particularly following the introduction of snowmobiles, airplanes and ice breakers, led to a huge decline in certain subpopulations. The introduction of the International Agreement on the Conservation of Polar Bears in 1973, which restricted or even banned hunting in some circumstances, consequently resulted in an increase in polar bear numbers.\nNot all subpopulations are affected to the same degree by climate change, and while some subpopulations are well studied, for others there is insufficient data to make broad statements about current and past numbers.\nWith this caveat in mind, what do the figures actually say? According to a 2009 report by the IUCN Polar Bear Specialist Group, of the 19 recognised subpopulations of polar bears, 8 are in decline, 1 is increasing, 3 are stable and 7 don’t have enough data to draw any conclusions. Figure 1 below compares the data for 2005 and 2009.\nFigure 1: Subpopulation status of polar bears for 2005 and 2009 (Source: Polar Bear Specialist Group)\nBoth habitat degradation and over-harvesting are responsible for the decline in some subpopulations. To understand why the IUCN and US Endangered Species Act consider polar bears to be at risk, it is important to look at how rising temperatures are likely to affect their habitat in the future. Polar bears are highly specialised mammals which rely heavily on sea ice for food and other aspects of their life cycle. Satellite data show that Arctic sea ice has been decreasing for the past 30 years, and projections show that this trend will continue as temperatures carry on rising. The changes in sea ice affect polar bears in several ways:\nThe early retreat of summer sea ice means that bears have less time to hunt and therefore less time to build up fat reserves.\nThe fragmentation and reduction in sea ice has several impacts. It forces the bears to swim longer distances, using up some of their fat reserves. It also reduces the number of seals, which are the bears’ main source of food, and impedes travelling and den making. And it also forces the bears to spend more time on land, with increased interactions with humans potentially leading to higher mortality.\nTo get an idea of the potential impacts of future climate change on polar bears, we can look at subpopulations found at the bears’ southern range, where habitat changes have been most noticeable so far. A good example is the western Hudson Bay subpopulation, which is one of the best studied. Here, ice floe break-up is taking place earlier than 30 years ago, effectively reducing the feeding period by about three weeks. As a result, the average weight of female polar bears has dropped by about 21% between 1980 and 2004, and the population declined by 22% between 1987 and 2004. In Alaska, there is evidence of increased cub mortality caused by a decline in sea ice.\nIn conclusion, the reason polar bears have been classed as threatened comes from the impacts of future climate change on the bears’ habitat. Current analysis of subpopulations where data is sufficient clearly shows that those subpopulations are mainly in decline. Further habitat degradation will increase the threats to polar bears.\nBasic rebuttal written by Anne-Marie Blackburn\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nA further website of interest is from WWF.\nLast updated on 22 July 2017 by pattimer. View Archives"
  },
  {
   "title": "Positive feedback means runaway warming",
   "paragraph": "Does positive feedback necessarily mean runaway warming?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nPositive feedback won't lead to runaway warming; diminishing returns on feedback cycles limit the amplification.\nClimate Myth...\nPositive feedback means runaway warming\n\"One of the oft-cited predictions of potential warming is that a doubling of atmospheric carbon dioxide levels from pre-industrial levels — from 280 to 560 parts per million — would alone cause average global temperature to increase by about 1.2 °C. Recognizing the ho-hum nature of such a temperature change, the alarmist camp moved on to hypothesize that even this slight warming will cause irreversible changes in the atmosphere that, in turn, will cause more warming. These alleged \"positive feedback\" cycles supposedly will build upon each other to cause runaway global warming, according to the alarmists.\" (Junk Science)\nSome skeptics ask, \"If global warming has a positive feedback effect, then why don't we have runaway warming? The Earth has had high CO2 levels before: Why didn't it turn into an oven at that time?\"\nPositive feedback happens when the response to some change amplifies that change. For example: The Earth heats up, and some of the sea ice near the poles melts. Now bare water is exposed to the sun's rays, and absorbs more light than did the previous ice cover; so the planet heats up a little more.\nAnother mechanism for positive feedback: Atmospheric CO2 increases (due to burning of fossil fuels), so the enhanced greenhouse effect heats up the planet. The heating \"bakes out\" CO2 from the oceans and arctic tundras, so more CO2 is released.\nIn both of these cases, the \"effect\" reinforces the \"cause\", which will increase the \"effect\", which will reinforce the \"cause\"... So won't this spin out of control? The answer is, No, it will not, because each subsequent stage of reinforcement & increase will be weaker and weaker. The feedback cycles will go on and on, but there will be a diminishing of returns, so that after just a few cycles, it won't matter anymore.\nThe plot below shows how the temperature increases, when started off by an initial dollop of CO2, followed by many cycles of feedback. We've plotted this with three values of the strength of the feedback, and you can see that in each case, the temperature levels off after several rounds.\nSo the climatologists are not crazy to say that the positive feedback in the global-warming dynamic can lead to a factor of 3 in the final increase of temperature: That can be true, even though this feedback wasn't able to cook the Earth during previous periods of high CO2.\nNote: A more detailed explanation is provided here.\nLast updated on 13 September 2010 by nealjking."
  },
  {
   "title": "Postma disproved the greenhouse effect",
   "paragraph": "Joseph Postma and the greenhouse effect\nLink to this page\nWhat the science says...\nJoseph Postma published an article criticizing a very simple model that nonetheless produces useful results. He made several very simple errors along the way, none of which are very technical in nature. In no way does Postma undermine the existence or necessity of the greenhouse effect.\nClimate Myth...\nPostma disproved the greenhouse effect\n\"Skeptics hope that Postma’s alternative thermal model will lead to the birth of a new climatology, one that actually follows the laws of physics and properly physical modeling techniques...Postma deftly shows how the systemically tautologous conjecture that is “back-radiative heating” just doesn't add up. We see how climatologists fudged the numbers to make it appear as if Earth actually raises its own temperature by having its own radiation fall back upon it - a conjecture contrary to fundamental physics.\" (John O'Sullivan)\nSome recent attention has recently been going around the web concerning a new “paper” done by Joseph E. Postma (PDF here) which claims to “…physically negate the requirement for a postulation of a radiative atmospheric greenhouse effect.”\nThe claims are of course extraordinary, along the lines of Gerlich and Tseuchner’s alleged falsification of the atmospheric greenhouse effect. As is often the case with these types of “skeptics,” the more extravagant the claim, the more obscure the publishing venue; in this case the host is Principia Scientific International, which according to the website “…was conceived after 22 international climate experts and authors joined forces to write the climate science bestseller, ‘Slaying the Sky Dragon: Death of the Greenhouse Gas Theory.’” Most rational people would stop here, but this is the Americanized age where we need to glorify everyone’s opinion and must provide rebuttals for everything, so here it goes:\nI ask that the reader have the paper open in a new window so they can follow along with this article.\nThe Foundations\nMost of Postma’s first 6 pages are actually correct. He describes the greenhouse effect through the so-called layer model, which is a simple way to break up the planet into a “surface” and an “atmosphere,” with outer space overlying the top layer. This model is described in many climate books such as Dennis Hartmann’s Global Physical Climatology, David Archer’s Understanding the Forecast, Marshall and Plumb’s Atmosphere, Ocean and Climate Dynamics, and radiation books like Grant Petty’s First Course in Atmospheric Radiation. I will say that I do not particularly like this model as a suitable introduction to the greenhouse effect. It is useful in many regards, but it fails to capture the physics of the greenhouse effect on account of making a good algebra lesson, and opens itself up to criticism on a number of grounds; that said, if you are going to criticize it, you need to do it right, but also be able to distinguish the difference between understood physics and simple educational tools.\nThe atmosphere in Postma’s paper is just a single slab, so he has two layers (atmosphere+surface), but in general you can have many atmospheric layers. He goes on to solve for the energy balance of each layer (see equations 11-14). RealClimate derived the same result in less than a page here.\nFigure 1: Layer model is Postma's paper. Click to Enlarge\nPostma actually doesn’t get the atmospheric radiative flux right. The emission is not σTa4, it is fσTa4, where f is the atmospheric emissivity/absorptivity (following his notation) and Ta is the atmospheric temperature. The emissivity is a unitless factor between 0 and 1 descrbing how good of an absorber/emitter the object is relative to an ideal body. f = 1 describes a blackbody. By Kirchoff's law, the absorptivity of a layer must be equal to the emissivity (at the same wavelength), Both right hand sides of equations 11 and 12 are thus wrong, but it turns out that those errors cancel each other out and he gets equation 14 right. The factor of 2 in Equation 12 comes about because the atmosphere emits both up and down, although Postma clearly doesn't know how to derive this result formally, based on later statements he makes about this. Toward the end of page 14 he says this is invalid since the atmosphere radiates in 3-D, not just up and down. In fact, the quantity σT4 refers not only to the total power output of an object (the rate of energy emission), but it also refers to isotropic (equally intense in all directions) radiation. The result σT4 is obtained if one assumes that a plane radiates uniformly over a hemisphere (for example, the domed \"half sphere\" field of vision that a human can see when you stand outside, with the base of that half-sphere being the surface you sre standing on; the other hemisphere is invisible (see this image).\nSo far, it is simple textbook stuff with not much promise.\nGeometry of the Global Energy Budget\nPostma then goes on to describe fictitious “boundary conditions.” In particular, he seems to have serious objections to the averaging of the solar radiative flux over the Earth. In essence, he would prefer we had one sun delivering 1370 W/m2 of energy to the planet, with a day side and a night side, noon and twilight, etc. instead of the simple model where we average 1370/4=342.5 W/m2 over the planet (so that the whole Earth is receiving the appropriate \"average\" solar radiation). The number becomes ~240 W/m2 when you account for the planetary albedo (or reflectivity).\nThe factor of 4 is the ratio of the surface area to the cross section of the planet, and is the shadow cast by a spherical Earth. It is therefore a geometrical re-distribution factor; it remains “4” if all the starlight is distributed evenly over the sphere; it is “2” if the light is uniformly distributed over the starlit hemisphere alone; with no re-distribution, the denominator would be 1/cosine(zenith angle) for the local solar flux.\nIn simple textbook models, we like to prefer explanations that get a point across, and then build in complexity from there (see Smith 2008 for descriptions on a rotating Earth). Of course, students who use this model are probably educated to the point where they know that day and night exist, and certainly GCMs have a diurnal cycle. The radiative calculations are done explicitly by accounting for the temperature distribution and absorber amount that is encountered at each grid box. Postma is simply tackling a non-issue, just as how people criticize the term “greenhouse effect” for not working like a glass greenhouse. Postma objects to teaching this simple model because it is not real. All that is done, however, is to use a brilliant and sophisticated technique, taught only to the geniuses among us, called averaging! And of course, simple models are used in any classroom...it is how we learn.\nBut, in actuality, the globally averaged solar re-distribution approximation is not bad when we use it to describe the temperature for planets like Earth or Venus. These planets have an atmosphere or ocean that transport heat effectively, especially Venus with virtually no day-to-night or pole-to-equator temperature gradient. The atmosphere and/or ocean help smooth the diurnal temperature difference very well. Therefore, when coming up with a temperature estimate, it is a great first approximation. If you want the local equilibrium temperature for an airless body like Mercury or the Moon (that does not transport heat), then you want to use the no-redistribution or hemisphere only solar factor. This is well-known (see e.g., Selsis et al 2007). On Mercury, there is no heat distribution and very little thermal inertia; before the sunrise the temperature on the surface is somewhere near 100 K (-173 °C) and by noon the temperature on the surface of Mercury rises to about 700 K (427 °C). This may also be relevant for tide-locked planets (very slow rotation since one side is always facing the host star, the other in perpetual darkness). Earth does not experience any such changes of the sort. On Venus, the variability is even less, and most of the planet is at around 735 K.\nSummary so far...\nTo summarize so far, Joseph E. Postma did not like a simple model of Earth’s radiative balance where we approximate the Earth as a sphere with uniform solar absorption. Of course, this is never done in climate modeling or in more detailed analyses appropriate for scholarly literature, so it is more an exercise in complaining about undergraduate education than an attempt to correct what he calls a “paradigm” in climatology. Nonetheless, the 0-D energy balance model is a useful approximation on Earth when coming up with an average emission temperature (~255 K), since air circulations and oceans tend to even out the diurnal temperature gradient on Earth, in addition to the thermal inertia provided by the system.\nVenus is More Optically Thick Than a One Layer Model Can Give You\nPostma starts by using Venus as a template for where the greenhouse model he is using breaks down. And indeed, he is right. His argument is that f (the emissivity) cannot possibly be greater than 1 (which is correct), and yet it must be in order to produce the Venus surface temperature in his Equation 29) Based on this, he then states that the standard greenhouse model does not work in general. The problem is that his Equation 29 assumes a one-layer atmosphere, which is an absurd assumption when you approach the extremely high optical thickness of Venus. Venus has a 90 bar atmosphere that has well over 90% carbon dioxide, some water vapor, and a greenhouse effect generated by suluric acid droplets and SO2. The radiative transfer on Venus works much differently than on Earth, owing in part to intense collisional broadening of CO2 molecules. A photon has an extremely difficult time escaping Venus, unable to do so until it reaches the very outer parts of its atmosphere.\nUsing the layer model, you would need many atmospheric layers to produce something close to Venus; with enough layers you would find that you could produce the surface temperature of Venus without violating conservation of energy. With just one perfect absorbing atmospheric layer, the surface temperature cannot exceed 21/4 times the emission temperature (Te=~230 K on Venus). But with two perfectly absorbing atmospheric layers, it can rise to 31/4Te. With three layers, the maximum temperature is 41/4Te, and so on. The reason the surface temperature is capped in this way is because the atmosphere itself must be emitting radiation and heats up when it absorbs photons from the surface, which in turn increases emission. If the atmospheric layer were instead a good infrared reflector (i.e., it has a high thermal albedo), then you could delay heat loss to space that way and increase the surface temperature well beyond this value. This could happen with CO2 clouds instead of H2O clouds, the latter are much more effective IR absorbers than IR scatterers, whereas the former could raise the IR albedo.\nIn essence, Postma stretches a simplified model to areas that it was never designed to go to, and then declares that its failure to work means the whole paradigm of the greenhouse effect is wrong. The incompetence is overwhelming. Postma is not done though, and decides to dig in further. His next argument is amusing, but perhaps a bit strange to follow, so I will try to explain.\nLapse Rate Confusion\nHe claims that observations of the atmospheric lapse rate (the rate at which temperature declines with height) disallow the greenhouse effect. His reasoning is that the atmosphere is at a fixed height. When greenhouse gases warm the surface, and cool the upper atmosphere, that height still remains fixed, but obviously the temperature difference between the bottom and top of the atmosphere must increase. Postma then claims that this necessarily implies that the lapse rate must have a greater slope than the theoretical value that he derived of about -10 K per kilometer (which is about right for a dry air parcel ascending). That is, if the atmospheric height remains fixed, and the temperature difference between bottom and top is increased, then the rate at which air cools with height must increase. Since this is not observed, then we have a problem, right?\nIn actuality, the atmospheric height is a distraction. The adiabatic lapse rate does not extend beyond the point where convection breaks down, which is the tropopause. The whole point of the greenhouse effect is that increasing atmospheric greenhouse gases does increase the “average” height at which emission to space takes place (and the tropopause increases in height too), so one IS allowed to extrapolate further down the adiabat to reach a higher surface temperature. On Venus, the optical thickness forces the tropopause to some 60 km altitude. Additionally, it is worth pointing out that greenhouse gases warm the upper troposphere, not cool it, but they do cool the stratosphere.\nFigure 1: Qualitative schematic of the old (blue) and new (e.g., after CO2 increase) temperature with height in a dry atmosphere. Moisture tends to enhance the tropical upper atmosphere warming relative to surface. Temperature increases to the right.\nTOA vs. Surface\nPerhaps just as crucial to all of this, Postma cannot get around the surface energy budget fallacy, which says that increased CO2 causes surface warming by just increasing the downward infrared flux to the surface. This problem is described in standard treatments of the greenhouse effect, which he does not seem to know exist, such as in Ray Pierrehumbert’s recent textbook. The primacy of the top of the atmosphere budget, rather than the surface energy budget, has been known at least since the work of Manabe in the 1960s (see also Miller, 2011 submitted)\nIn reality, the top of the atmosphere budget controls the surface temperature even more than the surface forcing, because the atmosphere itself is adjusting its outgoing radiation to space (and much of the radiation to space is originating in the upper atmosphere, owing to its IR opacity). Where the atmosphere is well-stirred by convection, the adjustment in temperature at this layer is communicated to the surface. I described this in more detail here. (As a side note, I hope people can bookmark the home page to that blog, which is run by a team of meteorologists, climatologists, and grad students in atmospheric science, at the University of Albany in NY, and we will be posting periodically on many different issues from ENSO to climate change to recent weather around the country).\nPostma runs into this mistake again when he claims that the low water vapor in hot deserts is a problem for greenhouse theory, but this is largely due to the lack of evaporation cooling, which is just one component of the surface energy budget, and nearly absent in a desert. This is one scenario where a detailed consideration of the surface budget is critical, as well as in other weakly coupled regimes.\nThe way CO2-induced warming really works in a well mixed atmosphere is by reducing the rate of infrared radiation loss to space. Virtually all of the surface fluxes, not just the radiative ones, should change in a warming climate, and act to keep the surface and overlying air temperature relatively similar. The back-radiation will indeed increase in part because of more CO2 and water vapor, but also simply because the atmosphere is now at a higher temperature. But if the lower atmosphere was already filled with water vapor or clouds to the point where it emitted like a blackbody (at its temperature), increasing CO2 would not directly increase downward emission before temperature adjustment, but would nonetheless warm the planet by throwing the TOA energy budget out of whack.\nConclusions\nIn summary, Joseph Postma published an article criticizing a very simple model that nonetheless produces useful results. He made several very simple errors along the way, none of which are very technical in nature. More sophisticated models are obviously designed to handle the uneven distribution of solar heating (which is why we have weather!); nonetheless, the educational tools are useful for their purpose, and in no way does Postma undermine the existence or necessity of the greenhouse effect. Without a greenhouse effect, multiple studies have shown that the Earth collapses into a frozen iceball (Pierrehumbert et al., 2007; Voigt and Marotzke 2009, Lacis et al 2010) and indeed, after an ice-albedo feedback, plummets below the modern effective temperature of 255 K. This work makes extraordinary claims and yet no effort was made to put it in a real climate science journal, since it was never intended to educate climate scientists or improve the field; it is a sham, intended only to confuse casual readers and provide a citation on blogs. The author should be ashamed.\nLast updated on 1 January 2015 by Chris Colose. View Archives"
  },
  {
   "title": "Record high snow cover was set in winter 2008/2009",
   "paragraph": "Long-term trend in snow cover in rapid decline\nLink to this page\nWhat the science says...\nWinter snow cover in 2008/2009 was not a record high - in fact, it was quite average. But more importantly, the long-term trend in spring, summer, and annual snow cover is one of rapid decline. As a result, the planet as a whole is becoming less reflective and absorbing more sunlight, which is accelerating global warming.\nClimate Myth...\nRecord high snow cover was set in winter 2008/2009\nCherries Don't Grow in Winter\nThe fundamental flaw in this particular myth is rather obvious - cherrypicking of short-term data. In this case choosing a 3-month period to claim that snow cover is not falling. Quite obviously we cannot determine a long-term trend from 3 months' worth of data.\nOn top of that, this is a very peculiar 3-month period to evaluate. Why is winter snow cover more indicative of the long-term trend than fall, spring, or summer? In fact, the best way to support the false claim that global snow cover is not declining is to ignore the hotter months of the year.\nWhy Should Snow Cover Decline?\nIn the near future, global warming will not necessarily result in less winter snowfall globally. Some regions will experience more winter precipitation, which in many regions will fall primarily as snow even if temperatures rise a few degrees, and some regions will receive less. The top map in the figure below shows climate model projections of future winter precipitation changes from the 2007 IPCC report.\nIn short, we don't necessarily expect winter snow fall or snow cover to decline in the short-term in a warming world. What we do expect is for this snow cover to melt earlier as spring arrives sooner, and at higher temperatures.\nWhat Does the Data Show?\nThe Rutgers University Global Snow Lab has perhaps the most commonly-used snow cover data, though only for the Northern Hemisphere (NH). In fact, Rutgers does the work for us in plotting seasonal NH snow extents. Here is their plot for the winter:\nAs you can see, the data do not support this myth. The largest winter NH snow extent was in 1977-78. 2007-08 had the third-highest extent. The winter of 2008-09 comes in about 23rd place out of 44 winters on record. The winter of 2009-10 did come in second, however.\nBut more importantly, Rutgers also plots the spring NH snow extent:\nThis paints quite a different picture. As expected in a warming world, the spring NH snow cover extent is declining quite significantly. Rutgers also provides weekly and monthly data for their entire record in tabular format. We can use this to compare snow cover extent in every season, as well as annually:\nAs expected, there is again a significant long-term drop in NH snow extent, this time looking at all the available data. The trend demonstrates a decline of approximately 1.3 million square kilometers from 1972 to 2010. This decline is confirmed by Déry and Brown (2007), which found a 1.28 million square kilometer decline in NH snow cover from 1972 to 2006. In other words, NH snow extent is declining by approximately 34,000 square kilometers per year.\nWhy Does it Matter?\nSnow is white and highly reflective. When it melts, it reveals the much darker ground beneath. Thus the larger the snow cover extent and the longer it lasts into the spring, the more solar energy the planet will reflect. As snow cover declines, the planet absorbs more solar energy (this loss of reflectivity is known as decreasing albedo), accelerating global warming as a result.\nIn fact, a new study by Flanner et al. (2011) has found that so far, snow cover is declining more rapidly and causing more global warming than climate models expect.\n\"We find that cyrospheric cooling declined by 0.45 W m−2 from 1979 to 2008, with nearly equal contributions from changes in land snow cover and sea ice. On the basis of these observations, we conclude that the albedo feedback from the Northern Hemisphere cryosphere falls between 0.3 and 1.1 W m−2 K−1, substantially larger than comparable estimates obtained from 18 climate models.\"\nFlanner et al. also show that the radiative forcing from snow cover in the winter months is relatively small, whereas the cooling effect is largest in the spring and summer months (March through July in the NH). This is because in winter, the days are shorter and the sunlight weaker, so albedo has less impact. This again confirms that if we want to evaluate the impact of changing snow cover on the climate, we should be looking at the spring and summer months, not the winter, as Monckton does. Flanner et al. find that the change in snow radiative forcing in the spring and summer months has been significantly positive (less cooling) from 1979 to 2008.\nSo not only is the suggestion that snow cover is not declining incorrect, but in fact it's declining faster than climate scientists anticipated, and contributing significantly to global warming as a result. This myth relies on not only cherrypicking 3 months out of a 40 year record (0.6% of the available data), but also looks at the least relevant months of the year (winter).\nLast updated on 3 January 2012 by dana1981. View Archives"
  },
  {
   "title": "Record snowfall disproves global warming",
   "paragraph": "Does record snowfall disprove global warming?\nLink to this page\nWhat the science says...\nTo claim that record snowfall is inconsistent with a warming world betrays a lack of understanding of the link between global warming and extreme precipitation. Warming causes more moisture in the air which leads to more extreme precipitation events. This includes more heavy snowstorms in regions where snowfall conditions are favourable. Far from contradicting global warming, record snowfall is predicted by climate models and consistent with our expectation of more extreme precipitation events.\nClimate Myth...\nRecord snowfall disproves global warming\n\"Global warming continues to cause trouble to this tiny, blue planet: A new record was set Wednesday when Chicago had its ninth consecutive day of measurable snowfall and Flint, Michigan, broke a 95-year-old record early Wednesday morning when the temperature plummeted to a frigid 19 below zero. The previous record? Minus 10, set in 1914. Meanwhile, it will likely to continue to snow in Chicago in the coming days. Global warming sure is… cold!\" (Michael van der Galien)\nThe 2009/2010 winter saw a number of dramatic, record breaking snowstorms. Early February saw two \"once in a 100 years\" snowstorms hit Philadelphia, now being dubbed \"Snowmageddon\". Does record snowfall prove that global warming isn't happening? What do observations say? 2009 was the second hottest year on record. January 2010 was the hottest January in the UAH satellite record. Satellites data indicates last month was the second hottest February in the satellite record. Observations tell us that rumours of global warming's death have been greatly exaggerated.\nFigure 1: UAH satellite measurement of near surface temperature. January 2010 is the hottest January in the satellite record. February 2010 is the second hottest February in the satellite record. Click on the image for larger version.\nIf global warming is still happening, why are some areas experiencing record snowfall events? As climate warms, evaporation from the ocean increases. This results in more water vapour in the air. Globally, atmospheric water vapour has increased by about 5% over the 20th century. Most of the increase has occurred since 1970 (IPCC AR4 3.4.2.1). This is confirmed by satellites that find the total atmospheric moisture content has been increasing since measurements began in 1988 (Santer 2007).\nFigure 2: Change in water vapor percentage relative to the 1988 to 2004 period over the global ocean plus linear trend, measured by satellite (IPCC AR4 3.4.2.1).\nThe extra moisture in the air is expected to produce more precipitation, including more extreme precipitation events. Observations bear this out. A study of precipitation trends over the United States found that heavy precipitation events (over 50mm in a day) have increased 20% over the 20th Century (Groisman 2004). Most of this increase occured after 1970. Various analyses of precipitation over the globe have similarly found a widespread increase in heavy precipitation days since 1950 (Alexander 2006, Groisman 2006).\nFigure 3: Global number of days per year when precipitation was greater than 10mm per day, expressed as an anomaly from the 1961 tp 1990 reference period (Alexander 2006).\nSnowstorms can occur if temperatures are in the range of -10°C to 0°C. Global warming decreases the likeliness of snowstorm conditions in warmer, southern regions. However, in northern, colder regions, temperatures are often too cold for very heavy snow so warming can bring more favourable snowstorm conditions (Kunkel 2008). This is borne out in observations. Over the last century, there has been a downward trend in snowstorms across the lower Midwest, South and West Coast. Conversely, there's been an increase in snowstorms in the upper Midwest East, and Northeast with the overall national trend also upwards (Changnon 2006).\nTo claim that record snowfall is inconsistent with a warming world betrays a lack of understanding of the link between global warming and extreme precipitation. Global temperatures in the last few months of record snowfall are some of the hottest on record. Warming causes more moisture in the air which leads to more extreme precipitation events. This includes more heavy snowstorms in regions where snowfall conditions are favourable. Far from contradicting global warming, record snowfall is predicted by climate models and consistent with our expectation of more extreme precipitation events.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 26 October 2016 by pattimer. View Archives"
  },
  {
   "title": "Removing all CO2 would make little difference",
   "paragraph": "Removing all CO2 would cool Earth by 30 degrees C\nLink to this page\nWhat the science says...\n75% of the greenhouse effect is caused by water vapor and clouds, which rain out of the atmosphere if it cools. This makes water vapor a strong positive feedback to any change in non-condensing greenhouse gases. CO2 constitutes 80% of the non-condensing greenhouse gas forcing. Removing CO2 would remove most of the water, cancelling most of the greenhouse effect and cooling the Earth by 30 C.\nClimate Myth...\nRemoving all CO2 would make little difference\n\"Cloud cover in models is poorly treated and inaccurately predicted. Yet clouds reflect about seventy-five watts per square meter. Given that a doubling of carbon dioxide would change the surface heat flux by only two watts per square meter, it is evident that a small change in cloud cover can strongly affect the response to carbon dioxide.\" (Richard Lindzen)\nThe ability for CO2 to warm the surface of a planet through the absorption of infrared radiation is well known. What is much less appreciated, however, is just how effective of a gas it is in maintaining the greenhouse framework that helps to characterize the modern climate.\nThe question of how the climate would change in a completely CO2-free atmosphere was brought up recently in a testimony to the subcommittee of the House Science and Technology Committee. An answer was provided by MIT scientist Dr. Richard Lindzen, who suggested such a hypothetical removal of all the CO2 in the air would translate into a global cooling of about 2.5 degrees, presumably in Celsius (see here, about 47 minutes into the video). Dr. Cicerone, who was also on the panel, expressed disagreement although didn't really provide a better answer of his own.\nIt may be that Lindzen was just giving an estimate off the top of his head in a hearing, but the question is nonetheless interesting because it provides perspective on how to decompose the greenhouse effect into its individual components.\nOne might naively suggest that removing all the CO2 in the atmosphere would produce a similar temperature change as doubling CO2, just in the other direction. Even without considering the complex nature of feedbacks, this conclusion would be wrong however. The radiative forcing generated by CO2 change goes as the logarithm of the concentration, and thus removing the whole CO2 inventory would generate a much larger impact than had you doubled it. In fact, when you get to very small amounts of CO2 (on the order of a few parts per million), the radiative forcing is much faster than logarithmic since you’re opening up new opacity in the central 15 micron band where Earth is strongly emitting (it is for this reason methane changes are commonly cited as being “more powerful than CO2,” which is only true molecule-for-molecule in the modern atmosphere, as methane is starting off at much lower background concentrations. Methane has no intrinsic properties that make this the case, and when comparing with CO2 side-by-side is even worse of a greenhouse gas).\nEven this qualitative reasoning might have led Lindzen to a better ballpark answer than just 2-3 C, which would be even too small if feedbacks were completely neutral. It’s still qualitative however, and getting a better handle on the question requires radiative transfer modeling and simulations that can adequately handle a greenhouse-free atmosphere without blowing up. From here, the first question one can ask is how much of the total greenhouse effect is provided by CO2? This is dependent on the background atmosphere itself, since overlapping absorption with other molecules will give a different number than if that molecule were all by itself, even at the same concentration. When you do these calculations with overlapping absorption, it was found by Schmidt et al (2010) that CO2 contributes to about 20% of the modern greenhouse effect. The greenhouse effect can be defined energetically as:\nwhere Ts and Te are the surface (288 K) and emission temperature (255 K), respectively. σ is the Stefan-Boltzmann constant. It follows that CO2 alone provides nearly 30 W/m2 of radiative forcing, much larger than the ~4 W/m2 when you double it in the modern climate. Assuming the same climate sensitivity, Lindzen’s estimate of a 2.5°C drop for a -30 W/m2 forcing would imply that currently doubling CO2 would warm the planet by only a third of a degree at equilibrium, which is well outside the bounds of IPCC estimates and even very low by most skeptical standards.\nAt this point, we need to incorporate feedbacks into the problem to get a better feel for how nature really operates. It is often mentioned as a somewhat routine talking point that “water vapor is the most important greenhouse gas.” This is true in the sense that it makes up the bulk of the infrared opacity in our atmosphere (~50%) and clouds another 25%, but because it condenses at Earth-like temperatures is very short-lived in the air as it goes through the evaporation, condensation, and precipitation cycle. Because of the temperature-dependency on the pressure at which water saturates, this makes water vapor a feedback. Another interpretation would then be that the non-condensing greenhouse gases (chiefly CO2, only about 5% of the greenhouse effect is provided by ozone, methane, N2O, etc) are the “most important,” since they provide enough warming so as to produce the skeleton by which the water vapor greenhouse effect can be strong enough.\nThere have been a number of studies which examine the evolution of the climate system with no CO2 in the atmosphere. Such experiments are described for example in Pierrehumbert et al (2007) , or by Voigt and Marotzke (2009). From these papers, one can trigger a full snowball Earth with a sufficient reduction in atmospheric CO2. A substantial reduction in water vapor (shown below, from Lacis et al (2010) as well as increase in the surface albedo are important feedbacks here, showing that removing the non-condensing greenhouse gases (mostly CO2) in the atmosphere can collapse nearly the entire terrestrial greenhouse effect. What’s more, since the albedo increases substantially, the total greenhouse effect can be thought of as providing even more than 33 K of warming relative to Earth’s blackbody emission temperature. In the Lacis et al experiments, removing the CO2 from the atmosphere generates a cooling of around 30 C, an order of magnitude difference from Lindzen's answer.\nFigure 1: Time evolution of global surface temperature, TOA net flux, column water vapor, planetary albedo, sea ice cover, and cloud cover, after zeroing out all of the noncondensing GHG’s. From Lacis et al (2010)\nWhy might we care about such a hypothetical situation anyway? Aside from Lindzen, there are a number of interesting applications to low solar or low CO2 cases. A concern with “large” climate changes (i.e., on the scale of snowball Earths or runaway greenhouses) is that there’s bifurcation (loosely, tipping points) in the system. This is to say that one can conceivably draw down CO2 from the atmosphere and trigger a snowball, although it would take extremely high values of CO2 (much higher than the original amount) to get back out of this glaciated state. In case skeptics bring it up, this is at least one way to have an ice-covered planet with very high CO2 levels. Indeed, precisely how to escape a full-blown snowball is one of the grand unresolved questions in climate science. Once you're out of a snowball however, you're left with a very hot climate until weathering can draw down CO2 to moderate values. Shown below (from Pierrehumbert et al., 2011, accepted) is a bifurcation sketch of the temperature as a function of the CO2 content in the atmosphere, with a lower solar insolation than today (the paper is a review article on the Neoproterozoic climate). One can do a similar type of diagram against the incoming shortwave radiation, but in any case the presence of an albedo feedback makes multiple temperature solutions possible, even for the same incoming stellar radiation and greenhouse effect. For example, if the Earth were magically ice-covered today, this would be a completely stable situation and there would be no tendency to escape that state unless the greenhouse effect was substantially enhanced or the sun got brighter. This is a big problem in planetary habitability studies, especially if a planet succumbs to a snowball fate early in its history when the sun is faint. Once you begin to melt ice however, the temperature jumps rapidly to a very hot solution.\nThis is germane to a recent paper by Rosing et al (2010) which purported to show that the \"faint sun\" paradox can be resolved through a lower albedo rather than through a substantially enhanced greenhouse effect. This paper is interesting, and an example of good scientific skepticism, in the sense that the authors are proposing a new idea for a subject still open for research. The idea is problematic however, since even for Neoproterozoic insolation, you kick over into a snowball at about 3x present CO2, and the situation is even worse for early Earth insolation, which is some 20-25% lower than today. There is no mechanism to adjust the albedo in such a way as to offset temperature changes, whereas the long-term silicate weathering thermostat acts as a negative feedback between CO2 concentations and temperature over geologic time.\nFigure 2: Bifurcation diagram for a zero-dimensional energy balance model. Calculations performed with L = 1,285Wm−2. For CO2 inventories up to approximately 1,000 Pa, the inventory can be converted to a mixing ratio in parts per million by volume (ppmv) by multiplying by 6.6. The CO2 concentrations on the upper horizontal axis are stated as fractions (e.g., 0.0065 corresponds to6,500 ppmv). The vertical dashed lines marked “Left” and “Right” indicate the left and right boundaries of the hysteresis loop for the case with ice albedo equal to 55%. Adopted From Pierrehumbert et al., 2011, accepted\nLindzen has argued for a relatively insensitive climate system in the past, in which case it would be difficult to explain the magnitude of large climate changes in the past, ranging from snowballs, the PETM, glacial-interglacial cycles, etc. However, arguing that the climate would cool only be 2.5 degrees when you remove all the CO2 in the atmosphere is really just a made up number and ignored several articles on the subject that show otherwise. Just the opposite, evidence shows that CO2 provides the building block for the terrestrial greenhouse effect, both because it absorbs strongly near the peak emission for Earth, and because it allows Earth to be warm enough to sustain a powerful water vapor greenhouse effect.\nLast updated on 14 May 2011 by Chris Colose."
  },
  {
   "title": "Renewable energy investment kills jobs",
   "paragraph": "Renewable energy creates more jobs than fossil fuels\nLink to this page\nWhat the science says...\nThe claim that 2.2 conventional jobs are destroyed for every new job created in the alternative energy industry is based on a study which relies on incorrect numbers, cherrypicked dates, faulty theory, flawed methodology, and is disproven by real-world examples. In reality, renewable energy investment and development tends to create more jobs than fossil fuel energy.\nClimate Myth...\nRenewable energy investment kills jobs\n\"for every renewable energy job that the State manages to finance, Spain’s experience...reveals with high confidence, by two different methods, that the U.S. should expect a loss of at least 2.2 jobs on average\" (Gabriel Calzada Álvarez)\nA study published by a Spanish economist claims that investment in renewable energy will result in a net job loss:\n\"for every renewable energy job that the State manages to finance, Spain’s experience...reveals with high confidence, by two different methods, that the U.S. should expect a loss of at least 2.2 jobs on average\"\nThis study was authored by Gabriel Calzada Álvarez, who happens to be the founder of one libertarian think tank and a fellow of a second, which in recent years received funding from Exxon Mobil. But of course despite this potential conflict of interest, we will evaluate the study's claims on their own merits (or more accurately as we will see, the lack thereof).\nUnderestimating Green Job Creation\nOne key flaw in the Calzada study is in its estimate of green job creation through renewable energy projects. Obviously when evaluating the net effect on Spanish employment, this is a key figure to get right. According to Calzada (page 25),\n\"Since 2000, the renewable subsidies have created less than 50,200 jobs.\"\nHowever, his reference for this figure is unclear. Calzada cites an Instituto Sindical de Trabajo, Ambiente y Salud (ISTAS) study for the beakdown of those jobs (percentage of construction, maintenance, administration, etc.), but not for the 50,200 job claim itself. The number seems to stem from estimated predictions made in 2003 of what the renewable energy job creation would turn out to be, rather than recent estimates of what the numbers did turn out to be.\nIn reality, as noted by a United Nations Environment Programme (UNEP) study, the ISTAS actually estimates that 188,000 green jobs have been created in Spain, the majority since 2000. Getting this figure so wrong invalidates Calzada's conclusions by itself.\nFalling Unemployment in Navarre\nNavarre is a region in Spain which has invested heavily in renewable energy, currently obtaining 65% of its energy from renewable sources including 993 megawatts (MW) of wind and almost 100 MW of solar photovoltaic power. So if Calzada's argument is correct and investing in renewable energy kills jobs, we would expect to see high unemployment in this region.\nThe Regional Minister of Innovation, Enterprise and Employment for the Government of Navarre, José María Roig Aldasoro, wrote a letter in response to Calzada's study, in which he noted:\n\"In Navarre, the development of renewable energies, and above all wind energy, has created wealth, employment and technological development, and I can assert that this can be achieved in any other region or country.\nOur region’s GDP is among the three highest in Spain, participation by the industrial sector is 12 points higher than the entire country’s, and for many years Navarre has had unemployment rates inferior to Spain’s. Before the beginning of the current world crisis our region enjoyed full employment. Now, after the strong economic and employment crisis that affects Spain in particular, Navarre maintains itself as the Spanish region with the least unemployment.\"\nAldasoro went on to note that in 1994, when the first wind farm was erected in Navarre, unemployment in the region was at 12.8%. As more and more renewable energy was installed and worker training centers were opened, the unemployment rate consistently dropped, reaching a level of under 5% in 2007. Navarre provides a real-world example which is hard to jive with Calzada's claims of job destruction.\nCherrypicking\nAnother letter was published in response to the Calzada report by Jesús Caldera, the vice president of the IDEAS Foundation and former minister for public works, and Carlos Mulas-Granados, the executive director of the IDEAS Foundation and former economic advisor to Prime Minister Zapatero. Caldera and Mulas-Granados note that the Calzada report suffers from a common tactic which Skeptical Science readers will recognize: cherrypicking.\n\"Professor Calzada tries to find a long-term trend, but only cites employment data for the last year during Spain’s serious recession. He argues that solar energy has destroyed 15,000 jobs in the last year, but neglects to cite official figures showing an increase in this job sector of about 500% in the preceding three years. The loss he refers to is thus nothing more than a minor downturn in an economy that is troubled by the recent economic crisis....This is a report which fails to meet even the minimum standards of academic integrity. But worst of all, Professor Calzada’s report ignores – or hides - the positive figures in net employment creation of other renewable energy sectors, such as windmills, where Spain has truly become a world leader.\"\nFaulty Theory\nThe Calzada report explains the theory behind its faulty numbers on page 37:\n\"the resources used to create “green jobs” must be obtained from elsewhere in the economy. Therefore, this type of policy tends to create not just a crowding-out effect but also a net destruction of capital insofar as the investment necessary must be subsidized to a great extent and this is carried out by absorbing or destroying capital from the rest of the economy.\nThe money spent by the government cannot, once committed to “green jobs”, be consumed or invested by private parties and therefore the jobs that would depend on such consumption and investment will disappear or not be created.\"\nIn short, Calzada argues that government investment \"crowds out\" private investment, which he claims is more efficient at job creation, and thus any public investment in renewable energy must necessarily result in job destruction. In reality, there are only a few circumstances in which this \"crowding out\" argument holds true; generally when the economy's resources are being fully utilized, which is rarely the case. It's certainly not true in today's stagnant economic conditions, when private investment and growth is low. Under these conditions, public investment provide jobs to the unemployed without \"crowding out\" private investment. The study also fails to take infrastructure improvements into account, which improves private-sector performance by raising average productivity.\nFlawed Methodology\nA white paper from the US National Renewable Energy Laboratory (NREL) describes the many fundamental flaws in the Calzada paper methodologies, summarizing the study as follows:\n\"The analysis by the authors from King Juan Carlos University represents a significant divergence from traditional methodologies used to estimate employment impacts from renewable energy. In fact, the methodology does not reflect an employment impact analysis. Accordingly, the primary conclusion made by the authors – policy support of renewable energy results in net jobs losses – is not supported by their work.\"\nThe NREL paper also provides a list of recent studies in Europe as a whole (including Spain) and Germany in particular have found that public investment in renewable energy development results in a net positive impact on employment.\n\"recent research has found that it is only when conventional energy prices are forecast to be very low that net employment impacts from [renewable energy] investments are negative.\"\nRenewable Energy Creates More Jobs than Fossil Fuels\nCalzada's argument is also directly contradicted by reality, because renewable energy investment and development tends to create more jobs than fossil fuel energy because a larger share of renewable energy expenditures go to manufacturing equipment, installation, and maintenance, all of which are typically\nmore labor-intensive than extracting and transporting fossil fuels.\nIndeed a 2004 UC Berkeley study concluded:\n\"Across a broad range of scenarios, the renewable energy sector generates more jobs than the fossil fuel-based energy sector per unit of energy delivered (i.e., per average megawatt).\"\nThe study found that implementing a Renewable Portfolio Standard and investing in various types of renewable energy would create approximately twice as many jobs in the USA by 2020 as investing in coal and natural gas. Similarly, a 2001 Renewable Energy Policy Project report found that wind and solar photovoltaic investments lead to at least 40% more jobs per dollar than coal.\nIt's a complicated comparison, because renewable energy sources tend to be more expensive than fossil fuel energy. Thus hypothetically, the extra money invested in renewable energy could have been spent elsewhere to create new jobs in a different sector of the economy. However, fossil fuel energy is also artificially cheap because its price does not account for various external costs like climate change and impacts on public health. When accounting for all factors, it's likely that renewable energy results in more jobs per dollar invested than fossil fuels.\nCalzada is Wrong\nAs shown above, the Calzada paper is flawed in almost every conceivable way: it relies on incorrect numbers, cherrypicked dates, faulty theory, flawed methodology, and is disproven by real-world examples. In reality, investment in renewable energy results in a net positive effect on employment.\nLast updated on 13 June 2011 by dana1981."
  },
  {
   "title": "Renewable energy is too expensive",
   "paragraph": "The true cost of fossil fuels\nLink to this page\nWhat the science says...\nWhen you account for the effects which are not reflected in the market price of fossil fuels, like air pollution and health impacts, the true cost of coal and other fossil fuels is higher than the cost of most renewable energy technologies.\nClimate Myth...\nRenewable energy is too expensive\n\"[Wind energy] is a more expensive way of producing energy than the alternative.\" (David Montgomery)\nDue to its abundance and low market price, coal combustion is the largest source of energy production in the world, accounting for 40% of all electricity worldwide. In the USA it accounts for 45% of electricity generation, and approximately 75% in Australia.\nUnfortunately, coal combustion is a major contributor to global greenhouse gas emissions as well, accounting for 30% of total anthropogenic carbon dioxide (CO2) emissions worldwide, and 72% of CO2 emissions from global power generation. In addition, non-power generation uses increase its contribution to global human CO2 emissions to a whopping 41% (as of 2005).\nMany people prefer coal combustion to renewable energy because it seems to be cheaper. However, when accounting for the true costs of coal power, most renewable energy sources are actually significantly cheaper in the long-run.\nCoal Externalities\nA major problem with coal is that its full costs are not reflected in its market price, and thus while we may seemingly purchase and burn coal cheaply, in reality we are paying a much higher cost in the long run, if we look at the big picture. Economists refer to the impacts on human and environmental health which are not reflected in the price of coal as \"externalities\". Those who benefit from the seemingly cheap electricity don't pay for these externalities directly, but the public eventually has to pay in the form of medical bills, environmental cleanups, etc.\nA 2013 report published by the International Monetary Fund concluded that global fossil fuel subisides amount to $1.9 trillion annually. $1.4 trillion of this is due to externalities, $800 billion due to climate change. This estimate is based on a conservative social cost of carbon of $25 per tonne of CO2 emitted. An arguably more realistic estimate of $100 per tonne of CO2 would bring global fossil fuel subsidies to over $4 trillion per year, with $3.2 trillion due to climate change.\nIn a report published in the Annals of the New York Academy of Sciences, Epstein et al. (2011) do a full cost accounting for the life cycle of coal, taking these externalities into account. Among the factors included in this analysis were:\ngovernment coal subsidies\nincreased illness and mortality due to mining pollution\nclimate change from greenhouse gas emissions\nparticulates causing air pollution\nloss of biodiversity\ncost to taxpayers of environmental monitoring and cleanup\ndecreased property values\ninfrastructure damages from mudslides resulting from mountaintop removal\ninfrastructure damage from mine blasting\nimpacts of acid rain resulting from coal combustion byproducts\nwater pollution\nMost of these external factors do not apply to most renewable energy sources. The majority of the externality costs come from reduction in air quality, contribution to climate change, and impacts to public health. Epstein et al. find that the total cost of these externalities ranges from approximately 9 to 27 cents per kilowatt-hour (kWh) of electricity generated, with a median of approximately 18 cents per kWh. The authors note that this is a conservative estimate, because they have not accounted for every associated impact.\nFigure 1: Coal externalized cost (cents per kWh) from Epstein et al. (2011)\nAnother study by economists Muller, Mendelsohn, and Norhaus (MMN11) looked at just the external costs associated with the damage done by air pollution, and arrived at a best estimate of 3.6 cents per kWh of external costs, despite being unrealistically conservative (Figure 2).\nFigure 2: Average US coal electricity price vs. MMN11 and Epstein 2001 best estimate coal external costs.\nCost Comparison\nThe US Energy Information Administration provides a comparison of levelized costs for different power generation sources. Levelized cost represents the present value of the total cost of building and operating a generating plant over a period of time, and reflects overnight capital cost, fuel cost, operation and maintenance costs, financing costs, and an assumed utilization rate for each plant type. To convert from dollars per megawatt-hour to cents per kWh, move the decimal point in the table below one spot to the left (for example, conventional coal is 9.48 cents per kWh on average).\nAs you can see, the externalities are sufficient to triple the cost of coal power, if they were reflected in its price. If we include the coal externalities, it increases the levalized costs to approximately 18 to 28 cents per kWh, which is more than hydroelectric, onshore wind, geothermal, biomass, nuclear, natural gas, and on par with solar photovoltaic and solar thermal (whose costs are falling rapidly), and offshore wind. Suddenly coal doesn't look like such a good deal.\nRecommendations\nEpstein et al. conclude by offering a number of recommendations:\nComprehensive comparative analyses of life cycle costs of all electricity generation technologies and practices are needed to guide the development of future energy policies.\nBegin phasing out coal and phasing in cleanly powered smart grids, using place-appropriate alternative energy sources.\nA healthy energy future can include electric vehicles, plugged into cleanly powered smart grids; and healthy cities initiatives, including green buildings, roof-top gardens, public transport, and smart growth.\nAlternative industrial and farming policies are needed for coal-field regions, to support the manufacture and installation of solar, wind, small-scale hydro, and smart grid technologies. Rural electric co-ops can help in meeting consumer demands.\nWe must end mountaintop removal (MTR) mining, reclaim all MTR sites and abandoned mine lands, and ensure that local water sources are safe for consumption.\nFunds are needed for clean enterprises, reclamation, and water treatment.\nFund-generating methods include: maintaining revenues from the workers’ compensation coal tax; increasing coal severance tax rates; increasing fees on coal haul trucks and trains; reforming the structure of credits and taxes to remove misaligned incentives; reforming federal and state subsidies to incentivize clean technology infrastructure.\nTo transform our energy infrastructure, we must realign federal and state rules, regulations, and rewards to stimulate manufacturing of and markets for clean and efficient energy systems. Such a transformation would be beneficial for our health, for the environment, for sustained economic health, and would contribute to stabilizing the global climate.\nReal World Data\nIn the USA, there is no correlation between state renewable electricity production and electricity price (Figure 3) or renewable production and electricity price increase over the past two decades (Figure 4).\nFigure 3: State renewable (excluding hydroelectricity) electricity percentage of total electricity generation vs. electricity price (blue diamonds) with a linear trend (black line). Data from EIA (here and here).\nFigure 4: State renewable (excluding hydroelectricity) electricity percentage of total electricity generation vs. the percent annual increase in electricity price 1990—2011 (blue diamonds) with a linear trend (black line). Data from EIA (here and here).\nIf deploying renewable energy does not raise electricity prices, then clearly it is not an expensive proposition.\nBottom Line\nUltimately it's a significant problem that we rely so heavily on coal to meet our energy needs due to its artificially low market price. It's like eating junk food for every meal. It's cheap, it tastes good, but it's not healthy and eventually you'll pay the price through poor health, high medical bills, and a shortened lifespan.\nWe may not pay the costs of climate change, lost biodiversity, air and water pollution, adverse health effects, etc. up front, but we do have to pay them eventually. We need to follow the recommendations of Epstein et al., transform our energy infrastructure, and move away from our dependence on coal and other fossil fuels.\nLast updated on 14 July 2018 by dana1981. View Archives"
  },
  {
   "title": "Renewables can't provide baseload power",
   "paragraph": "Can renewables provide baseload power?\nLink to this page\nWhat the science says...\nSelect a level... Intermediate Advanced\nAlthough renewable energy does not necessarily need to provide baseload power in the short-term, there are several ways in which it can do so. For example, geothermal energy is available at all times, concentrated solar thermal energy has storage capability, and wind energy can be stored in compressed air.\nClimate Myth...\nRenewables can't provide baseload power\nDoes Renewable Energy Need to Provide Baseload Power?\nA common myth is that because some types of renewable energy do not provide baseload power, they require an equivalent amount of backup power provided by fossil fuel plants. However, this is simply untrue. As wind production fluctuates, it can be supplemented if necessary by a form of baseload power which can start up or whose output can be changed in a relatively short period of time. Hydroelectric and natural gas plants are common choices for this type of reserve power (AWEA 2008). Although a fossil fuel, combustion of natural gas emits only 45% as much carbon dioxide as combustion of coal, and hydroelectric is of course a very low-carbon energy source.\nThe current energy production structure consists primarily of coal and nuclear energy providing baseload power, while natural gas and hydroelectric power generally provide the variable reserves to meet peak demand. Coal is cheap, dirty, and the plant output cannot be varied easily. It also has high initial investment cost and a long return on investment time. Hydroelectric power is also cheap, clean, and good for both baseload and meeting peak demand, but limited by available natural sources. Natural gas is less dirty than coal, more expensive and used for peak demand. Nuclear power is a low-carbon power source, but with an extremely high investment cost and long return on investment time.\nRenewable energy can be used to replace some higher-carbon sources of energy in the power grid and achieve a reduction in total greenhouse gas emissions from power generation, even if not used to provide baseload power. Intermittent renewables can provide 10-20% of our electricity, with hydroelectric and other baseload renewable sources (see below) on top of that. Even if the rapid growth in wind and other intermittent renewable sources continues, it will be over a decade before storage of the intermittent sources becomes a necessity.\nRenewable Baseload Energy Sources\nOf course in an ideal world, renewable sources would meet all of our energy needs. And there are several means by which renewable energy can indeed provide baseload power.\nConcentrated Solar Thermal\nOne of the more promising renewable energy technologies is concentrated solar thermal, which uses a system of mirrors or lenses to focus solar radiation on a collector. This type of system can collect and store energy in pressurized steam, molten salt, phase change materials, or purified graphite.\nThe first test of a large-scale thermal solar power tower plant was Solar One in the California Mojave Desert, constructed in 1981. The project produced 10 megawatts (MW) of electricity using 1,818 mirrors, concentrating solar radiation onto a tower which used high-temperature heat transfer fluid to carry the energy to a boiler on the ground, where the steam was used to spin a series of turbines. Water was used as an energy storage medium for Solar One. The system was redesigned in 1995 and renamed Solar Two, which used molten salt as an energy storage medium. In this type of system, molten salt at 290ºC is pumped from a cold storage tank through the receiver where it is heated to about 565ºC. The heated salt then moves on to the hot storage tank (Figure 1). When power is needed from the plant, the hot salt is pumped to a generator that produces steam, which activates a turbine/generator system that creates electricity (NREL 2001).\nFigure 1: Solar Two Power Tower System Diagram (NREL 2001)\nThe Solar Two molten salt system was capable of storing enough energy to produce power three hours after the Sun had set. By using thermal storage, power tower plants can potentially operate for 65 percent of the year without the need for a back-up fuel source. The first commercial concentrated solar thermal plant with molten salt storage - Andasol 1 - was completed in Spain in 2009. Andasol 1 produces 50 MW of power and the molten salt storage can continue to power the plant for approximately 7.5 hours.\nAbengoa Solar is building a 280 MW solar thermal plant in Arizona (the Solana Generating Station), scheduled to begin operation in 2013. This plant will also have a molten salt system with up to 6 hours worth of storage. The electrical utility Arizona Public Service has contracted to purchase the power from Solana station for approximately 14 cents per kilawatt-hour.\nItalian utility Enel recently unveiled \"Archimede\", the first concentrated solar thermal plant to use molten salts for both heat storage and heat transfer. Molten salts can operate at higher temperatures than oils, which gives Archimede higher efficiency and power output. With the higher temperature heat storage allowed by the direct use of salts, Archimede can extend its operating hours further than an oil-operated solar thermal plant with molten salt storage. Archimede is a 5 MW plant with 8 hours of storage capacity.\nIn southern Spain, the Gemasolar plant opened in 2011. It produces 19.9 MW, or approximately 110 gigawatt-hours per year. Gemasolar stores energy in molten salt for up to 15 hours, and is thus able to provide energy 24 hours per day for a minimum of 270 days per year (74% of the year).\nThe National Renewable Energy Laboratory provides a long list of concentrated solar thermal plants in operation, under construction, and in development, many of which have energy storage systems. In short, solar thermal molten salt power storage is already a reality, and a growing resource.\nGeothermal\nGeothermal systems extract energy from water exposed to hot rock deep beneath the earth's surface, and thus do not face the intermittency problems of other renewable energy sources like wind and solar. An expert panel concluded that geothermal sources could produce approximately 100 gigawatts (GW) of baseload power to the USA by mid-century, which is approximately 10% of current US generating capacity (MIT 2006). The panel also concluded that a research and development investment of less than $1 billion would make geothermal energy economically viable.\nThe MIT-led report focuses on a technology called enhanced or engineered geothermal systems (EGS), which doesn't require ideal subsurface conditions and could theoretically work anywhere. installing an EGS plant typically involves drilling a 10- to 12-inch-wide, three- to four-kilometer-deep hole, expanding existing fractures in the rock at the bottom of the hole by pumping down water under high pressure, and drilling a second hole into those fractures. Water pumped down one hole courses through the gaps in the rock, heats up, and flows back to the surface through the second hole. Finally, a plant harvests the heat and circulates the cooled water back down into the cracks (MIT 2007).\nCurrently there are 10.7 GW of geothermal power online globally, with a 20% increase in geothermal power online capacity since 2005. The USA leads the world in geothermal production with 3.1 GW of installed capacity from 77 power plants (GEA 2010).\nWind Compressed Air Energy Storage (CAES)\nVarious methods of storing wind energy have been explored, including pumped hydroelectric storage, batteries, superconducting magnets, flywheels, regenerative fuel cells, and CAES. CAES has been identified as the most promising technology for utility-scale bulk wind energy storage due to relatively low costs, environmental impacts, and high reliability (Cavallo 2005). CAES plants are currently operational in Huntorf, Germany (290 MW, since 1978) and Macintosh, Alabama (110 MW, since 1991). Recently this type of system has been considered to solve the intermittency difficulties associated with wind turbines. It is estimated that more than 80% of the U.S. territory has geology suitable for such underground storage (Gardner and Haynes 2007).\nThe Iowa Stored Energy Park has been proposed to store air in an underground geologic structure during time periods of low customer electric demand and high wind. The project is hoping to store a 20 week supply of compressed air and have approximately 270 MW of generating capacity. The project is anticipated to be operational in 2015.\nA similar system has been proposed to create a wind turbine-air compressor. Instead of generating electricity, each wind turbine will pump air into CAES. This approach has the potential for saving money and improving overall efficiency by eliminating the intermediate and unnecessary electrical generation between the turbine and the air compressor (Gardner and Haynes 2007).\nPumped Heat Energy Storage\nAnother promising energy storage technology involves pumping heat between tanks containing hot and cold insulated gravel. Electrical power is input to the system, which compresses/expands air to approximately 500°C on the hot side and -150°C on the cold side. The air is passed through the two piles of gravel where it gives up its heat/cold to the gravel. In order to regenerate the electricity, the cycle is simply reversed. The benefits of this type of system are that it would take up relatively little space, the round-trip efficiency is approximately 75%, and gravel is a very cheap and abundant material.\nSpent Electric Vehicle (EV) Battery Storage\nAs plug-in hybrids and electric vehicles become more commonplace, the possibility exists to utilize the spent EV batteries for power grid storage after their automotive life, at which point they will still have significant storage capacity. General Motors has been examining this possibility, for example. If a sufficiently large number of former EV batteries could be hooked up to the power grid, they could provide storage capacity for intermittent renewable energy sources.\n100% Energy from Renewables Studies\nA few studies have put forth plans detailing exactly how we can meet 100% of global energy needs from renewable sources.\nEnergy consulting firm Ecofys produced a report detailing how we can meet nearly 100% of global energy needs with renewable sources by 2050. Approximately half of the goal is met through increased energy efficiency to first reduce energy demands, and the other half is achieved by switching to renewable energy sources for electricity production (Figure 2).\nFigure 2: Ecofys projected global energy consumption between 2000 and 2050\nStanford's Mark Jacobson and UC Davis' Mark Delucchi (J&D) recently published a study in the journal Energy Policy examining the possibility of meeting all global energy needs with wind, water, and solar (WWS) power. They find that it would be plausible to produce all new energy from WWS in 2030, and replace all pre-existing energy with WWS by 2050.\nIn Part I of their study, J&D examine the technologies, energy resources, infrastructure, and materials necessary to provide all energy from WWS sources. In Part II of the study, J&D examine the variability of WWS energy, and the costs of their proposal. J&D project that when accounting for the costs associated with air pollution and climate change, all the WWS technologies they consider will be cheaper than conventional energy sources (including coal) by 2020 or 2030, and in fact onshore wind is already cheaper.\nSummary\nTo sum up, there are several types of renewable energy which can provide baseload power. It will be over a decade before we can produce sufficient intermittent renewable energy to require high levels of storage, and there are several promising energy storage technologies. One study found that the UK power grid could accommodate approximately 10-20% of energy from intermittent renewable sources without a \"significant issue\" (Carbon Trust and DTI 2003). By the time renewable energy sources begin to displace a significant part of hydrocarbon generation, there may even be new storage technologies coming into play. The US Department of Energy has made large-scale energy storage one if its research priorities, recently awarding $24.7 million in research grants for Grid-Scale Rampable Intermittent Dispatchable Storage. And several plans have been put forth to meet 100% of global energy needs from renewable sources by 2050.\nLast updated on 4 November 2016 by dana1981. View Archives"
  },
  {
   "title": "Roy Spencer finds negative feedback",
   "paragraph": "Roy Spencer's paper on climate sensitivity\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nSpencer's model is too simple, excluding important factors like ocean dynamics and treats cloud feedbacks as forcings.\nClimate Myth...\nRoy Spencer finds negative feedback\n\"NASA satellite data from the years 2000 through 2011 show the Earth's atmosphere is allowing far more heat to be released into space than alarmist computer models have predicted, reports a new study in the peer-reviewed science journal Remote Sensing. The study indicates far less future global warming will occur than United Nations computer models have predicted, and supports prior studies indicating increases in atmospheric carbon dioxide trap far less heat than alarmists have claimed.\" (James Taylor)\nClimate scientists have identified a number of fundamental problems in Spencer and Braswell's 2011 study which wrongly concludes that the climate is not sensitive to human greenhouse gas emissions. One of the main problems with the paper is that it uses Roy Spencer's very simple climate model which we've previously looked at in .\nThis simple model does not have a realistic representation of the Earth's oceans, which are a key factor in the planet's climate, and it also doesn't model the Earth's water cycle. One key aspect in the Earth's temperature changes is the El Niño Southern Oscillation (ENSO), which is a cycle of the Pacific Ocean. Spencer's model does not include ENSO, and he assumes that ENSO responds to changes in cloud cover, when in reality it's the other way around.\nThere are some other key problems in the paper. It doesn't provide enough information for other scientists to repeat the study. When two other climate scientists (Kevin Trenberth and John Fasullo) tried to replicate its results as best they could with the information provided, they found quite different results (see the Advanced version of this rebuttal for further details). Spencer and Braswell's conclusions also only seems to work using the satellite data set they chose, but Trenberth and Fasullo found that using other data sets also changes their results.\nTrenberth and Fasullo also found that when using a few different climate models, the one which replicated the observed data best was the one with a climate more sensitive to greenhouse gases, which directly contradicts Spencer and Braswell's conclusion that the climate is not sensitive to greenhouse gases.\nIt's also worth noting that the journal which published Spencer and Braswell's paper does not normally publish climate science research. This may explain how the paper made it through their peer-review system with so many problems. In the end, Trenberth and Fasullo find that the Spencer and Braswell study has no merit.\nThe model it uses is far too simple to accurately represent the Earth's climate\nThe paper doesn't provide enough information to replicate their results\nTheir results depend on using one particular data set\nThey assume that ENSO responds to cloud cover changes, when in reality, the reverse is true\nThe study's conclusions are incorrect and unsupportable\nUPDATE 3 Sep 2011: Wolfgang Wagner, has stepped down as editor-in-chief of the journal Remote Sensing. Wagner concluded the Spencer's paper was \"fundamentally flawed and therefore wrongly accepted by the journal\". More here...\nLast updated on 1 August 2011 by dana1981."
  },
  {
   "title": "Royal Society embraces skepticism",
   "paragraph": "Has the Royal Society embraced climate skepticism?\nLink to this page\nWhat the science says...\nThe Royal Society strongly state that human activity is the dominant cause of global warming.\nClimate Myth...\nRoyal Society embraces skepticism\n\"The UK today has made it official. With the release of its revised guide to climate change by the Royal Society, the nation’s preeminent scientific organization, the UK now formally joins the ranks of the denier nations. The science on climate change is no longer certain, the Society now says.\" (Lawrence Solomon)\nFollowing complaints, the Royal Society has published a guide to climate science which counts 2 self selected “skeptics” among its co-authors. Traditional skeptical sources have enjoyed the release, including the Global Warming Policy Foundation (GWPF) and The Daily Mail, with the Mail quoting the Foundation’s Director;\n\"The Royal Society now also agrees with the GWPF that the warming trend of the 1980s and 90s has come to a halt in the last 10 years.\"\nAnd gleefully reporting that the Royal Society “admits that there are ‘uncertainties’” - I remember a Professor who drilled \"numbers mean nothing without uncertainties!\" into students, so this shouldn't surprise most people with scientific experience!\nThe Intergovernmental Panel on Climate Change (IPCC) carefully includes them with all its statements. The Society report is worth reading if you have time but if you want detail then read the IPCC report: the two agree on everything they both cover.\nThe Society split simple statements into 3 sections: widespread scientific agreement, widespread consensus but active discussion, and ‘not well understood’.\nWidespread agreement\n0.8 ± 0.2 °C warming since 1850.\nRise in CO2 caused by humans.\nIPCC heating or ‘radiative forcing’ values.\nDoubling CO2 causes 1 °C of direct warming, feedbacks are expected to add more.\nWide consensus but continuing debate and discussion\nSolar heating less than 10% of CO2’s, but research is checking to see if it’s magnified somehow.\nDoubling CO will cause 2-4.5 °C global warming (the IPCC says “likely to be in the range 2 to 4.5 °C with a best estimate of about 3 °C“), and IPCC global warming projections are repeated.\nSea levels will rise at least at the rate they have been.\nNot well understood\nModels struggle with clouds, regional changes, and long term carbon cycle feedback.\nModels don’t catch ice sheet breakup, so sea level rise they give is a minimum.\nSummary\nNon-model evidence for future sea level rise and global warming are ignored. This tends to suggest that doubling CO2 will cause 2-4.5 °C warming and new evidence suggests that sea level rise will be 100%+ more than IPCC estimates.\nThe GWPF concludes that;\n\"The UK now formally joins the ranks of denier nations,\"\nwhich seems remarkable from the Society's statement that;\n\"There is strong evidence that changes in greenhouse gas concentrations due to human activity are the dominant cause of the global warming that has taken place over the last half century. \"\nFinally, what of the GWPF’s claim that the Society now agrees global warming has halted? Another case of confusing short term trends, being ignorant of heat on Earth and seemingly based on;\n\"This warming has... been largely concentrated... from around 1975 to around 2000,\"\nbut ignoring;\n\"The decade 2000-2009 was, globally, around 0.15 °C warmer than the decade 1990-1999.\"\nMake of that what you will.\nLast updated on 1 October 2010 by MarkR."
  },
  {
   "title": "Satellite error inflated Great Lakes temperatures",
   "paragraph": "What is the significance of the Great Lakes temperature as measured by satellites?\nLink to this page\nWhat the science says...\nTemperature errors in the Great Lakes region are not incorporated in any of the global mean temperature records. In particular, there is no connection to the satellite microwave temperature analyses by RSS and UAH, which use entirely different sensors operating in a quite different portion of the electromagnetic spectrum.\nClimate Myth...\nSatellite error inflated Great Lakes temperatures\n\"Global warming data apparently cooked by U.S. government-funded body shows astounding temperature fraud with increases averaging 10 to 15 degrees Fahrenheit. The tax-payer funded National Oceanic and Atmospheric Administration (NOAA) has become mired in fresh global warming data scandal involving numbers for the Great Lakes region that substantially ramp up averages.\" (John O'Sullivan)\nThere are a variety of rumors involving claimed problems with satellite temperature measurements. Unfortunately, there is a great deal of confusion on this point. Here are the main points:\nThese stories were sparked by image maps of surface temperatures for the US/Canada Great Lakes posted by an automated processing system at the NOAA-funded Great Lakes Coastwatch program website. In areas with full or partial cloud cover, these maps sometimes show obviously erroneous temperature estimates over the lakes.\nThere is a great deal of confusion about whether this represents a broader problem with satellite temperature measurements, and perhaps \"invalidates\" the satellite temperature record.\nAre the Great Lakes Coastwatch data used in any of the global mean temperature records?\nFirst off, the Great Lakes Coastwatch data products aren't part of any global climate data set. They are produced by a local team of investigators supported by the Michigan Sea Grant program and NOAA's Great Lakes Environmental Research Laboratory (GLERL), and are primarily used by fishing vessels, natural resource managers, and scientists with an interest in the Great Lakes.\nThe images posted at the Great Lakes Coastwatch site come from an automated algorithm that assimilates thermal infrared imagery from the Advanced Very High Resolution Radiometer (AVHRR) instrument on NOAA's Polar Orbiting Environmental Satellite (POES) constellation. Where clouds are present, the temperature measurements are not reliable, and the algorithm flags these areas with black or gray tints on the maps to indicate this uncertainty.\nIt's important to note immediately that these data have no connection whatsoever to the main global satellite temperature records, which are produced by researchers at Remote Sensing Systems (RSS) and at the University of Alabama at Huntsville (UAH). Those data sets come from measurements by different sensors entirely, operating in the microwave portion of the electromagnetic spectrum, rather than the thermal infrared range used by AVHRR.\nThe Great Lakes Coastwatch data are likewise not merged with any of the global mean temperature records produced by NASA, NOAA, the University of East Anglia, the Japanese Meteorological Agency, or others.\nSo, to allay people's fears, none of the Great Lakes Coastwatch data are used in any global temperature reconstruction, and the primary satellite temperature trends don't even come from the same sensor or the same portion of the electromagnetic spectrum. Conflating the two data sets isn't just like confusing apples and oranges, it's like comparing apples and salmon, or apples and lettuce (or pick any two completely unrelated food groups of your choice).\nImplications for sea surface temperature measurements elsewhere\nThat said, data from AVHRR are used as one (among many) sources used to measure sea surface temperatures (SST). Do the errors in some of the Great Lakes Coastwatch images indicate potential problems for SST data sets elsewhere?\nThere is a particular concern about the status of the AVHRR instrument on the NOAA-16 spacecraft. The stories circulating in the skeptic blogosphere this week generally don't recognize that NOAA-16 is a \"secondary\" satellite in the POES constellation, that AVHRR is present on other satellites, and that there are additional sensors on other US and international spacecraft with similar wavelength ranges in the thermal infrared range that are used for sea (or lake) surface temperature measurement.\nSatellite thermal infrared measurements of sea surface are one of the most important global weather and climate records, and are used for everything from tropical storm prediction to fishing vessel operations. These data are provided by different sensors on multiple spacecraft, and are continuously validated using in-situ temperature measurements from buoys and ships.\nIn summary\nStories in the \"skeptic\" blogosphere recently have given their readers a mistaken impression that there are reasons to question the validity of observed warming in the various satellite-derived global climate data sets. This is not the case.\nThe cited examples of temperature errors in the Great Lakes region are not incorporated in any of the global mean temperature records. In particular, there is no connection to the satellite microwave temperature analyses by RSS (Figure 1) and UAH, which use entirely different sensors operating in a quite different portion of the electromagnetic spectrum.\nFigure 1. RSS (satellite-derived) monthly temperatures through 2000 (in gray) and 2001-2010 (red and blue). The pink trendline is the linear trend 1979-2000.\nLast updated on 2 September 2010 by Ned."
  },
  {
   "title": "Satellite record is more reliable than thermometers",
   "paragraph": "Which is a more reliable measure of global temperature: thermometers or satellites?\nLink to this page\nWhat the science says...\nSatellites don't measure temperatures, and the uncertainty in the trend is five times as large as that in the global surface temperature record.\nClimate Myth...\nSatellite record is more reliable than thermometers\n\"The satellite data are the best data we have.\"\nTed Cruz\nSatellites don't measure temperature. As Carl Mears of the Remote Sensing Systems (RSS) satellite dataset and Ben Santer wrote,\nthey are not thermometers in space. The satellite [temperature] data ... were obtained from so-called Microwave Sounding Units (MSUs), which measure the microwave emissions of oxygen molecules from broad atmospheric layers. Converting this information to estimates of temperature trends has substantial uncertainties.\nScientists process the MSU data, applying a model to make numerous adjustments, in order to come up with a synthetic estimate of the atmospheric temperature.\nAs Andrew Dessler describes in the video below by Peter Sinclair, MSUs and advanced MSUs (AMSUs) measure voltages on detectors, which themselves are detecting microwave signals emitted by oxygen molecules in the Earth's atmosphere that change proportionally to temperature changes. To translate these microwave detections into estimates of the temperature of various layers of the Earth's atmosphere requires a model and a lot of data processing and adjustments.\nSatellite Temperature Record Challenges\nConverting those MSU microwave detections into a reliable long-term synthetic atmospheric temperature record is a challenging proposition, made all the more difficult by a number of confounding factors. For example, the satellites themselves have a limited life span. The overall satellite MSU record is comprised of data from numerous satellites, and each has a different calibration, orbit, etc. that must be accounted for. During that life span, the satellites also experience friction, which causes their orbits to drift. If not correctly taken into account, these factors can create a bias in the synthetic temperature record.\nAnother issue is that the MSU detections can be influenced by factors besides just oxygen microwave signals, for example, cloud liquid water. Weng et al. (2014) found that the MSU channel (Channel 3) that focuses on the lowest level of the atmosphere (the lower troposphere) is most influenced by the presence of cloud liquid water. Weng et al. suggest,\nthe global mean temperature in the low and middle troposphere has a larger warming rate (about 20–30% higher) when the cloud-affected radiances are removed from AMSU-A data.\nRoy Spencer who, with John Christy, runs the University of Alabama at Huntsville (UAH) satellite synthetic temperature dataset disagrees, believing that the cloud-caused bias is insignificant. The magnitude of this bias in the satellite data remains an unresolved question.\nAnother issue related to changes in the satellites' orbits is called 'diurnal drift'. The satellites are in 'sun synchronous orbits' and are meant to stay aligned with the sun so that they always cross the equator at the same time. If they don’t, then the normal daily temperature cycles below will start to add a false bias to the data. The UAH team tried to get around this bias in version 5 of their dataset by attempting to use these satellites during periods when the diurnal drift is small, while other groups (RSS and NOAA) apply a correction based on the diurnal drift in a global climate model. Po-Chedley et al. (2015) argue that the UAH method creates a cool bias in their dataset.\nThere are still further challenges, for example the fact that the increased greenhouse effect cools the stratosphere, which is the layer of the atmosphere above the troposphere. If microwave measurements from the stratosphere bleed into estimates of tropospheric temperatures, that can also cause a cool bias in the trend. The UAH dataset has required a number of significant adjustments since its inception to correct for these sorts of factors.\nThe figure below shows all the processing required to get from voltage measurements on an MSU sensor for the Remote Sensing Systems (RSS) group to an estimate of the temperature in the atmosphere.\nFlowchart of the processing algorithm for the RSS satellite data, from Mears et al. (2011)\nAs a result, the uncertainty in the satellite data is larger than that in the surface temperature record, which is based on direct measurements by thermometers. In particular, estimated satellite trend observational uncertainty is five times greater.\nUncertainty in the temperature satellite and surface temperature trends, estimated from the RSS and HadCRUT4 ensembles. The boxes show the mean and interquartile range of the trends on 1979-2012 in each ensemble. Whiskers indicate the 95% interval (2.5%-97.5%). Crosses indicate outliers. Created by Kevin Cowtan.\nKevin Cowtan summarizes,\non the basis of the best understanding of the record providers themselves, the surface temperature record appears to be the better source of trend information. The satellite record is valuable for its uniform geographical coverage and ability to measure different levels in the atmosphere, but it is not our best source of data concerning temperature change at the surface.\nTropospheric temperature trend disagreements\nThe UAH and RSS groups aren't the only ones trying to estimate the temperature of the atmosphere. We also have direct measurements made by temperature sensors on weather balloons, for example. These are generally in good agreement with the MSU lower atmosphere temperature estimates, until recent years. Since the turn of the century, the UAH and RSS datasets show little warming of lower tropospheric temperatures, while weather balloons show continued warming. For example, Sherwood et al. (2015) concluded that the weather balloon data \"contradicts suggestions that atmospheric warming has slowed in recent decades or that it has not kept up with that at the surface.\"\nSimilarly, a weather balloon dataset from NOAA called RATPAC shows a growing divergence from the satellite lower troposphere temperature estimates over the past several years.\nRATPAC vs. RSS lower troposphere temperature estimates. Figure created by Tamino.\nThere are also other groups besides UAH and RSS that have used the MSU data to create their own lower and mid-troposphere temperature estimates, and these different estimates show quite a lot of variation. These groups are all using the same raw MSU and AMSU data, and arrive at very different final estimates of atmospheric temperatures and trends.\nAs Carl Mears of RSS has said,\nI consider [surface temperature datasets] to be more reliable than satellite datasets (they certainly agree with each other better than the various satellite datasets do!).\nTropical mid-troposphere temperature satellite trend estimates from different groups using different diurnal drift correction approaches. From Po-Chedley et al. (2014).\nWhat makes \"the best\" the best?\nThis particular myth deals with the claim that the satellite data are \"the best data we have\" for measuring global warming. What's \"best\" is a subjective judgment call, but if the goal is to have greater certainty about the data and trends, the surface temperature record is \"better\" than the satellite data.\nAdditionally, only about 2% of the global energy imbalance goes into warming the atmosphere. Over 90% goes into warming the oceans, so if the goal is to best measure the overall warming of the Earth, ocean heat content data would be \"the best\" single source.\nA visual depiction of how much global warming heat is going into the various components of the climate system for the period 1993 to 2003, calculated from IPCC AR4 5.2.2.3.\nAnother consideration is that humans live on the surface, not kilometers up in the atmosphere. The surface temperature record measures temperature changes where we live, which is arguably the most relevant in terms of effects on human society. By that standard, the surface temperature record would be \"the best.\"\nAs Carl Mears of RSS noted toward the end of the video above, what we should really do, rather than trying to decide which piece of data is \"best,\" is to consider all the data. That includes the temperature of the atmosphere, surface, and oceans, melting ice, rising oceans, shifting ecosystems, and so on. These paint a clear picture of what's happening with the Earth's climate – it's warming and changing fast.\nGlobal heat content data, from Nuccitelli et al. (2012).\nThe distinguishing feature of the satellite data is that it shows relatively little warming of the lower atmosphere in recent years. But given the fact that all other related datasets disagree, and given the large uncertainties in the satellite data, that may be more of a bug than an accurate feature.\nLast updated on 18 January 2016 by dana1981. View Archives"
  },
  {
   "title": "Satellites show no warming in the troposphere",
   "paragraph": "Satellite measurements of warming in the troposphere\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nThe most recent satellite data show that the earth as a whole is warming.\nClimate Myth...\nSatellites show no warming in the troposphere\n\"Satellite measurements indicate an absence of significant global warming since 1979, the very period that human carbon dioxide emissions have been increasing rapidly. The satellite data signal not only the absence of substantial human-induced warming but also provide an empirical test of the greenhouse hypothesis - a test that the hypothesis fails.\" (Bob Carter)\nJohn Christy and Roy Spencer of the University of Alabama published a series of papers starting about 1990 that implied the troposphere was warming at a much slower rate than the surface temperature record and climate models indicated Spencer and Christy (1992). One early version of their data even showed a cooling trend (Christy et al. 1995).\nSeveral groups of scientists began looking closely at this discrepancy. With so many other pieces of evidence indicating warming, it seemed unlikely that the troposphere would not be warming. Errors were discovered in the methods the UAH group used to adjust the data.\nTo understand what was wrong: The satellites must pass over the same spot on Earth at the same time each day to get a temperature average. In reality the time the satellite passes drifts slightly as the orbit slowly decays. To compensate for this and other orbital changes a series of adjustments must be applied to the data.\nThe MSU satellite data is collected from a number of satellites orbiting & providing daily coverage of some 80% of the Earth's surface. Each day the orbits shift and 100% coverage is achieved every 3-4 days. The microwave sensors on the satellites do not directly measure temperature, but rather radiation given off by oxygen in the Earth's atmosphere. The intensity of this radiation is directly proportional to the temperature of the air and is therefore used to estimate global temperatures.\nThere are also differences between the sensors that were onboard each satellite and merging this data to one continuous record is not easily done. It was nearly 13 years after the orginal papers that the adjustments that Christy and Spencer originally applied were found to be incorrect. Mears et al. (2003) and Mears et al. (2005).\nWhen the correct adjustments to the data were applied the data matched much more closely the trends expected by climate models. It was also more consistent with the historical record of troposphere temperatures obtained from weather balloons. As better methods to adjust for biases in instruments and orbital changes have been developed, the differences between the surface temperature record and the troposphere have steadily decreased.\nAt least two other groups keep track of the tropospheric temperature using satellites and they all now show warming in the troposphere that is consistent with the surface temperature record. Furthermore data also shows now that the stratosphere is cooling as predicted by the physics.\nAll three groups measuring temperatures of the troposphere show a warming trend. The U.S. Climate Change Science Program produced a study (pdf) in April 2006 on this topic. Lead authors included John Christy of UAH and Ben Santer of Lawrence Livermore National Labs. The first page has this quote:\nPreviously reported discrepancies between the amount of warming near the surface and higher in the atmosphere have been used to challenge the reliability of climate models and the reality of human-induced global warming... This significant discrepancy no longer exists because errors in the satellite and radiosonde data have been identified and corrected. New data sets have also been developed that do not show such discrepancies.\"\nThere are still some discrepancies between satellite measured temperatures in the tropics and those measured by radiosondes. Most researchers believe this difference is likely due to instrument errors.\nThe original discrepancy is an excellent example of how science works and of critical thinking. With many different indicators showing warming, it did not make sense that the troposphere would be cooling. This discrepancy was taken very seriously by the scientific community, and the consistency and accuracy of all relevant data were examined intensely.\nScience advances by trial and error. The result is an increased knowledge of how to measure the temperature of the troposphere from space.\nLast updated on 24 July 2015 by David Kirtley. View Archives"
  },
  {
   "title": "Schmittner finds low climate sensitivity",
   "paragraph": "Putting climate sensitivity estimates in context\nLink to this page\nWhat the science says...\nSchmittner et al. find very low probability of both low and high equilibrium climate sensitivities. Their estimated likely range of equilibrium climate sensitivity values is only marginally lower than that of the IPCC, and their lower value is almost entirely based on a new estimate of Last Glacial Maximum (LGM) temperatures warmer than previous estimates. It remains to be seen whether this new estimate - and thus the Schmittner climate sensitivity - withstands the test of time.\nClimate Myth...\nSchmittner finds low climate sensitivity\n\"a paper...in Science magazine concludes that the climate sensitivity—how much the earth’s average temperature will rise as a result of a doubling of the atmospheric concentration of carbon dioxide—likely (that is, with a 66% probability) lies in the range 1.7°C to 2.6°C, with a median value of 2.3°C. This is a sizeable contraction and reduction from the estimates of the climate sensitivity given by the Intergovernmental Panel on Climate Change (IPCC) Fourth Assessment Report (AR4), in which the likely range is given as 2.0°C to 4.5°C, with a best estimate of 3.0°C\" (Pat Michaels)\nA new paper in Science from Schmittner et al. (2011) attempts to constrain climate sensitivity based on temperature reconstructions of the Last Glacial Maximum (LGM) approximately 20,000 years ago:\n\"combining extensive sea and land surface temperature reconstructions from the Last Glacial Maximum with climate model simulations we estimate a lower median (2.3 K) and reduced uncertainty (1.7–2.6 K 66% probability).\"\nThis estimate is significantly narrower and a bit lower than the IPCC-estimated 66% probability range for equilibrium climate sensitivity of 2 to 4.5°C for doubled atmospheric CO2, and is illustrated in Figure 1.\nFigure 1: Marginal posterior probability distributions for equilibrium climate sensitivity to doubled atmospheric CO2 (ECS2xC) from Schmittner et al. (2011), estimated from land and ocean, land only, and ocean only temperature reconstructions.\nConcerns About the Study\nThere are some unusual aspects about this study which require further investigation before the conclusions of the study can be accepted, as the authors themselves point out. For example, the study uses a relatively new global mean surface temperature reconstruction for the LGM of just 2.2°C cooler than interglacial temperatures in the locations where they have proxy data, or 2.6°C from the global model average. This is significantly lower than most paleoclimate estimates, which generally put the LGM in the range of 4 to 7°C cooler than current temperatures. For comparison, in their study also using the LGM to constrain climate sensitivity, Hansen and Sato (2011) used a mean surface temperature change of 5°C, consistent with the body of literature (Figure 2).\nFigure 2: Climate forcings during the ice age 20 ky ago relative to the pre-industrial Holocene from Hansen and Sato (2011)\nSince the radiative forcing associated with doubled CO2 is 3.7 Watts per square meter (W/m2), Hansen and Sato's result implies a fast-feedback climate sensitivity of 2.8°C, which is slightly outside the Schmittner et al. 66% probability range (at the upper end of their 90% probability range). In fact, as Urban explains, the main reason Schmittner et al. arrive at a lower climate sensitivity estimate than previous studies is due to their lower LGM temperature reconstruction:\n\"our LGM temperature reconstruction is quite different from what has been commonly assumed, and our study may prove inconsistent with other evidence that we have not yet considered. This is something that will have to be sorted out by further debate and research...our new temperature reconstruction explains a lot of the difference between our climate sensitivity estimate and previous estimates.\"\nIn an interview with New Scientist on this paper, Gavin Schmidt said:\n\"The model estimate of the cooling during the Last Glacial Maximum is a clear underestimate...A different model would give a cooler Last Glacial Maximum, and thus a larger sensitivity.\"\nAs Figure 1 shows, the Schmittner et al. global climate sensitivity estimate is dominated by the ocean data, which is based on from the Multiproxy Approach for the Reconstruction of the Glacial Ocean (MARGO) project, about which Richard Alley noted:\n\"MARGO made a solid effort, which indicates very small temperature changes. But, there are other ways to do it, and indeed, [Schmittner et al.] coauthor Alan Mix has published independent papers indicating that the temperature changes were larger in some regions than indicated by MARGO. David Lea and others have also obtained larger temperature shifts….\nIn short, the MARGO data for the ocean show very small temperature change from the ice age to today, and thus lead to the low climate sensitivity, but they disagree with some independent estimates showing larger temperature change. They also lead to disagreement with the pollen-based land temperature data. Furthermore, they lead to an answer that disagrees with many other lines of evidence for climate sensitivity.\"\nAnother concern regarding the study is in the model they used - the University of Victoria (UVic) climate model. UVic is an admittedly simple model compared to other global climate models, as co-author Nathan Urban discussed in an interview with Planet 3.0:\n\"UVic isn’t the most complex model either. It has a simplified atmosphere, which is an advantage and disadvantage. The disadvantage is that it has a very approximate representation of atmospheric processes. The advantage is that this makes the simulations run faster. It is less computationally expensive.\"\nA number of other climate scientists interviewed for a BBC article also expressed reservations about the study's assumptions and results. For example, the climate sensitivity in transitioning from a cold to warm period may be different than that in transitioning from a warm to a hot period, as Andrey Ganopolski noted:\n\"There is evidence the relationship between CO2 and surface temperatures is likely to be different [during] very cold periods than warmer.\"\nThis is particularly true since the LGM only experienced fast feedbacks, whereas due to the rapid rate of the current climate change, slower feedbacks may be triggered on century timescales.\nThe authors themselves have their own concerns, as is the case with any good scientific study, and expressed a number of caveats about their findings. For example:\n\"Two things are immediately apparent from these [Figure 1] curves. First, the sea surface temperature data support lower climate sensitivities and the land surface temperature data support higher sensitivities. There isn’t a great deal of overlap between these curves, so this suggests a possible inconsistency between the land and ocean analyses. Second, when we combine the land and ocean data, the ocean data dominate the result (the black and blue curves are very similar), “overruling” what the land data have to say. I think this is, at least in part, because there are more ocean data than land data.\"\n\"There are many hypotheses for what’s going on here. There could be something wrong with the land data, or the ocean data. There could be something wrong with the climate model’s simulation of land temperatures, or ocean temperatures. The magnitudes of the temperatures could be biased in some way. Or, more subtly, they could be unbiased, on average, but the model and observations could disagree on the cold and warm spots are, as I alluded to earlier. Or something even more complicated could be going on.\nUntil the above questions are resolved, it’s premature to conclude that we have disproven high climate sensitivities, just because our statistical analysis assigns them low probabilities.\"\nSetting these concerns aside for the moment, what would the Schmittner et al. results mean, if correct?\nThe Good News\nThe climate denialists have of course focused on the good news aspect of this paper - that it claims to rule out the 'long tail' of high climate sensitivity. Most individual studies estimating climate sensitivity are unable to rule out very high sensitivity values (Figure 3).\nFigure 3: Probability distribution of climate sensitivity to a doubling of atmospheric CO2\nHowever, Annan and Hargreaves (2009) used a Bayesian statistical approach to investigate various probabilistic estimates of climate sensitivity, and concluded that\n\"the long fat tail that is characteristic of all recent estimates of climate sensitivity simply disappears, with an upper 95% probability limit...easily shown to lie close to 4°C, and certainly well below 6°C.\"\nSo Schmittner et al. would not be the first study to find low probability of very high values of (fast feedback) climate sensitivity. Nevertheless, their conclusion that high sensitivity models do not simulate LGM changes well is good news:\n\"models with ECS2xC > 4.5 K overestimate the cooling almost everywhere, particularly at low latitudes. High sensitivity models (ECS2xC > 6.3 K) show a runaway effect resulting in a completely ice-covered planet.\"\nThe Bad News\nFor those true skeptics among us who look at the entire study, unfortunately it contains substantial bad news. Firstly, in addition to ruling out very high equlibrium climate sensitivity values, it would also rule out very low values:\n\"Models with ECS2xC < 1.3 K underestimate the cooling at the LGM almost everywhere, particularly at mid latitudes and over Antarctica\"\nIn other words, Schmittner et al. find equilibrium sensitivities of less than 1.3°C just as unrealistic as sensitivities greater than 4.5°C. The low sensitivity arguments made by the likes of Spencer, Lindzen, Christy, Monckton, etc., which are the climate denialist \"endgame\", proclaim that climate sensitivity is indeed less than 1.3°C for doubled CO2. According to Schmittner et al., they're wrong. Somehow the climate denialists glossed over this aspect in their reporting on the paper.\nIt's worth briefly noting here that when confronted with the fact that paleoclimate data are inconsistent with their asserted low climate sensitivity values, the \"skeptics\" suddenly find the proxy data and models unreliable. For example, Pielke Sr.:\n\"I do not find the glacial and interglacial periods as useful comparisons with the current climate since when we study them with models\"\nBut once the proxy data and models support a conclusion they want to believe - climate sensitivity is not extremely high - suddenly the supposed \"skeptics\" (including Pielke's 'colleagues') are willing to accept the results entirely uncritically. Just another of those denialist self-contradictions to add to the list.\nSecondly, as noted above, Schmittner et al. have assumed that the difference between a glacial maximum and interglacial temperature is a mere 2.6°C. The global average surface temperature has already warmed 0.8°C over the past century. During the LGM, the surface was covered with huge ice sheets, plant life was different, and sea levels were 120 meters lower. As Schmittner notes:\n\"Very small changes in temperature cause huge changes in certain regions, so even if we get a smaller temperature rise than we expected, the knock-on effects would still be severe.\"\nand in a Science Daily interview:\n\"Hence, drastic changes over land can be expected. However, our study implies that we still have time to prevent that from happening, if we make a concerted effort to change course soon\".\nThe Ugly News\nIn short, if Schmittner et al. are correct and such a small temperature change can cause such a drastic climate change, then we may be in for a rude awakening in the very near future, because their smaller glacial-interglacial difference would imply a quicker climate response a global temperature change, as illustrated in Figure 4.\nFigure 4: IPCC and Schmittner et al. CO2-caused warming based on business-as-usual (BAU) emissions (defined as IPCC Scenario A2) and their equilibrium climate sensitivity best estimates, assuming transient sensitivity is ~67% of equilibrium sensitivity (solid lines) vs. their best estimates for the average global surface temperature change between the LGM and current interglacial (dashed lines).\nAs Figure 4 illustrates, although the Schmittner et al. best estimate for climate sensitivity results in approximately 20% less warming than the IPCC best estimate, we also achieve their estimated temperature change between glacial and interglacial periods (the dashed lines) much sooner. The dashed lines represent the temperature changes between glacial and interglacial periods in the Schmittner (blue) and IPCC (red) analyses. If Schmittner et al. are correct, we are on pace to cause a temperature change of the magnitude of an glacial-interglacial transition - and thus likely similarly dramatic climate changes - within approximately the next century.*\n* - Note that this calculation and Figure 4 exclude warming caused by non-CO2 greenhouse gases (GHGs) whose warming effects are currently approximately offset by aerosols, but this offset probably won't continue in the future as GHG emissions continue to rise and aerosol emissions likely fall due to efforts to achieve clean air. Thus our CO2-caused warming estimates are likely conservative, underestimating total future global warming.\nSchmittner Take-Home\nTo summarize,\nSchmittner et al. believe they have found low probabilities for both very high and very low equilibrium climate sensitivities, and their best-fit model sensitivity is 2.4°C for doubled CO2\nThere are some concerns about the Schmittner et al. methodology, such as their use of the simple and outlying UVic climate model, and their estimate that the temperature change between interglacial periods and the LGM was just 2.6°C\nIf Schmittner et al. are right about climate sensitivity and LGM temperature change, then if we continue with business-as-usual GHG emissions, we will match the amount of warming between glacial and interglacial periods within roughly the next century. Some of the differences between glacial and interglacial periods include 120 meter sea level rise, and a completely different global landscape - very dramatic climate changes.\nIn short, we should not over-emphasize the results of Schmittner et al., as the authors themselves warn. Their results are roughly consistent with other estimates of climate sensitivity (Figure 5).\nFigure 5: Distributions and ranges for climate sensitivity from different lines of evidence. The circle indicates the most likely value. The thin colored bars indicate very likely value (more than 90% probability). The thicker colored bars indicate likely values (more than 66% probability). Dashed lines indicate no robust constraint on an upper bound. The IPCC likely range (2 to 4.5°C) and most likely value (3°C) are indicated by the vertical grey bar and black line, respectively (Source: Knutti and Hegerl 2008)\nIn fact if Schmittner et al. are totally correct, we may be in for some rapid climate changes in the relatively near future, as we approach the amount of warming that separates a glacial from an interglacial period.\nLast updated on 23 October 2016 by dana1981. View Archives"
  },
  {
   "title": "Scientists can't even predict weather",
   "paragraph": "The difference between weather and climate\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nWeather and climate are different; climate predictions do not need weather detail.\nClimate Myth...\nScientists can't even predict weather\n...Since modern computer models cannot with any certainty predict the weather two weeks from now, how can we rely upon computer models to predict what the Earth's climate might be like a hundred years from now? They can't! Yet people like Al \"Carbon-Credit\" Gore want you to believe that these models can predict the future. I bet I can do at least as well with a crystal ball (source: Kowabunga)\nThis claim is based more on an appeal to emotion than fact. The inference is that climate predictions, decades into the future, cannot be possibly right when the weather forecast for the next day has some uncertainty.\nIn spite of the claim in this myth, short term weather forecasts are highly accurate and have improved dramatically over the last three decades. However, slight errors in initial conditions make a forecast beyond two weeks nearly impossible.\nAtmospheric science students are taught \"weather is what you get and climate is the weather you expect\". This is why this common skeptical argument doesn't hold water. Climate models are not predicting day to day weather systems. Instead, they are predicting climate averages.\nFigure 1: Record highs are an example of extreme weather, but an increase in record highs versus record lows is a symptom of a changing climate. From Meehl et al.*\nA change in temperature of 7º Celsius from one day to the next is barely worth noting when you are discussing weather. Seven degrees, however, make a dramatic difference when talking about climate. When the Earth's AVERAGE temperature was 7ºC cooler than the present, ice sheets a mile thick were on top of Manhattan!\nA good analogy of the difference between weather and climate is to consider a swimming pool. Imagine that the pool is being slowly filled. If someone dives in there will be waves. The waves are weather, and the average water level is the climate. A diver jumping into the pool the next day will create more waves, but the water level (aka the climate) will be higher as more water flows into the pool.\nIn the atmosphere the water hose is increasing greenhouse gases. They will cause the climate to warm but we will still have changing weather (waves). Climate scientists use models to forecast the average water level in the pool, not the waves. A good basic explanation of climate models is available in Climate Change- A Multidisciplinary Approach by William Burroughs.\nSource: AMS Policy Statement on Weather Analysis and Forecasting. Bull. Amer Met. Soc., 79, 2161-2163\n*Image source: Meehl, G. A., C. Tebaldi, G. Walton, D. Easterling, and L. McDaniel (2009), Relative increase of record high maximum temperatures compared to record low minimum temperatures in the U.S., Geophys. Res. Lett., 36, L23701, doi:10.1029/2009GL040736.\nBasic rebuttal written by dansat\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 15 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Scientists retracted claim that sea levels are rising",
   "paragraph": "Paper retracted for predicting too low sea level rise\nLink to this page\nWhat the science says...\nThe retracted paper actually predicts a low range of future sea level rise. The retraction removes a lower bound of sea level prediction. This increases confidence in other peer-reviewed research predicting sea level rise of 80cm to 2 metres by 2100.\nClimate Myth...\nScientists retracted claim that sea levels are rising\n'Another global warming myth comes crashing down. No warming since at least 1995, no melting glaciers and now no rising sea levels. Basically this leaves the warmers with no credibility, along with the U.S. media, which is largely ignoring this massive scandal.' (JammieWearingFool)\nIn February 2010, scientists who published a 2009 paper on sea level rise retracted their prediction due to errors in their methodology. This has led some to claim sea levels are no longer predicted to rise. This interpretation was helped no doubt by the unfortunate Guardian headline \"Climate scientists withdraw journal claims of rising sea levels\". However, reading the article and perusing the peer-reviewed science on future sea level shows that the opposite is the case.\nThe IPCC 4th Assessment Report predicted sea level will rise between 18 to 59 cm by the year 2100. Many consider this a conservative estimate as observed sea level rise is tracking at the top range of IPCC estimates (Rahmstorf 2007, Allison 2009). However, a study led by Mark Siddall examined how sea levels have changed over the past 22,000 years in response to temperature change (Siddall 2009). This enabled them to predict how sea level would respond to future warming, estimating sea level rise between 7 to 82 cm by the year 2100. Siddall's paper concluded that this increased confidence in the IPCC projections.\nHowever, a later study using similar methods to Siddall 2009 came to dramatically different results, estimating sea level rise of 75 to 190 cm by 2100 (Vermeer & Rahmstorf 2009). Why the discrepancy? Judging by the acknowledgement in Siddall's retraction, one speculates that Vermeer and Rahmstorf discovered flaws in Siddall's methodology and notified the authors. Siddall saw that the errors undermined their results and retracted their paper. So we have two papers using similar methods - one predicting low sea level rise, the other predicting high sea level rise. The low sea level rise is found to be in error. While some are spinning this result to imply no sea level rise, in actuality it increases our confidence in high sea level rise.\nVermeer's results are confirmed by another study that approach the sea level question from a different angle, examining the dynamics of calving glaciers (Pfeffer 2008). The conclusion was a predicted sea level rise of 80 cm to 2 metres by 2100. Further evidence of the ice sheets' high sensitivity to warmer temperature comes from paleoclimate studies of the last interglacial period 125,000 years ago. At that time, global temperatures were around 2 degrees warmer than now. This is the amount of warming expected for some of the lower emission scenarios. At that time of the last interglacial, sea levels were at least 6 metres higher than present levels. So while we expect sea levels to rise up to 2 metres by 2100, they will continue to rise afterwards to at least 6 metres.\nFuture sea level rise will be one of the most serious impacts of global warming on humanity, with much of the world's population clustered around coastlines. Our children and grandchildren will see sea level rise of 1 to 2 metres in their lifetime. This scientific reality is a stark contrast to the 'Now You Can Forget About Those Rising Seas' attitude. Despite the serious picture painted by the peer-reviewed science, these kinds of misinterpretations turn the climate debate in an almost farcical direction. One could blame the Guardian for a carelessly worded headline. More blame should be apportioned to those who pontificate from their soapboxes without bothering to acquaint themselves with the science. That skeptics allow themselves to be tossed and turned by media headlines is the very antithesis of genuine skepticism.\nLast updated on 9 August 2010 by John Cook."
  },
  {
   "title": "Scientists tried to 'hide the decline' in global temperature",
   "paragraph": "Clearing up misconceptions regarding 'hide the decline'\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nThe \"decline\" refers to a decline in northern tree-rings, not global temperature, and is openly discussed in papers and the IPCC reports.\nClimate Myth...\nScientists tried to 'hide the decline' in global temperature\n'Perhaps the most infamous example of this comes from the \"hide the decline\" email. This email initially garnered widespread media attention, as well as significant disagreement over its implications. In our view, the email, as well as the contextual history behind it, appears to show several scientists eager to present a particular viewpoint-that anthropogenic emissions are largely responsible for global warming-even when the data showed something different.' (David Lungren)\nThere are a number of misconceptions concerning Phil Jones' email. These are easily cleared up when one takes the time to read Jones' words in context.\nThe \"decline\" is about northern tree-rings, not global temperature\nPhil Jones' email is often cited as evidence of an attempt to \"hide the decline in global temperatures\". This claim is patently false and shows ignorance of the science discussed. The decline actually refers to a decline in tree growth at certain high-latitude locations since 1960.\nTree-ring growth has been found to match well with temperature. Hence, tree-rings are used to plot temperature going back hundreds of years. However, tree-rings in some high-latitude locations diverge from modern instrumental temperature records after 1960. This is known as the \"divergence problem\". Consequently, tree-ring data in these high-latitude locations are not considered reliable after 1960 and should not be used to represent temperature in recent decades.\nThe \"decline\" has nothing to do with \"Mike's trick\".\nPhil Jones talks about \"Mike's Nature trick\" and \"hide the decline\" as two separate techniques. However, people often abbreviate the email, distilling it down to \"Mike's trick to hide the decline\". Professor Richard Muller from Berkeley commits this error in a public lecture:\n\"A quote came out of the emails, these leaked emails, that said \"let's use Mike's trick to hide the decline\". That's the words, \"let's use Mike's trick to hide the decline\". Mike is Michael Mann, said \"hey, trick just means mathematical trick. That's all.\" My response is I'm not worried about the word trick. I'm worried about the decline.\"\nMuller quotes \"Mike's nature trick to hide the decline\" as if its Phil Jones's actual words. However, the original text indicates otherwise:\n\"I’ve just completed Mike’s Nature trick of adding in the real temps to each series for the last 20 years (ie from 1981 onwards) and from 1961 for Keith’s to hide the decline.\"\nIt's clear that \"Mike's Nature trick\" is quite separate to Keith Briffa's \"hide the decline\". \"Mike's Nature trick\" refers to a technique (a \"trick of the trade\") by Michael Mann to plot recent instrumental data along with reconstructed past temperature. This places recent global warming trends in the context of temperature changes over longer time scales.\nThere is nothing secret about \"Mike's trick\". Both the instrumental and reconstructed temperature are clearly labelled. Claiming this is some sort of secret \"trick\" or confusing it with \"hide the decline\" displays either ignorance or a willingness to mislead.\nFigure 1: Northern Hemisphere mean temperature anomaly in °C (Mann et al 1999).\nThe \"decline\" has been openly and publicly discussed since 1995\nSkeptics like to portray \"the decline\" as a phenomena that climate scientists have tried to keep secret. In reality the divergence problem has been publicly discussed in the peer-reviewed literature since 1995 (Jacoby 1995). The IPCC discuss the decline in tree-ring growth openly both in the 2001 Third Assessment Report and in even more detail in the 2007 Fourth Assessment Report.\nThe common misconception that scientists tried to hide a decline in global temperatures is false. The decline in tree-ring growth is plainly discussed in the publicly available scientific literature. The divergence in tree-ring growth does not change the fact that we are currently observing many lines of evidence for global warming. The obsessive focus on a misquote taken out of context, doesn't change the scientific case that human-caused climate change is real.\nBasic rebuttal written by doug_bostrom\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "Sea level fell in 2010",
   "paragraph": "Why did sea level fall in 2010?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nSea level fluctuations during El Niño (rising) and La Niña (falling) are the result of large exchanges of water between land and ocean in the form of rain and snow. This averages out to zero over time. It does not affect long-term sea level rise, which comes from melting icesheets, glaciers, and thermal expansion.\nClimate Myth...\nSea level fell in 2010\nLarge sea level fall in 2010 means IPCC sea level projections are wrong.\nA number of climate not-so-skeptics have been exploiting global sea level data in their latest attempt to hide the incline. Skeptical Science readers will be very familiar with the tactics the \"skeptics\" use to make this argument:\nCherrypick a very small amount of data during which the short-term noise has dampened the long-term incline.\nIgnore the long-term trend.\nRefuse to examine the reasons behind the short-term change.\nClimate \"skeptics\" have used this exact same strategy to hide the incline in global surface temperatures (here and here and here), lower troposphere temperatures (here), and ocean heat content (here and here). We've found that an effective way to reveal the deception of these arguments is with an animated GIF, comparing the long-term data with the short-term \"skeptic\" cherrypick. Figure 1 makes this comparison for the global mean sea level data during the satellite radar altimiter record (since 1993) from the University of Colorado. The first frame shows the entire record, the second shows four periods of flat or declining mean sea level, and the third shows the most recent short-term decline.\nFigure 1: University of Colorado global mean sea level data with a 12-month running average, and short-term declines.\nCause of Short-Term Decline\nFigure 1 confirms that yes, global mean sea level has declined slightly over the past year or so, and even slightly more than previous recent short-term declines. But a true skeptic should ask what has caused this short-term decline, especially since it appears counter-intuitive. After all, land-based ice continues to melt rapidly, and the oceans continue to warm rapidly (thermal expansion of ocean water contributes to sea level rise). So what has dampened the long-term sea leve rise illustrated in Figure 1?\nAs Skeptical Science has previously reported, climate scientists attribute the short-term decline to extreme flooding in 2010. This period also saw a strong La Niña cycle, which typically results in an increase of rain and snow falling over land, which corresponds with a fall in global sea level. 2009 to 2011 saw some epic deluges throughout the world; countries such as Pakistan, Sri Lanka, Australia, the Philippines, Brazil, Colombia and the United States have been hammered with extreme flooding. Figure 2 illustrates where the water has gone.\nFigure 2: change in land-based global water storage in the period March 2010 to March 2011, as observed by GRACE gravity satellites. Image from NASA JPL.\nCherry-Flavored Water\nIn short, arguments that sea level rise has stopped are based on the same tired old \"skeptic\" tricks of cherrypicking short-term data and ignoring the long-term trend. We know that ocean warming and melting ice will cause sea level to rise over the long-term, and the only reason the sea level rise has temporarily slowed is that there was so much flooding in 2010 - hardly a result worth celebrating. As long as humans continue to warm the planet by increasing the amount of greenhouse gases in the atmosphere, we can expect the long-term sea level rise to continue.\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 24 August 2017 by skeptickev. View Archives"
  },
  {
   "title": "Sea level is not rising",
   "paragraph": "Doctoring sea level data\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThe claim sea level isn’t rising is based on blatantly doctored graphs contradicted by observations.\nClimate Myth...\nSea level is not rising\n\"Together, these two unaltered [sea level] datasets indicate that global mean sea level trend has remained stable over the entire period 1992-2007, altogether eliminating the apparent 3.2 mm/year rate of sea-level rise arising from the “adjusted” data.\" (Christopher Monckton)\nWe have a new entry in the contest for most bizarre \"skeptic\" argument. The \"Science\" and Public Policy Institute (\"S\"PPI) Monthly CO2 Report for January 2011, edited by Christopher Monckton, claims on page 29: \"Sea level is not rising.\"\nThis is of course false. In fact, sea level is not only rising, but the rise is accelerating. Immediately below this false claim, the document contains the following figure:\nNotice that the caption claims the blue curve is observational sea level data \"up to 1960 according to Professor Morner.\" Of course, after 1960 (when the \"observations\" are apparently just made up) is not only when the \"observations\" supposedly diverge from the models, but also when they diverge from reality! Ah, but it gets even worse from here. On page 33, the document presents what may be my favorite figure of all time:\nMonckton and \"S\"PPI have taken the sea level graph from the University of Colorado at Boulder which shows a 3.2 milimeter per year sea level rise trend (as is still visible in the bottom right of the graph), rotated it to make the trend look flat, and claimed that this is what the \"unaltered\" and \"uncorrected\" data looks like.\nIt boggles the mind that Monckton and the \"S\"PPI think they can convince people that sea level has dropped since 1950 based on nothing more than their own unsubstantiated claims and blatantly doctored graphs which are completely contradicted by the actual observational data.\nBasic rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Sea level rise is decelerating",
   "paragraph": "Is sea level rise accelerating?\nLink to this page\nWhat the science says...\nLooking at global data (rather than tide gauge records just from the U.S.) show that sea level rise has been increasing since 1880. The recent rate of sea level rise is greater than its average value since 1930. As for future sea level rise, these predictions are based on physics, not statistics.\nClimate Myth...\nSea level rise is decelerating\n\"A former research director with the Army Corps of Engineers and a former civil-engineering professor at the University of Florida decided to put the sea-rise claims to the test. They gathered U.S. tide-gauge readings from 57 stations where water levels had been continuously recorded for as long as 156 years. The result did suggest the sea level was increasing in the western Pacific, but this was offset by a drop in the level near the Alaskan coast. “Our analyses do not indicate acceleration in sea level in U.S. tide gauge records during the 20th century,” the study’s authors concluded. “Instead, for each time period we consider, the records show small decelerations that are consistent with a number of earlier studies of worldwide-gauge records.\" (Washington Times)\nA paper by Houston & Dean studies 57 tide gauge records from the U.S. (including Hawaii and oceanic territories) and concludes that sea level rise has not accelerated. In fact the authors seem to go out of their way to state that the average result shows deceleration at every opportunity. But there are some big questions about their analysis. Why do they use tide gauge records from just U.S. stations? Why not a global sample? Why use individual tide gauge records when we have perfectly good combinations, from much larger samples, which give a global picture of sea level change and show vastly less noise? Why do they restrict their analysis to either the time span of the individual tide gauge records, or to the period from 1930 to 2009? Why do they repeatedly drone on about “deceleration” when the average of the acceleration rates they measure, even for their extremely limited and restricted sample, isn’t statistically significant?\nBut the biggest question of all is: what’s the big deal?\nHere’s some sea level data, in fact two data sets. One is a global combination of tide gauge records by Domingues et al. (2008, Nature, 453, 1090-1094, doi:10.1038/nature07080). Using around 500 tide gauge records globally, it’s the latest version of the “Church & White” dataset. The other is satellite data:\nI averaged the two data sources during their period of overlap, and computed a smoothed version:\nThis is a global data set, and it’s a worldwide average so its shows vastly less noise than individual tide gauge records. We could even use it to look for acceleration or deceleration in sea level rise. But one thing we should not do is restrict consideration to the quadratic term of a quadratic polynomial fit from 1930 onward. That would be pretty ignorant — maybe even misleading.\nAs so often happens, one thing to be cautious of is that the noise shows autocorrelation. As Houston & Dean point out, the Church & White data since 1930 are approximately linear, so to get a conservative estimate of the autocorrelation I used the residuals from a linear fit to just the post-1930 data and fit an ARMA(1,1) model.\nIf we compute the linear trend rate for all possible starting years from 1880 to 1990, up to the present, we get this:\nAccording to this, the recent rate of sea level rise is greater than its average value since 1930. Significantly so (in the statistical sense), even using a conservative estimate of autocorrelation. But the increase itself hasn’t been steady, so the sea level curve hasn’t followed a parabola, most of the increase has been since about 1980. How could Houston & Dean have missed this?\nHere’s how: first, they determined the presence or absence of acceleration or deceleration based only on the quadratic term of a quadratic fit. That utterly misses the point. Changes in the rate of sea level rise don’t have to follow a parabola, since 1930 or any time point you care to name. In fact, by all observations and predictions, they have not done so and will not do so.\nSecond, by using individual tide gauge records, the noise level is so high that you can’t really hope to find acceleration or deceleration of any kind, with any consistency. Not using quadratic fits, and certainly the non-parabolic trend which is present can’t be found in such noisy data sets.\nEven so, we can also fit a quadratic (as Houston & Dean did), and estimate the acceleration (which is twice the quadratic coefficient):\nWell well … it looks like starting at 1930 is the way to get the minimum “acceleration” by this analysis method. Could that be why Houston & Dean chose 1930 as their starting point?\nIf we restrict to only the data since 1930, as Houston & Dean did, and fit a quadratic trend, we get this:\nCan you tell, just by looking, whether it curves upward or downward? Clearly, the parabolic fit doesn’t show much acceleration or deceleration, if any. We can get a better picture by first subtracting a linear fit, then fitting a parabola to the residuals?\nThat answers the question: the quadratic fit shows acceleration in the Church & White data. But, when autocorrelation is taken into account, the “acceleration” is not statistically signficant.\nBut — just because the data don’t follow a parabola, doesn’t mean that sea level hasn’t accelerated. Let’s take those residuals from a linear model, and fit a cubic polynomial instead:\nWell well … there seems to be change after all, with both acceleration and deceleration but most recently, acceleration. And by the way, this fit is significant.\nAnd now to the really important part, which is not the math but the physics. Whether sea level showed 20th-century acceleration or not, it’s the century coming up which is of concern. And during this century, we expect acceleration of sea level rise because of physics. Not only will there likely be nonlinear response to thermal expansion of the oceans, when the ice sheets become major contributors to sea level rise, they will dominate the equation. Their impact could be tremendous, it could be sudden, and it could be horrible.\nThe relatively modest acceleration in sea level so far is not a cause for great concern, but neither is it cause for comfort. The fact is that statistics simply doesn’t enable us to foresee the future beyond a very brief window of time. Even given the observed acceleration, the forecasts we should attend to are not from statistics but from physics.\nMany thanks to Tamino from Open Mind for allowing us to republish his post So What?\nAdvanced rebuttal written by Tamino\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Sea level rise is exaggerated",
   "paragraph": "How much is sea level rising?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nA variety of different measurements find steadily rising sea levels over the past century.\nClimate Myth...\nSea level rise is exaggerated\n\"We are told sea level is rising and will soon swamp all of our cities. Everybody knows that the Pacific island of Tuvalu is sinking. ...\nAround 1990 it became obvious the local tide-gauge did not agree - there was no evidence of 'sinking.' So scientists at Flinders University, Adelaide, set up new, modern, tide-gauges in 12 Pacific islands.\nRecently, the whole project was abandoned as there was no sign of a change in sea level at any of the 12 islands for the past 16 years.\" Vincent Gray).\nGavin Schmidt investigated the claim that tide gauges on islands in the Pacific Ocean show no sea level rise and found that the data show a rising sea level trend at every single station. But what about global sea level rise?\nSea level rises as ice on land melts and as warming ocean waters expand. As well as being a threat to coastal habitation and environments, sea level rise corroborates other evidence of global warming\nThe blue line in the graph below clearly shows sea level as rising, while the upward curve suggests sea level is rising faster as time goes on. The upward curve agrees with global temperature trends and with the accelerating melting of ice in Greenland and other places.\nBecause sea level behavior is such an important signal for tracking climate change, skeptics seize on the sea level record in an effort to cast doubt on this evidence. Sea level bounces up and down slightly from year to year so it's possible to cherry-pick data falsely suggesting the overall trend is flat, falling or linear. You can try this yourself. Starting with two closely spaced data points on the graph below, lay a straight-edge between them and notice how for a short period of time you cancreate almost any slope you prefer, simply by being selective about what data points you use. Now choose data points farther apart. Notice that as your selected data points cover more time, the more your mini-graph reflects the big picture. The lesson? Always look at all the data, don't be fooled by selective presentations.\ngraph from Church 2008\nOther skeptic arguments about sea level concern the validity of observations, obtained via tide gauges and more recently satellite altimeter observations.\nTide gauges must take into account changes in the height of land itself caused by local geologic processes, a favorite distraction for skeptics to highlight. Not surprisingly, scientists measuring sea level with tide gauges are aware of and compensate for these factors. Confounding influences are accounted for in measurements and while they leave some noise in the record they cannot account for the observed upward trend.\nVarious technical criticisms are mounted against satellite altimeter measurements by skeptics. Indeed, deriving millimeter-level accuracy from orbit is a stunning technical feat so it's not hard to understand why some people find such an accomplishment unbelievable. In reality, researchers demonstrate this height measurement technique's accuracy to be within 1mm/year. Most importantly there is no form of residual error that could falsely produce the upward trend in observations.\nAs can be seen in an inset of the graph above, tide gauge and satellite altimeter measurements track each other with remarkable similarity. These two independent systems mutually support the observed trend in sea level. If an argument depends on skipping certain observations or emphasizes uncertainty while ignoring an obvious trend, that's a clue you're being steered as opposed to informed. Don't be mislead by only a carefully-selected portion of the available evidence being disclosed.\nCurrent sea level rise is after all not exaggerated, in fact the opposite case is more plausible. Observational data and changing conditions in such places as Greenland suggest if there's a real problem here it's underestimation of future sea level rise. IPCC synthesis reports offer conservative projections of sea level increase based on assumptions about future behavior of ice sheets and glaciers, leading to estimates of sea level roughly following a linear upward trend mimicking that of recent decades. In point of fact, observed sea level rise is already above IPCC projections and strongly hints at acceleration while at the same time it appears the mass balance of continental ice envisioned by the IPCC is overly optimistic (Rahmstorf 2010 ).\nBasic rebuttal written by doug_bostrom\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Sea level rise predictions are exaggerated",
   "paragraph": "How much will sea levels rise in the 21st Century?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nSea levels are rising faster now than in the previous century, and could rise between 50cm to 1.5 metres by 2100\nClimate Myth...\nSea level rise predictions are exaggerated\n\"Professor Niklas Mörner, who has been studying sea level for a third of a century, says it is physically impossible for sea level to rise at much above its present rate, and he expects 4-8 inches of sea level rise this century, if anything rather below the rate of increase in the last century. In the 11,400 years since the end of the last Ice Age, sea level has risen at an average of 4 feet/century, though it is now rising much more slowly because very nearly all of the land-based ice that is at low enough latitudes and altitudes to melt has long since gone.\" (Christopher Monckton)\nMeasuring Sea Levels\nSea levels are rising due to thermal expansion and melting of land-based ice. Global warming is causing the oceans to absorb a lot of extra heat (up to 90%). This makes the volume of water expand, and sea levels rise. The Greenland and Antarctic ice caps, and many of the world’s glaciers, are all slowly melting. The runoff feeds into rivers and directly into the oceans. This too adds to sea levels.\nPrior to the use of satellite systems, measurements were taken using tide-gauges, devices that measure the height of a water level relative to a fixed point on land. Global estimates of sea level rise were subject to substantial differences in measurement from different parts of the world.\nSea levels change all the time. They are affected by seasons, astronomical tides, storm surges, currents and density, among other influences. Tidal gauges reflect these short term influences, introducing a large margin of error.\nThe IPCC Fourth Assessment Report described studies that estimated sea level rise for the 20th century between 0.5 and 3.0 mm a year. The most likely range, according to the IPCC, was between 1.0 and 2.0 mm a year.\nSatellite altimetry since 1993 provides a more accurate measure of global sea level rise. Three different satellites take measurements: TOPEX/Poseidon (launched 1992), Jason-1 (launched 2001) and Jason-2 (launched 2008).\nFigure 1: Source - CSIRO\nThe IPCC projections are derived from climate models. Using both tide gauge and satellite data, we can see that sea levels are rising. Unfortunately, sea level rise is already tracking the worst-case projections, as this graph shows:\nFigure 2: Sea level change. Tide gauge data are indicated in red and satellite data in blue. The grey band shows the projections of the IPCC Third Assessment report (Allison et al 2009).\nIn fact, the climate models underestimated the rate of sea level rise because the rapid melting of the ice sheets and glaciers was not incorporated in the last IPCC report. (It was left out because the data were not considered sufficiently robust).\nDamaging Potential\nRising sea levels are widely considered to be the greatest threat posed by climate change. They threaten low-lying countries with inundation, forcing inhabitants to migrate. Coastal cities and ports could be flooded, as could cities sited near tidal estuaries, like London. Many nuclear installations are built by the sea so they can use sea water for cooling.\nThe potential for sea level rise is enormous. This is because the ice caps - Greenland and Antarctic - contain huge amounts of fresh water - around 70% of all the freshwater on Earth. Estimates suggest that if the Greenland ice sheet was to melt away to nothing, sea levels would rise around 6 metres. To put that a different way, a loss of just one per cent of the Greenland ice cap would result in a sea level rise of 6cm.\nIf the West Antarctic Ice Sheet (WAIS) were to melt, this would add around 6 metres to sea levels. If the East Antarctic Ice Sheet (EAIS) were to melt as well, seas would rise by around 70 metres.\nIn a process that is accelerating, all three ice caps are losing mass. While nobody is suggesting any of the ice caps will melt away to nothing, only a small amount of melting would cause great problems.\nA 1% loss of ice from these three sources would produce a likely increase in sea levels of around 76cm. With the thermal expansion implied by such melting, and contributions from melting glaciers, the oceans would actually rise far more.\nPredictions for future sea levels\nFuture sea level rises depend on a number of factors. The amount of CO2 emitted will determine how much global warming takes place. The amount of ice that melts will vary according to the amount of global warming. The same is true of thermal expansion.\nPrevious estimates of sea level rise have been based on a set of possible outcomes called emissions scenarios. These theoretical scenarios range from emissions which fall very quickly, to emissions that continue to rise even faster than they have already. Scientists then calculate possible outcomes for each scenario.\nIn the next IPCC report (AR5), due in 2014, a new method has been used. Emission scenarios have been replaced by Representative Concentration Pathways (RCP). Four trajectories were chosen, based not on emissions, but possible greenhouse gas concentrations in the year 2100. From the concentrations, the RCPs project a ‘forcing’ for each pathway (the amount of warming); 2.6, 4.5, 6.0, and 8.5 Watts per metre squared. Each pathway is named after it’s forcing e.g. RCP4.5. The lowest emission scenario is also referred to as RPC3PD, because it posits a peak warming of 3 w/m2 by 2070 (~490 ppm CO2 and equivalents), and a reduction to 2.6 w/m2 by 2100. (PD stands for Peak/Decline).\nA draft version of the next report from the IPCC (AR5), due for publication in 2014, was recently leaked. Although the information is subject to change, the draft report says sea levels are likely to rise by between 29 and 82 centimeters by the end of the century, (compared to 18-59 centimeters in the 2007 report).\nOther recent studies have projected comparable sea level increases. Jevrejeva 2011 for example modelled sea level rise using RPC scenarios. ) This table shows best and worst cases (RPC3PD and RCP8.5), with two in between. The figures for each projection are listed in this table:\nTable 1: Projected sea level rise (m) by 2100 for the RCP scenarios. Results presented as median, upper (95% confidence interval) and lower (5% confidence interval) limits, calculated from 2,000,000 model runs. Sea level rise is given relative the period 1980–2000. (Jevrejeva 2011)\nAnother study (Rahmstorf 2011) obtained much the same results:\nFigure 4: Sea level hindcasts and projections for different models calibrated with different temperature and sea level data. The error bars on the right indicate 90% confidence intervals (5–95 percentile, using the GISS temperature dataset); for the proxy-based projection the uncertainty is as presented in Kemp et al., 2011. (Rahmstorf 2011)\nWhat's in the pipeline?\nThe 'pipeline' is a term used to describe the slow reaction of the oceans to heating (inertia). Even if we were to stop emitting greenhouse gases tomorrow, the oceans would continue to rise, driven by the heat already stored. (90% of all the sun's energy falling on the surface of Earth is absorbed by the oceans as heat). This sea level rise is said to be 'in the pipeline'.\nA paper published in PNAS - Levermann 2013 - has found that greenhouse gases emitted today will cause sea levels to rise for several centuries. For every degree of warming, sea levels will rise by more than 2 meters in the next few centuries. The Earth's temperature has already risen 0.8 degrees C over pre-industrial temperatures.\nJevrejeva 2011 also found increased rates of sea level rise, even if emissions were to stabilise at 490 ppm by 2070 following the scenario in RPC3PD (RPC2.6):\nTable 2: Projected sea level rise (m) by 2500 for the RCP scenarios. Results presented as median, upper (95% confidence interval) and lower (5% confidence interval) limits, calculated from 2,000,000 runs of the model. Values of sea level rise are given relative the period 1980–2000.\nConclusion\nBased on the new mid-range IPCC RCP4.5 scenario - around 650 ppm CO2 and equivalents producing a forcing of approximately 4.5 watts/metre2 - the most likely sea level rise by 2100 is betweem 80cm and 1 metre. Longer term, sea levels will continue to rise even after emissions have been reduced or eliminated.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 12 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "Skeptics were kept out of the IPCC?",
   "paragraph": "Were skeptic scientists kept out of the IPCC\nLink to this page\nWhat the science says...\nThe Independent Climate Change Email Review investigated the CRU scientists' actions as IPCC authors. Official records, Review Editors, and even the emails themselves suggest the CRU scientists acted in the spirit if not the letter of the IPCC rules. Anyway, the relevant texts were team responsibilities.\nClimate Myth...\nSkeptics were kept out of the IPCC?\n\"The Team had tampered with the complex, bureaucratic processes of the UN’s climate panel, the IPCC, so as to exclude inconvenient scientific results from its four Assessment Reports, and to influence the panel’s conclusions for political rather than scientific reasons. The Team had conspired in an attempt to redefine what is and is not peer-reviewed science for the sake of excluding results that did not fit what they and the politicians with whom they were closely linked wanted the UN’s climate panel to report.\" (Christopher Monckton)\nExhibit No. 1 of the climate conspiracy theory is a collection of emails stolen from the Climatic Research Unit (CRU) of the University of East Anglia (UEA), which appeared on the internet in November 2009. Though some of these \"Climategate\" emails can sound damning when quoted out of context, several inquiries have cleared the scientists. The most comprehensive inquiry, the Independent Climate Change Email Review, did something the media completely failed to do: it put the emails into context by investigating the main allegations. Its general findings (summarised here) were that the scientists' rigour and honesty are not in doubt, and their behaviour did not prejudice the advice given to policymakers, though they did fail to display the proper degree of openness.\nOne set of allegations is that CRU scientists abused their positions on Intergovernmental Panel on Climate Change (IPCC) writing groups to impede the consideration of papers challenging CRU’s work. Two papers in particular: the first about the instrumental record, and the second about tree-ring-based temperature reconstructions. The Review goes into each of these allegations in meticulous detail.\nThe first paper, McKitrick and Michaels (2004), or “MM2004”, argued that most of the observed late 20th century warming was due to the urban heat island effect. Jones’ reaction to the paper, according to an email dated 8/7/2004, was:\nThe other paper by MM is just garbage. […] I can’t see either of these papers being in the next IPCC report. Kevin and I will keep them out somehow — even if we have to redefine what the peer-review literature is!\nThe MM2004 paper was indeed omitted from the first and second drafts of AR4 WG1 Chapter 3, but mentioned and refuted in the final text. McKitrick claims that Jones wrote that paragraph and that it gave contrived reasons for rejecting the paper’s conclusions.\nThe second paper, McIntyre and McKitrick (2003), or “M&M2003”, criticized the famous “hockey stick” proxy temperature reconstruction by Mann, Bradley, and Hughes (1998), or “MBH98”. It argued that the “hockey stick” shape was primarily an artifact of statistical errors and the selection of specific tree ring series. The allegation is that Briffa, as lead author of AR4 WG1 Chapter 6, broke IPCC rules to include a paper by Wahl and Ammann then in press, or “WA2007”; which refuted M&M2003 but was clearly published after the deadline for inclusion in the AR4. Contrarians cite an email dated 18/7/2006, in which Briffa wrote to Wahl:\nGene I am taking the liberty (confidentially) to send you a copy of the reviewers comments (please keep these to yourself) of the last IPCC draft chapter. I am concerned that I am not as objective as perhaps I should be and would appreciate your take on the comments from number 6-737 onwards, that relate to your reassessment of the Mann et al work. I have to consider whether the current text is fair or whether I should change things in the light of the sceptic comments. In practice this brief version has evolved and there is little scope for additional text, but I must put on record responses to these comments — any confidential help, opinions are appreciated.\nContrarians argue the above email is evidence of Briffa breaking rules of confidentiality to ask for help in rebutting criticism of WA2007. Chapter 6 contained a paragraph referencing WA2007 as a rebuttal of M&M2003, which contrarians assumed to have been written by Briffa.\nThe Review asked Jones about the MM2004 allegations. He stated that the “keep them out” email was “sent on the spur of the moment and quickly forgotten”, but there were good scientific reasons for his intention to exclude MM2004. (Namely, it did not account for signals like El Niño; and in any case its conclusions about the land temperature record are at odds with the independent lines of evidence provided by the ocean and satellite records.) Jones also denied having written the paragraph in question, saying the inclusion of MM2004 was a collective decision by the Chapter 3 writing team. IPCC records confirm that MM2004 was discussed by the group.\nThe inquiry also took evidence from one of the three Review Editors for Chapter 3, Professor Sir Brian Hoskins. He “was very impressed by Jones’ attention to detail, and the rigour of the Chapter 3 process.” He pointed out the writing group had joint responsibility for the text and it was unlikely for one voice to have dominated.\nThe Review found the rebuttal of MM2004 does not appear to have been “invented”. Instead there has been “a consistence of view amongst those who disagree with MM2004 that has been sustained over the last 6 years”. Overall, the Review found no more than “mere speculation” that MM2004 was unfairly excluded:\nWe conclude that there is evidence that the text was a team responsibility. It is clear that Jones (though not alone) had a strongly negative view of the paper but we do not find that he was biased, that there was any improper exclusion of material or that the comments on the MM2004 paper in the final draft were “invented” given the (continuing) nature of the scientific debate on the issue. [9.3.6]\nWhat about the treatment of M&M2003? Briffa says the text was the responsibility of the entire Chapter 6 writing group, and they took M&M2003 very seriously. That paper excluded 77 of the 95 pre-1500 tree ring series used in MBH98, and WA2007 showed that the results of MBH98 could be replicated very closely using all the data. The AR4 text did not state that WA2007 had disproved the criticisms made by M&M2003, merely that their impact might be relatively small. Besides, MBH98 was only one of the 12 reconstructions shown in Figure 6.10 (M&M2003 was not shown because McKitrick commented that “we are not trying to offer ‘our’ climate history curve”).\nProfessor John Mitchell, one of the two Chapter 6 Review Editors, confirmed there was group responsibility, and told the Review that referencing unpublished material in the AR4 was not prohibited, but only allowed under exceptional circumstances. However, he says Briffa’s confidential email is “problematic”:\nOn the one hand it appears to reflect an honest request to an expert for a comment about the extent to which the author is being balanced and fair. On the other hand, it stresses the need for confidentiality in three places, implying that the author realizes that the approach may be improper. There was also a leak of an early draft of the WG1 report to the press which led to IPCC emphasizing the need to maintain confidentiality in general which may have been at the back of the author’s mind. [9.4.5]\nThe Review was persuaded that M&M2003 was “dealt with in a careful and reasonable fashion”. They found that the inclusion of WA2007 was “to ensure that assessments were as up to date as possible” and “appear[s] to be consistent with IPCC principles”. As for the allegation of breaking confidentiality, the IPCC rules do not prevent authors asking experts for objective advice.\nBut arguably the best evidence that Briffa was acting in good faith can be found in the emails themselves. Many of Briffa’s emails actually suggest a desire to ensure that uncertainty was fully explained. (Indeed, as the Review points out, “the e-mail correspondence with Wahl stresses in several places Briffa’s concern to be fair to sceptical views.”) I think they are worth quoting at some length. In an email dated 29/4/2003, Keith Briffa wrote (my emphasis):\nCan I just say that I am not in the MBH camp — if that be characterized by an unshakable “belief” one way or the other, regarding the absolute magnitude of the global MWP. I certainly believe the “medieval” period was warmer than the 18th century — the equivalence of the warmth in the post 1900 period, and the post 1980s, compared to the circa Medieval times is very much still an area for better resolution. […] On present evidence, even with such uncertainties I would still come out favouring the “likely unprecedented recent warmth” opinion…\nIn an email dated 3/2/2006, Briffa wrote:\nwe are having trouble to express the real message of the reconstructions — being scientifically sound in representing uncertainty, while still getting the crux of the information across clearly. It is not right to ignore uncertainty, but expressing this merely in an arbitrary way (and a total range as before) allows the uncertainty to swamp the magnitude of the changes through time. We have settled on this version (attached) of the Figure which we hoe [sic] you will agree gets the message over but with the rigor required for such an important document.\nIn an email dated 15/2/2006, Briffa wrote:\nWe should be careful not to push the conclusions beyond what we can securely justify — and this is not much other than a confirmation of the general conclusions of the TAR. […] Let us not try to over egg the pudding. For what it worth, the above comments are my (honestly long considered) views — and I would not be happy to go any further. Of course this discussion now needs to go to the wider Chapter authorship, but do not let Susan (or Mike) push you (us) beyond where we know is right.\nTo the Review Team (and to me), these emails suggest “Briffa was unlikely to be an uncritical defender of the MBH view of the ‘hockey stick’, and wished to respect the view of the writing team as a whole”. Basically, it looks like Briffa adhered to the spirit if not the letter of the IPCC rules.\nThe Review concluded that neither allegation of misuse of the IPCC process could be upheld: “neither Jones nor Briffa behaved improperly”, and both “were part of large groups of scientists taking joint responsibility” for the relevant IPCC texts. [9.5]\nDespite being heralded as “the final nail in the coffin of anthropogenic global warming”, Climategate did not even demonstrate corruption of the IPCC process, let alone corruption of the climate science community. In any case, the CRU scientists' influence extended to a couple of IPCC chapters covering only a small part of the large body of evidence for anthropogenic global warming. That mountain of evidence cannot be explained away by the behaviour of a few individuals.\nLast updated on 24 December 2010 by James Wight."
  },
  {
   "title": "Soares finds lack of correlation between CO2 and temperature",
   "paragraph": "Ignoring long-term trends due to distracting long-term trends\nLink to this page\nWhat the science says...\nSoares looks at short-term trends which are swamped by natural variations. Increasing CO2 causes a gradual long-term warming trend which is smaller than the short-term variations. The long-term correlation between CO2 and temperature is well established.\nClimate Myth...\nSoares finds lack of correlation between CO2 and temperature\n\"The comparison of temperature changes and CO2 changes in the atmosphere is made for a large diversity of conditions, with the same data used to model climate changes. Correlation of historical series of data is the main approach. CO2 changes are closely related to temperature. Warmer seasons or triennial phases are followed by an atmosphere that is rich in CO2, reflecting the gas solving or exsolving from water, and not photosynthesis activity. Interannual correlations between the variables are good. A weak dominance of temperature changes precedence, relative to CO2 changes, indicate that the main effect is the CO2 increase in the atmosphere due to temperature rising. Decreasing temperature is not followed by CO2 decrease, which indicates a different route for the CO2 capture by the oceans, not by gas re-absorption. Monthly changes have no correspondence as would be expected if the warming was an important absorption-radiation effect of the CO2 increase. The anthropogenic wasting of fossil fuel CO2 to the atmosphere shows no relation with the temperature changes even in an annual basis. The absence of immediate relation between CO2 and temperature is evidence that rising its mix ratio in the atmosphere will not imply more absorption and time residence of energy over the Earth surface.\" (Paulo Soares)\nA recent paper in an obscure journal (Soares, 2010) used correlations between temperatures and CO2 concentrations to conclude that;\n\"The absence of immediate relation between CO2 and temperature is evidence that rising its mix ratio in the atmosphere will not imply more absorption and time residence of energy over the Earth surface. This is explained because band absorption is nearly all done with historic CO2 values.\"\nSoares looks at correlations between change in CO2 and change in temperature for a month to a few years. He doesn't find a correlation between short term CO2 changes and temperature changes in the following months. His Figure 8 shows the change in temperature or CO2 from one year to the next.\nDo we live in Soares’ world where CO2 isn’t causing warming, or in the world of mainstream physics where theory and measurements show increased CO2 heating? What does mainstream physics expect to see in the above graph?\nFirstly it expects atmospheric temperatures to change regularly: natural cycles like El Nino transfer heat from the oceans and can change atmospheric temperature by up to 0.4 °C in a year causing the big vertical spread.\nThe graph below is based on Meehl et al, 2004 and shows a climate model estimate of how much global warming was expected from greenhouse gases for the past century: always less than 0.02 °C/year - so small that the noise effectively hides the incline if you only look at year to year changes. Fortunately, very simple statistical techniques work around this.\nSome rough calculations using the NASA global data shows that to detect the expected CO2 global warming for the past 40 years at 95% confidence would require about 160 years of measurements - and hundreds more measurements to detect the CO2 signal when it is smaller.\nWe have some more expectations for the graph: low CO2 emissions should mean slow warming and vice versa. On the left of the graph we expect average warming of under 0.01 °C/yr and on the right hand side we expect just under 0.02 °C/yr. So if you plot a slope you expect it to be positive – going from 0.01 on the left up to 0.02 on the right but practically impossible to find amongst so much noise (although Soares does plot it).\nThe next trick is to implicitly assume that nothing else shows a warming or cooling pattern: but we know that there is. From the 1940s to the 1970s we pumped enough reflective aerosols into the atmosphere to temporarily halt global warming by 'global dimming' (Ramanathan et al, 2001).\nThis is like putting a pan of water over a lit gas stove and then dropping in an icepack big enough to cool the water. Soares would say the cooling shows that burning gas can’t heat water, but mainstream science says that a big pack of ice temporarily masks the heating and that burning gas does, in fact, make water warmer than it would otherwise be. Importantly, you can account for the ice and determine whether the heat is on and other scientists would do this.\nSoares’ method is like searching for a needle in a huge haystack by picking a handful of hay rather than using a magnet. You almost certainly wouldn’t find the needle even if it was there, so to claim you’ve disproved its existence when other scientists have found it with their magnets is simply stunning.\nLast updated on 17 January 2011 by MarkR."
  },
  {
   "title": "Solar Cycle Length proves its the sun",
   "paragraph": "What does Solar Cycle Length tell us about the sun's role in global warming?\nLink to this page\nWhat the science says...\nThe claim that solar cycle length proves the sun is driving global warming is based on a single study published in 1991. Subsequent research, including a paper by a co-author of the original 1991 paper, finds the opposite conclusion. Solar cycle length as a proxy for solar activity tells us the sun has had very little contribution to global warming since 1975.\nClimate Myth...\nSolar Cycle Length proves its the sun\nIn 1991, Eigil Friis-Christensen and Knud Lassen published an article claiming \"strikingly good agreement\" between solar cycle lengths (the fluctuating lengths of cycles undergone by sunspot numbers) and northern hemisphere land temperatures over the period 1860–1990 (Friis-Christensen 1991).\nSolar cycle length is a useful indicator of long term changes in solar activity. When the sun gets hotter, we observe shorter solar cycles. When the sun shows a long term cooling trend, solar cycle length is longer. A 1991 study by Friis-Christensen and Lassen smoothed out data on solar cycle length and compared it to Northern Hemisphere temperature (Friis-Christensen 1991). The authors suggested the close correlation between solar cycle length and temperature supports the direct influence of solar activity on climate over the past 130 years. Note in particular the close correlation after 1980 during the modern global warming trend.\nFigure 1: Changes in solar cycle length (blue crosses) versus change in Northern Hemisphere temperature (red *).\nHowever, the solar cycle data presented in Figure 1 consists of two incongruous sets of data. The first 20 points of the graph are smoothed using a 1-2-2-2-1 running average. However, the last 4 points (marked 1 to 4 in the figure below) are not filtered in the same manner. Points 1 and 2 are only partially filtered. Points 3 and 4 are not filtered at all. In effect, it's like marrying two separate sets of data. When the latest data points are properly filtered using the latest solar data, the decrease in solar cycle length from 1980 disappears (Laut 2003).\nFigure 2: Left: Original solar cycle length data from Friis-Christensen 1991. The last two points, 3 and 4 are due to errors in the authors’ arithmetic. Right: Updated solar cycle lengths using latest data from Thejll 2000.\nIn 1999, one of the co-authors of the original 1991 paper updated their analysis with the latest data (Lassen 1999). They found that the solar cycle length showed no trend in the last few decades of global warming. They concluded that \"since around 1990 the type of Solar forcing that is described by the solar cycle length model no longer dominates the long-term variation of the Northern hemisphere land air temperature\".\nFigure 3: the top figure compares temperature to solar cycles. The bottom figure plots the difference between temperature and solar cycle length, showing a strong divergence in the mid 1970s (Lassen 1999).\nOther studies confirm Lassen's conclusion:\nKelly 1992 models the effects of a combination of greenhouse and solar-cycle-length forcing and compare the results with observed temperatures. They find that \"even with optimized solar forcing, most of the recent warming trend is explained by greenhouse forcing\".\nLaut 1998 analyses the period 1579–1987 and finds \"the solar hypothesis—instead of contradicting—appears to support the assumption of a significant warming due to human activities\".\nDamon 1999 uses the pre-industrial record as a boundary condition and finds the SCL-temperature correlation corresponds to an estimated 25% of global warming to 1980 and 15% to 1997.\nBenestad 2005 concludes \"There have been speculations about an association between the solar cycle length and Earth's climate, however, the solar cycle length analysis does not follow Earth's global mean surface temperature. A further comparison with the monthly sunspot number, cosmic galactic rays and 10.7 cm absolute radio flux since 1950 gives no indication of a systematic trend in the level of solar activity that can explain the most recent global warming\".\nClaims that solar cycle length prove the sun is causing global warming are based on a single paper published nearly 20 years ago. Subsequent research, including a paper by a co-author of the original 1991 paper, finds the opposite conclusion. Solar cycle length as a proxy for solar activity tells us the sun has had very little contribution to global warming since 1975. In fact, direct measurements of solar activity indicate the sun has had a slight cooling effect on climate in recent decades while global temperatures have been rising.\nLast updated on 26 October 2016 by John Cook. View Archives"
  },
  {
   "title": "Southern sea ice is increasing",
   "paragraph": "Why is southern sea ice increasing?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nAntarctic sea ice has grown in recent decades despite the Southern Ocean warming at the same time.\nClimate Myth...\nSouthern sea ice is increasing\n'Antarctic sea ice set a new record in October 2007, as photographs distributed by the National Oceanic and Atmospheric Administration showed penguins and other cold-weather creatures able to stand farther north on Southern Hemisphere sea ice than has ever been recorded. The news of expanding Antarctic sea ice stole headlines from global warming alarmists who asserted Arctic sea ice had reached its lowest extent since 1979.' (James Taylor)\nFirst of all, it’s worth remembering that sea ice is not to be confused with land ice. This distinction might seem obvious, but the two are often confused in media reports. Sea ice is frozen seawater floating on the surface, whereas land ice is a layer of snow that has accumulated over time on a landmass. Antarctica is losing land ice at an accelerating rate.\nHowever, it is clear that the extent of sea ice around the coast of the continent is growing. Why? The first explanation which comes to mind is that the Southern Ocean must be cooling. But on the contrary, the Southern Ocean has warmed by around 0.5°C in the three decades since satellites began measuring sea ice trends.\nThe true reasons for the increasing ice are a complex set of factors. One factor is an increase in precipitation over the Southern Ocean, which means more snowfall. However, this trend is expected to reverse in coming decades as the Antarctic continues to warm.\nFinally, southern sea ice is not particularly important to the climate. Unlike land ice, sea ice doesn’t affect sea levels because it’s already displacing water. And unlike the situation in the Arctic, where disappearing sea ice is making the Arctic Ocean less reflective and amplifying Arctic warming, a decline in southern sea ice would not warm the Antarctic climate. For as long as climatologists have studied it, the Southern Ocean has been almost ice-free in summer, the time of year when it would receive enough heat from the Sun to have a large effect. The issue of southern sea ice is really just a distraction which diverts our attention from the more important issue of sea ice melt in the Arctic.\nIn conclusion, the increase of southern sea ice does not contradict global warming. The Southern Ocean is in fact warming, the increase of sea ice is due to a variety of factors, and sea ice is not as important to the Antarctic climate as it is to the Arctic.\nBasic rebuttal written by James Wight\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Springs aren't advancing",
   "paragraph": "How is climate change affecting the advance of spring?\nLink to this page\nWhat the science says...\nA synthesis of nearly 400,000 first flowering records covering 405 species across the UK found that British plants are flowering earlier now than at any time in the last 250 years.\nClimate Myth...\nSprings aren't advancing\n'While there are numerous studies that indicate that spring has advanced, there is a recent satellite based study that indicates no consistent trends in the start of spring in North America.' (Indur M. Goklany)\nClimate change is being recognized as one of the most influential drivers of changes in biodiversity. This is particularly evident in the field of phenology, which looks at how climatic changes affecting timing of events in the natural world. Changes in the timing of one part of the ecosystem can have a ripple effect, disrupting other areas. For example, a change in timing of plant flowering can disrupt the creatures that pollinate them. Similarly, changes in timing of plant or insect behaviour can affect the birds that use them as food supplies. New research has been published stitching together nearly 400,000 first flowering records covering 405 species across the UK (Amano et al 2010). They've found that British plants are flowering earlier now than at any time in the last 250 years.\nThere's a strong correlation between temperature and the date when flowers first open each year. Consequently, much information can be gleaned from looking at flowering dates in the past. Since the mid-1700s, sightings have been made by full-time biologists and part-time enthusiasts. Systematic recording of flowering times began in the UK in 1875 by the Royal Meteorological Society. However, many sets of records are short-term, fragmented or focus on just one species. Amano 2010 developed a technique for blending fragmented records in a way that takes into account where the records came from, what length of time they cover and the differences between the flowering times of different species. This enabled them to develop a kind of nationwide, year-long, species-wide average.\nFigure 1: Average (red line) and 95% uncertainty range (grey area) of the estimated first flowering index index (day of the year). The black\nline indicates the average for every 25 years and the dotted line for the most recent 25 years (Amano 2010).\nThere's been a clear advance in the time of first flowering in recent decades. The average first flowering date has been earlier in the last 25 years than in any other period since 1760. The next step in this research is to see whether the same techniques can be employed on a larger scale, to give a regional or global picture of nature's response to temperature change.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 14 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Temp record is unreliable",
   "paragraph": "Are surface temperature records reliable?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nThe warming trend is the same in rural and urban areas, measured by thermometers and satellites, and by natural thermometers.\nClimate Myth...\nTemp record is unreliable\n\"We found [U.S. weather] stations located next to the exhaust fans of air conditioning units, surrounded by asphalt parking lots and roads, on blistering-hot rooftops, and near sidewalks and buildings that absorb and radiate heat. We found 68 stations located at wastewater treatment plants, where the process of waste digestion causes temperatures to be higher than in surrounding areas.\nIn fact, we found that 89 percent of the stations – nearly 9 of every 10 – fail to meet the National Weather Service’s own siting requirements that stations must be 30 meters (about 100 feet) or more away from an artificial heating or radiating/reflecting heat source.\" (Watts 2009)\nTemperature data is essential for predicting the weather. So, the U.S. National Weather Service, and every other weather service around the world, wants temperatures to be measured as accurately as possible.\nTo understand climate change we also need to be sure we can trust historical measurements. A group called the International Surface Temperature Initiative is dedicated to making global land temperature data available in a transparent manner.\nSurface temperature measurements are collected from about 30,000 stations around the world (Rennie et al. 2014). About 7000 of these have long, consistent monthly records (Fig. 1). As technology gets better, stations are updated with newer equipment. When equipment is updated or stations are moved, the new data is compared to the old record to be sure measurements are consistent over time.\nFigure 1. Station locations with at least 1 month of data in the monthly Global Historical Climatology Network (GHCN-M). This set of 7280 stations are used in the global land surface databank. (Rennie et al. 2014)\nIn 2009 some people worried that weather stations placed in poor locations could make the temperature record unreliable. Scientists at the National Climatic Data Center took those critics seriously and did a careful study of the possible problem. Their article \"On the reliability of the U.S. surface temperature record\" (Menne et al. 2010) had a surprising conclusion. The temperatures from stations that critics claimed were \"poorly sited\" actually showed slightly cooler maximum daily temperatures compared to the average.\nIn 2010 Dr. Richard Muller criticized the \"hockey stick\" graph and decided to do his own temperature analysis. He organized a group called Berkeley Earth to do an independent study of the temperature record. They specifically wanted to answer the question is \"the temperature rise on land improperly affected by the four key biases (station quality, homogenization, urban heat island, and station selection)?\" Their conclusion was NO. None of those factors bias the temperature record. The Berkeley conclusions about the urban heat effect were nicely explained by Andy Skuce in an SkS post in 2011. Figure 2 shows that the U.S. network does not show differences between rural and urban sites.\nFigure 2. Comparison of spatially gridded minimum temperatures for U.S. Historical Climatology Network (USHCN) data adjusted for time-of-day (TOB) only, and selected for rural or urban neighborhoods after homogenization to remove biases. (Hausfather et al. 2013)\nTemperatures measured on land are only one part of understanding the climate. We track many indicators of climate change to get the big picture. All indicators point to the same conclusion: the global temperature is increasing.\n------\nSee also\nUnderstanding adjustments to temperature data, Zeke Hausfather\nExplainer: How data adjustments affect global temperature records, Zeke Hausfather\nTime-of-observation Bias, John Hartz\nBerkeley Earth Surface Temperature Study: “The effect of urban heating on the global trends is nearly negligible,” Andy Skuce\nCheck original data\nAll the Berkeley Earth data and analyses are available online at http://berkeleyearth.org/data/.\nPlot your own temperature trends with Kevin's calculator.\nOr plot the differences with rural, urban, or selected regions with another calculator by Kevin.\nNASA GISS Surface Temperature Analysis (GISSTEMP) describes how NASA handles the urban heat effect and links to current data.\nNOAA Global Historical Climate Network (GHCN) Daily. GHCN-Daily contains records from over 100,000 stations in 180 countries and territories.\nLast updated on 15 August 2017 by Sarah. View Archives"
  },
  {
   "title": "The connection between Hurricane Sandy and global warming",
   "paragraph": "The connection between Hurricane Sandy and global warming\nLink to this page\nWhat the science says...\nThere are several ways in which human-caused global warming contributed to the damage caused by Hurricane Sandy - by causing higher sea levels (bigger storm surges and flooding), warmer oceans (a stronger hurricane), and more moisture in the air (more flooding).\nClimate Myth...\nThe connection between Hurricane Sandy and global warming\n\"The magnitude of ignorance on display by people claiming Sandy is a sign of global warming is stunning!\" (Joe Bastardi)\nHurricane Sandy was an unprecedented storm in modern times, arriving late in the hurricane season, making landfall abnormally far to the north on the US east coast, with an exceptionally low pressure, and a record-breaking storm surge. The hurricane also had among the most kinetic energy of all tropical cyclones on record at 222 trillion Joules (the equivalent of 3.5 Little Boy Hiroshima atomic bombs) - more energy than Category 5 hurricanes like Katrina despite Sandy just being Category 1, because Sandy was spread over a much larger area.\nGiven the unprecedented nature of this event, many people are asking whether it was caused by or its impacts amplified by global warming, and many others are of course trying to deny any hurricane-climate links. There is actually a fairly simple answer to this question: human-caused climate change amplified the hurricane's impacts.\nHigher Sea Levels Cause Bigger Storm Surges\nOne reason we can draw this conclusion is that as Michael Mann noted, sea levels around the New York area are now close to 1 foot higher than they were a century ago. For example, Figure 1 shows the annual mean sea level rise since 1900 for Battery Park, New York from tide gauge data.\nFigure 1: Annual mean sea level rise in Battery Park, New York from Permanent Service for Mean Sea Level (PSMSL) tide gauge data.\nWhile Battery Park represents just one tide gauge, there are many other tide gauges in the region which tell a very similar story, as you can see at the PSMSL site. And of course we know that the global sea level rise (approximately 0.6 feet since 1900, on average) is predominantly caused by melting land ice and the thermal expansion of the oceans. As Church et al. (2011) found, approximately 40% of the average global sea level rise since 1972 is due to thermal expansion, and approximately 60% due to land ice melting (Figure 2), both of which in turn are predominantly driven by human-caused global warming.\nFigure 2: The global sea level budget from 1961 to 2008. (a) The observed sea level using coastal and island tide gauges (solid black line with grey shading indicating the estimated uncertainty) and using TOPEX/Poseidon/Jason‐1&2 satellite altimeter data (dashed black line). The two estimates have been matched at the start of the altimeter record in 1993. Also shown are the various contributing components. (b) The observed sea level and the sum of components. The estimated uncertainties are indicated by the shading. The two time series are plotted such that they have the same average over 1972 to 2008. From Church et al. (2011).\nLooking into what we can expect for the impact of future sea level rise on hurricanes, Lin et al. (2012) found that:\n\"The combined effects of storm climatology change and a 1 m [sea level rise] may cause the present NYC 100-yr surge flooding to occur every 3–20 yr and the present 500-yr flooding to occur every 25–240 yr by the end of the century.\"\nSo this human contribution to the Sandy-related damage is quite straightforward. This is what we know:\nHumans increased the greenhouse effect.\nThe greenhouse effect caused the planet to warm.\nThe warming planet caused land ice to melt and the oceans to expand.\nMelting land ice and thermal expansion caused average sea level to rise.\nHigher sea level made the storm surge worse than it would have been in the past, thus causing more flooding.\nWarmer Oceans Fuel Hurricanes\nAs Katharine Hayhoe noted, Atlantic sea surface temperatures (SSTs) are also significantly warmer than they were a century ago as a result of human-caused global warming. Figure 3 shows global surface temperature anomalies for the period 2000 to 2011 compared to 1900 to 1910. SSTs over most of the Atlantic ocean warmed 0.5 to 1.0°C over that timeframe.\nFigure 3: Surface temperature change (°C) from 1900-1910 to 2000-2011, from NASA GISS.\nMIT hurricane expert Kerry Emanuel first proposed in Emanuel (1987) that warmer SSTs should lead to stronger hurricanes. Emanuel (2005) confirmed that hurricanes have grown stronger over the past several decades, in part due to human-caused global warming. As he put it in Emanuel (2012),\n\"In the North Atlantic region, where tropical cyclone records are longer and generally of better quality than elsewhere, power dissipation by tropical cyclones is highly correlated with sea surface temperature during hurricane season in the regions where storms typically develop\"\nAs a result, hurricane strength and damage are projected to increase in a warming world (Figure 4).\nFigure 4: Accumulated damage from 2000 to the year on the x-axis using the the GFDL CM2.0 global climate model with climate held fixed at its 1981–2000 mean condition (blue) and under the global warming scenario associated with IPCC SRES Scenario A1B (red). The error bars shows one standard deviation up and down from the ensemble mean. From Emanuel (2012).\nElsner et al. (2012) confirmed that warmer SSTs feed stronger hurricanes, finding when mean seasonal SSTs are above 25°C\n\"a significant trend trend with increasing SST indicating a sensitivity of 7.9 ± 1.19 m s-1 K-1\"\nElsner et al. and Knutson et al. (2010) also find that hurricanes will become stronger in a warming world.\n\"higher resolution modelling studies typically project substantial increases in the frequency of the most intense cyclones, and increases of the order of 20% in the precipitation rate within 100 km of the storm centre.\"\nAgain we have a very clear connection between human-caused global warming and impacts from Hurricane Sandy.\nHumans increased the greenhouse effect.\nThe greenhouse effect caused the planet (including oceans) to warm.\nWarmer oceans feed stronger hurricanes.\nHowever, note that during the hurricane event, SSTs along the coast were approximately 3°C above average, whereas global warming has increased SSTs by closer to 0.6°C. Thus as Kevin Trenberth notes, while global warming contributed to the hurricane intensity, so did natural variability.\nMore Atmospheric Moisture Causes More Rainfall\nKevin Trenberth notes that due to global warming there is now more moisture in the atmosphere than there was a century ago, which contributed to the flooding in the impacted areas, as Trenberth described:\n\"With every degree F rise in temperatures, the atmosphere can hold 4 percent more moisture. Thus, Sandy was able to pull in more moisture, fueling a stronger storm and magnifying the amount of rainfall by as much as 5 to 10 percent compared with conditions more than 40 years ago. Heavy rainfall and widespread flooding are a consequence.\"\nThis conclusion is consistent with the findings of Trenberth et al. (2005), which found that\n\"recent trends in precipitable water are generally positive and, for 1988 through 2003, average 0.40±0.09 mm per decade or 1.3±0.3% per decade for the ocean as a whole\"\nWe have another fairly simple causal relationship here:\nHumans increased the greenhouse effect.\nThe greenhouse effect caused the planet (including atmosphere) to warm.\nA warmer atmosphere can hold more water vapor.\nThis allows hurricanes to pull more moisture from the atmosphere.\nMore rainfall during the hurricane causes more widespread flooding.\nChanging Weather Patterns Resulting from Arctic Warming\nFrancis and Vavrus (2012) found evidence that that the decline in Arctic sea ice and snow cover are linked to extreme weather, for example through more frequent blocking patterns. Liu et al. (2012) also found that \"the decrease in autumn Arctic sea ice area is linked to changes in the winter Northern Hemisphere atmospheric circulation,\" which results in more frequent episodes of blocking patterns.\nAn atmospheric blocking pattern over Greenland, which may potentially be linked to this year's record low Arctic sea ice extent (though we can't say for certain), helped force the storm to make a left turn into the United States mainland. As Dr. Francis stated in an interview with Justin Gillis,\n\"While it’s impossible to say how this scenario might have unfolded if sea-ice had been as extensive as it was in the 1980s, the situation at hand is completely consistent with what I’d expect to see happen more often as a result of unabated warming and especially the amplification of that warming in the Arctic\"\nExtreme Weather on Steroids\nThe bottom line is that while global warming did not cause Hurricane Sandy, it did contribute to the \"Frankenstorm\" at least by causing higher sea levels (and thus bigger storm surges and flooding), warmer sea surface temperatures (and thus a stronger hurricane), and more moisture in the atmosphere (and thus more rainfall and flooding).\nMore importantly, as Francis noted and as many impacted residents are coming to realize, this type of extreme weather has and will continue to become more commonplace as the planet continues to warm. We know that many types of extreme weather events have already been linked to global warming, including hurricane intensity. A warmer world will \"load the dice\" and make extreme events, including strong hurricanes, more likely to occur. It's important not to lose sight of the long-term trends in arguing about whether or not climate change contributed to any single extreme weather event. As Dave Roberts notes,\n\"There is no division, in the physical world, between “climate change storms” and “non-climate change storms.” Climate change is not an exogenous force acting on the atmosphere. There is only the atmosphere, changing. Everything that happens in a changed atmosphere is “caused” by the atmosphere, even if it’s within the range of historical variability.\"\nAnd as Stephan Lewandowsky put it,\n\"We are living with climate change.\nIt is happening now.\nDebating the extent to which Frankenstorm Sandy was put on steroids by climate change is a distraction.\nNearly all weather events now have a contribution from climate change and it is up to us to manage and reduce that risk with mitigative action.\"\nWe often come back to the words of Lonnie Thompson, who said that climate change will result in some mix of mitigation, adaption, and suffering. So far we have failed to achieve significant mitigation of greenhouse gas emissions, and as a result, extreme weather events on steroids like Hurricane Sandy will cause more suffering than they would otherwise have, and we will have to adapt to a future in which these types of events occur more frequently. Unfortunately, as we saw in North Carolina, some science-denying policymakers are not even willing to implement the necessary adaptation measures. This type of denial will maximize future suffering.\nImagine a world where this type of extreme weather event happens once per decade instead of once per century. That's one small part of what we're talking about when we discuss the impacts of climate change.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 9 July 2015 by pattimer. View Archives"
  },
  {
   "title": "The IPCC consensus is phoney",
   "paragraph": "IPCC reflects scientific consensus on climate change\nLink to this page\nWhat the science says...\nClaims that the IPCC does not accurately represent the views and findings of the scientists, on whose work the IPCC reports are based, are not supported by the facts. Ironically, it's those who are mispresenting Hulme's paper that are the ones being misleading.\nClimate Myth...\nThe IPCC consensus is phoney\n'The UN’s Intergovernmental Panel on Climate Change misled the press and public into believing that thousands of scientists backed its claims on manmade global warming, according to Mike Hulme, a prominent climate scientist and IPCC insider. The actual number of scientists who backed that claim was “only a few dozen experts,” he states in a paper forProgress in Physical Geography, co-authored with student Martin Mahony.' (Lawrence Solomon)\nIt seems ironic that one key version of this argument – that the IPCC ‘misleads’ by misrepresenting the science of climate change and its potential consequences - is itself a gross misrepresentation of a statement made by Professor Mike Hulme, a climate change scientist who works at the University of East Anglia. He was also co-ordinating Lead Author for the chapter on ‘Climate scenario development’ for the IPCC’s AR3 report, as well as a contributing author for several other chapters. This is how Hulme dismissed the claim:\n\"I did not say the ‘IPCC misleads’ anyone – it is claims that are made by other commentators, such as the caricatured claim I offer in the paper, that have the potential to mislead.\"\nThe same argument also has a broader scope, demonstrated by the claim that within the IPCC, there is a politically motivated elite who filter and screen all science to ensure it is consistent with some hidden agenda. This position turns the structure of the IPCC into an argument, by claiming that the small number of lead reviewers dictate what goes into the IPCC reports.\nBefore considering this argument in full, it is prudent to observe that the IPCC does no science or research at all. Its job is purely to collate research findings from thousands of climate scientists (and others working in disciplines that bear on climate science indirectly, such as geology or chemistry). From this, the IPCC produces ‘synthesis reports’ – rather like an executive summary – in which they review and sum up all the available material. It is necessary therefore to have an organisational structure capable of dealing efficiently with so much information, and the hierarchical nature of the IPCC structure is a reflection of this requirement.\nHow does the process work? The IPCC primarily concerns itself with science that has been published in peer-reviewed journals, although, as it makes clear in the IPCC’s published operational appendices, it does also use so called ‘grey’ material where there is insufficient or non-existent peer-reviewed material available at the time the reports are prepared. See IPCC principles, Annex 2: Procedure for using non-published/non-peer-reviewed sources in IPCC reports. Many people are involved in this complex process:\n“More than 450 Lead Authors and more than 800 Contributing Authors (CAs) have contributed to the Fourth Assessment Report (AR4)\".\nSource: The role of the IPCC and key elements of the IPCC assessment process, February 2010\nTo suggest the IPCC can misrepresent the science belies the fact that such misrepresentations would be fiercely criticised by those it misrepresented. Considering how many lead authors and contributors are involved, any egregious misrepresentation would hardly remain unremarked for very long.\nThe Broader Consensus\nAs with all such disputes, it is helpful to consider if there is any evidence of credible independent support for the reports the IPCC has produced, and the conclusions those reports contain. If the accusations were true, such misrepresentation would also be problematic for official bodies, particularly national science academies and the like.\nOn that basis, it is reassuring to note that nearly every major national scientific body e.g. the Royal Society (UK) or the National Academy of Sciences (US), unreservedly supports the work and findings of the IPCC. An expanded list can be found here, including this statement:\n“With the release of the revised statement by the American Association of Petroleum Geologists in 2007, no remaining scientific body of national or international standing is known to reject the basic findings of human influence on recent climate change”.\nIn 2010 an independent investigation of the IPCC was launched. Conducted by the InterAcademy Council, which represents the world’s scientific academies, the report highlighted a number of organisational and procedural areas that the council felt could be improved. However, the recommendations did not detract from the council’s appreciation of the IPCC’s work:\n“The Committee found that the IPCC assessment process has been successful overall. However, the world has changed considerably since the creation of the IPCC, with major advances in climate science, heated controversy on some climate-related issues, and an increased focus of governments on the impacts and potential responses to changing climate”.\nSource: IAC Report Executive Summary\nLike all organisations, the IPCC can improve on its performance. Recent defensiveness regarding errors or ambiguities in the AR4 report may be mitigated in light of unpleasant attacks on the organisation and its director, but the criticisms are valid none the less.\nHowever, claims that the IPCC does not accurately represent the views and findings of the scientists, on whose work the IPCC reports are based, are not supported by the facts.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 29 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "The science isn't settled",
   "paragraph": "Is the science settled?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nThat human CO2 is causing global warming is known with high certainty & confirmed by observations.\nClimate Myth...\nThe science isn't settled\n\"Many people think the science of climate change is settled. It isn't. And the issue is not whether there has been an overall warming during the past century. There has, although it was not uniform and none was observed during the past decade. The geologic record provides us with abundant evidence for such perpetual natural climate variability, from icecaps reaching almost to the equator to none at all, even at the poles.\nThe climate debate is, in reality, about a 1.6 watts per square metre or 0.5 per cent discrepancy in the poorly known planetary energy balance.\" (Jan Veizer)\nSkeptics often claim that the science of anthropogenic global warming (AGW) is not “settled”. But to the extent that this statement is true it is trivial, and to the extent that it is important it is false. No science is ever “settled”; science deals in probabilities, not certainties. When the probability of something approaches 100%, then we can regard the science, colloquially, as “settled”.\nThe skeptics say that results must be double-checked and uncertainties must be narrowed before any action should be taken. This sounds reasonable enough – but by the time scientific results are offered up to policymakers, they have already been checked and double-checked and quintuple-checked.\nScientists have been predicting AGW, with increasing confidence, for decades (indeed, the idea was first proposed in 1896). By the 1970s, the scientific community were becoming concerned that human activity was changing the climate, but were divided on whether this would cause a net warming or cooling. As science learned more about the climate system, a consensus gradually emerged. Many different lines of inquiry all converged on the IPCC’s 2007 conclusion that it is more than 90% certain that anthropogenic greenhouse gases are causing most of the observed global warming.\nSome aspects of the science of AGW are known with near 100% certainty. The greenhouse effect itself is as established a phenomenon as any: it was discovered in the 1820s and the basic physics was essentially understood by the 1950s. There is no reasonable doubt that the global climate is warming. And there is also a clear trail of evidence leading to the conclusion that it’s caused by our greenhouse gas emissions. Some aspects are less certain; for example, the net effect of aerosol pollution is known to be negative, but the exact value needs to be better constrained.\nWhat about the remaining uncertainties? Shouldn’t we wait for 100% certainty before taking action? Outside of logic and mathematics, we do not live in a world of certainties. Science comes to tentative conclusions based on the balance of evidence. The more independent lines of evidence are found to support a scientific theory, the closer it is likely to be to the truth. Just because some details are still not well understood should not cast into doubt our understanding of the big picture: humans are causing global warming.\nIn most aspects of our lives, we think it rational to make decisions based on incomplete information. We will take out insurance when there is even a slight probability that we will need it. Why should our planet’s climate be any different?\nBasic rebuttal written by James Wight\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "The sun is getting hotter",
   "paragraph": "Has the sun been cooling or warming in recent decades?\nLink to this page\nWhat the science says...\nVarious independent measurements of solar activity all confirm the sun has shown a slight cooling trend since 1978.\nClimate Myth...\nThe sun is getting hotter\nThere is no single continuous satellite measurement of Total Solar Irradiance (TSI). Instead, the data is composited from various satellite measurements. The two most cited composites are PMOD and ACRIM. According to Nicola Scafetta, ACRIM more faithfully reproduces the observations whereas PMOD assumes the published TSI satellite data are wrong and need additional corrections. In particular, PMOD alters the data from the Nimbus7/ERB record from 1989 to 1991. Nimbus7/ERB data during such a short period show a clear upward trend while PMOD during the same period is almost constant. The alteration of the Nimbus7/ERB data is responsible for the different shape between the ACRIM and PMOD TSI composites (Shining More Light on the Solar Factor).\nThe ACRIM composite shows a slight increase in TSI - the PMOD composite shows a slight decrease. Regardless of which dataset you use, the trend is so slight, solar variations can at most have contributed only a fraction of the current global warming. Scafetta 2006 uses the ACRIM composite and finds 50% of warming since 1900 is due to solar variations. However, the warming from solar influence occured primarily in the early 20th century when the sun showed significant warming. As for the global warming trend that began around 1975, Scafetta concludes \"since 1975 global warming has occurred much faster than could be reasonably expected from the sun alone.\"\nACRIM vs PMOD\nWhile the argument over ACRIM vs PMOD has minimal bearing on the global warming debate, determining the more accurate TSI reconstruction is a significant piece in the climate puzzle. The major difference between the two composites is the handling of data between 1989 and 1991. There is a 2 year gap between ACRIM-I and ACRIM-II (tragically due to the Challenger space shuttle explosion). To fill the gap, both composites use the HF data but in dramatically different ways.\nFigure 2: PMOD TSI composite (top) versus the ACRIM TSI composite (bottom). Coloured lines give the daily values with the black solid lines giving the 81 day mean.\nPMOD applies corrections to the HF data, which has many sudden jumps due to changes in the orientation of the spacecraft and to switch-offs. Figure 2 demonstrates how the HF corrections are responsible for virtually all of the difference between the long-term drifts of the composites.\nFigure 3: The difference between the ACRIM and PMOD composites. The grey line gives the daily values, the black line the 81 day running mean. The step in the ACRIM gap during 1989 is clearly seen and is about half the amplitude of the solar cycle variation.\nIndependent tests of the PMOD and ACRIM composites\nSo which composite correctly handled the HF data? Does TSI dramatically increase during the HF period as ACRIM supposes and the raw HF data indicates? Or did PMOD get their calibrations right when they adjusted the data to show slight solar cooling over the ACRIM gap? There are a number of independent measurements that can confirm the trend in solar activity over this period.\nLee 1995 compares the ERBS satellite data with the Nimbus HF data and found the HF data drifted significantly over the period of the ACRIM gap while the ERBS data shows a slight cooling.\nKrivova 2003 compares TSI to UV levels. UV levels fluctuate more than TSI - a trend would be more visible. As UV correlates closely with TSI, Krivova concludes PMOD is more accurate and there has been little secular trend in TSI over the past few decades.\nA reconstruction of TSI using sunspot numbers (Krivova 2007) found the minimum of cycle 23 was lower than the minimum of cycle 22, in contrast to the ACRIM composite.\nZurich sunspot counts during the ACRIM gap show a slight downward trend consistent with the PMOD recalibrated data.\nGround based measurements of solar magnetograms (Wenzler 2006) show higher correlation with PMOD than with ACRIM. More on magnetograms...\nScafetta & Willson and the SATIRE model\nIn March 2009, one study claimed the ACRIM composite was independently confirmed by the SATIRE model (Scafetta 2009 ). This is a model of TSI created by Krivova and Solanki. In response, Krivova and Solanki published ACRIM-gap and total solar irradiance revisited: Is there a secular trend between 1986 and 1996? (Krivova 2009).\nThere are several versions of the SATIRE model, each developed from different data and optimised for different time scales. For periods after 1974, they calculate TSI values based on daily measurements of solar magnetograms. For longer periods going back centuries, they used sunspot numbers to reconstruct TSI. When parsing sunspot data, averages over several months must be used. Therefore, the sunspot model is significantly less accurate than the magnetogram model on short time scales.\nScafetta 2009 used the sunspot model in their analysis. By design, the sunspot model is suitable for decadal to centennial scales but significantly less accurate on time scales of months. The more appropriate model is based on daily measurements of solar magnetograms. Therefore, Krivova and Solanki take the next logical step and analyse the TSI results from the magnetogram model over the ACRIM gap. What they found was TSI does not increase over this period. Thus the SATIRE model is independent confirmation that the PMOD composite is the more accurate representation of solar activity.\nTo put things into perspective, the ACRIM vs PMOD debate is essentially arguing over whether the sun is showing a slight upwards trend or a slight downwards trend or if there's even a trend at all. This only underscores the sharp breakdown in correlation between sun and climate since temperatures started rising in the mid 1970's.\nLast updated on 9 August 2010 by John Cook."
  },
  {
   "title": "There is no consensus",
   "paragraph": "The 97% consensus on global warming\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\n97% of climate experts agree humans are causing global warming.\nClimate Myth...\nThere is no consensus\nThe Petition Project features over 31,000 scientists signing the petition stating \"There is no convincing scientific evidence that human release of carbon dioxide will, in the forseeable future, cause catastrophic heating of the Earth's atmosphere ...\". (Petition Project)\nScience achieves a consensus when scientists stop arguing. When a question is first asked – like ‘what would happen if we put a load more CO2 in the atmosphere?’ – there may be many hypotheses about cause and effect. Over a period of time, each idea is tested and retested – the processes of the scientific method – because all scientists know that reputation and kudos go to those who find the right answer (and everyone else becomes an irrelevant footnote in the history of science). Nearly all hypotheses will fall by the wayside during this testing period, because only one is going to answer the question properly, without leaving all kinds of odd dangling bits that don’t quite add up. Bad theories are usually rather untidy.\nBut the testing period must come to an end. Gradually, the focus of investigation narrows down to those avenues that continue to make sense, that still add up, and quite often a good theory will reveal additional answers, or make powerful predictions, that add substance to the theory.\nSo a consensus in science is different from a political one. There is no vote. Scientists just give up arguing because the sheer weight of consistent evidence is too compelling, the tide too strong to swim against any longer. Scientists change their minds on the basis of the evidence, and a consensus emerges over time. Not only do scientists stop arguing, they also start relying on each other's work. All science depends on that which precedes it, and when one scientist builds on the work of another, he acknowledges the work of others through citations. The work that forms the foundation of climate change science is cited with great frequency by many other scientists, demonstrating that the theory is widely accepted - and relied upon.\nIn the scientific field of climate studies – which is informed by many different disciplines – the consensus is demonstrated by the number of scientists who have stopped arguing about what is causing climate change – and that’s nearly all of them.\nAuthors of seven climate consensus studies — including Naomi Oreskes, Peter Doran, William Anderegg, Bart Verheggen, Ed Maibach, J. Stuart Carlton, and John Cook — co-authored a paper that should settle this question once and for all. The two key conclusions from the paper are:\n1) Depending on exactly how you measure the expert consensus, it’s somewhere between 90% and 100% that agree humans are responsible for climate change, with most of our studies finding 97% consensus among publishing climate scientists.\n2) The greater the climate expertise among those surveyed, the higher the consensus on human-caused global warming.\nExpert consensus results on the question of human-caused global warming among the previous studies published by the co-authors of Cook et al. (2016). Illustration: John Cook. Available on the SkS Graphics page\nScientific consensus on human-caused global warming as compared to the expertise of the surveyed sample. There’s a strong correlation between consensus and climate science expertise. Illustration: John Cook. Available on the SkS Graphics page\nExpert consensus is a powerful thing. People know we don’t have the time or capacity to learn about everything, and so we frequently defer to the conclusions of experts. It’s why we visit doctors when we’re ill. The same is true of climate change: most people defer to the expert consensus of climate scientists. Crucially, as we note in our paper:\nPublic perception of the scientific consensus has been found to be a gateway belief, affecting other climate beliefs and attitudes including policy support.\nThat’s why those who oppose taking action to curb climate change have engaged in a misinformation campaign to deny the existence of the expert consensus. They’ve been largely successful, as the public badly underestimate the expert consensus, in what we call the “consensus gap.” Only 16% of Americans realize that the consensus is above 90%.\nLead author John Cook explaining the team’s 2016 consensus paper.\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 8 May 2016 by BaerbelW . View Archives"
  },
  {
   "title": "There's no correlation between CO2 and temperature",
   "paragraph": "Does CO2 always correlate with temperature (and if not, why not?)\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nSurface temperature measurements are affected by short-term climate variability, and recent warming of deep oceans\nClimate Myth...\nThere's no correlation between CO2 and temperature\n\"Twentieth century global warming did not start until 1910. By that time CO2 emissions had already risen from the expanded use of coal that had powered the industrial revolution, and emissions only increased slowly from 3.5gigatonnes in 1910 to under 4gigatonnes by the end of the Second World War.\nIt was the post war industrialization that caused the rapid rise in global CO2 emissions, but by 1945 when this began, the Earth was already in a cooling phase that started around 1942 and continued until 1975. With 32 years of rapidly increasing global temperatures and only a minor increase in global CO2 emissions, followed by 33 years of slowly cooling global temperatures with rapid increases in global CO2 emissions, it was deceitful for the IPCC to make any claim that CO2 emissions were primarily responsible for observed 20th century global warming.\" (Norm Kalmanovitch).\nWhy doesn’t the temperature rise at the same rate that CO2 increases?\nThe amount of CO2 is increasing all the time - we just passed a landmark 400 parts per million concentration of atmospheric CO2, up from around 280ppm before the industrial revolution. That’s a 42.8% increase.\nA tiny amount of CO2 and other greenhouse gases, like methane and water vapour, keep the Earth’s surface 30°Celsius (54°F) warmer than it would be without them. We have added 42% more CO2 but that doesn't mean the temperature will go up by 42% too.\nThere are several reasons why. Doubling the amount of CO2 does not double the greenhouse effect. The way the climate reacts is also complex, and it is difficult to separate the effects of natural changes from man-made ones over short periods of time.\nAs the amount of man-made CO2 goes up, temperatures do not rise at the same rate. In fact, although estimates vary - climate sensitivity is a hot topic in climate science, if you’ll forgive the pun - the last IPCC report (AR4) described the likely range as between 2 and 4.5 degrees C, for double the amount of CO2 compared to pre-industrial levels.\nSo far, the average global temperature has gone up by about 0.8 degrees C (1.4 F).\n\"According to an ongoing temperature analysis conducted by scientists at NASA’s Goddard Institute for Space Studies (GISS)…the average global temperature on Earth has increased by about 0.8°Celsius (1.4°Fahrenheit) since 1880. Two-thirds of the warming has occurred since 1975, at a rate of roughly 0.15-0.20°C per decade.\"\nSource: NASA Earth Observatory\nThe speed of the increase is worth noting too. Unfortunately, as this quote from NASA demonstrates, anthropogenic climate change is happening very quickly compared to changes that occurred in the past (text emboldened for emphasis):\n\"As the Earth moved out of ice ages over the past million years, the global temperature rose a total of 4 to 7 degrees Celsius over about 5,000 years. In the past century alone, the temperature has climbed 0.7 degrees Celsius, roughly ten times faster than the average rate of ice-age-recovery warming.\"\nSource: NASA Earth Observatory\nSmall increases in temperature can be hard to measure over short periods, because they can be masked by natural variation. For example, cycles of warming and cooling in the oceans cause temperature changes, but they are hard to separate from small changes in temperature caused by CO2 emissions which occur at the same time.\nTiny particle emissions from burning coal or wood are also being researched, because they may be having a cooling effect. Scientists like to measure changes over long periods so that the effects of short natural variations can be distinguished from the effects of man-made CO2.\nThe rate of surface warming has slowed in the past decade. Yet the physical properties of CO2 and other greenhouse gases cannot change. The same energy they were re-radiating back to Earth during previous decades must be evident now, subject only to changes in the amount of energy arriving from the sun - and we know that has changed very little. But if that’s true, where is this heat going?\nThe answer is into the deep oceans. Here is a graphic showing where the heat is currently going:\nFrom Nuccitelli et.al (2012)\nThe way heat moves in the deep oceans is not well understood. Improvements in measurement techniques have allowed scientists to more accurately gauge the amount of energy the oceans are absorbing.\nThe Earth’s climate is a complex system, acting in ways we can’t always predict. The energy that man-made CO2 is adding to the climate is not currently showing up as surface warming, because most of the heat is going into the oceans. Currently, the heat is moving downwards from the ocean surface to deeper waters. The surface gets cooler, humidity reduces (water vapour is a powerful greenhouse gas), and air temperatures go down.\nThe rate at which surface temperatures go up is not proportional to the rate of CO2 emissions, but to the total amount of atmospheric CO2 added since the start of the industrial revolution. Only by looking at long-term trends - 30 years is the standard period in climate science - can we measure surface temperature increases accurately, and distinguish them from short-term natural variation.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 17 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "There's no empirical evidence",
   "paragraph": "Empirical evidence that humans are causing global warming\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nLess energy is escaping to space: Carbon dioxide (CO2) acts like a blanket; adding more CO2 makes the 'blanket' thicker, and humans are adding more CO2 all the time.\nClimate Myth...\nThere's no empirical evidence\n\"There is no actual evidence that carbon dioxide emissions are causing global warming. Note that computer models are just concatenations of calculations you could do on a hand-held calculator, so they are theoretical and cannot be part of any evidence.\" (David Evans)\nThe proof that man-made CO2 is causing global warming is like the chain of evidence in a court case. CO2 keeps the Earth warmer than it would be without it. Humans are adding CO2 to the atmosphere, mainly by burning fossil fuels. And there is empirical evidence that the rising temperatures are being caused by the increased CO2.\nThe Earth is wrapped in an invisible blanket\nIt is the Earth’s atmosphere that makes most life possible. To understand this, we can look at the moon. On the surface, the moon’s temperature during daytime can reach 100°C (212°F). At night, it can plunge to minus 173°C, or -279.4°F. In comparison, the coldest temperature on Earth was recorded in Antarctica: −89.2°C (−128.6°F). According to the WMO, the hottest was 56.7°C (134°F), measured on 10 July 1913 at Greenland Ranch (Death Valley).\nMan could not survive in the temperatures on the moon, even if there was air to breathe. Humans, plants and animals can’t tolerate the extremes of temperature on Earth unless they evolve special ways to deal with the heat or the cold. Nearly all life on Earth lives in areas that are more hospitable, where temperatures are far less extreme.\nYet the Earth and the moon are virtually the same distance from the sun, so why do we experience much less heat and cold than the moon? The answer is because of our atmosphere. The moon doesn’t have one, so it is exposed to the full strength of energy coming from the sun. At night, temperatures plunge because there is no atmosphere to keep the heat in, as there is on Earth.\nThe laws of physics tell us that without the atmosphere, the Earth would be approximately 33°C (59.4°F) cooler than it actually is.\nThis would make most of the surface uninhabitable for humans. Agriculture as we know it would be more or less impossible if the average temperature was −18 °C. In other words, it would be freezing cold even at the height of summer.\nThe reason that the Earth is warm enough to sustain life is because of greenhouse gases in the atmosphere. These gases act like a blanket, keeping the Earth warm by preventing some of the sun’s energy being re-radiated into space. The effect is exactly the same as wrapping yourself in a blanket – it reduces heat loss from your body and keeps you warm.\nIf we add more greenhouse gases to the atmosphere, the effect is like wrapping yourself in a thicker blanket: even less heat is lost. So how can we tell what effect CO2 is having on temperatures, and if the increase in atmospheric CO2 is really making the planet warmer?\nOne way of measuring the effect of CO2 is by using satellites to compare how much energy is arriving from the sun, and how much is leaving the Earth. What scientists have seen over the last few decades is a gradual decrease in the amount of energy being re-radiated back into space. In the same period, the amount of energy arriving from the sun has not changed very much at all. This is the first piece of evidence: more energy is remaining in the atmosphere.\nTotal Earth Heat Content from Church et al. (2011)\nWhat can keep the energy in the atmosphere? The answer is greenhouse gases. Science has known about the effect of certain gases for over a century. They ‘capture’ energy, and then emit it in random directions. The primary greenhouse gases – carbon dioxide (CO2), methane (CH4), water vapour, nitrous oxide and ozone – comprise around 1% of the air.\nThis tiny amount has a very powerful effect, keeping the planet 33°C (59.4°F) warmer than it would be without them. (The main components of the atmosphere – nitrogen and oxygen – are not greenhouse gases, because they are virtually unaffected by long-wave, or infrared, radiation). This is the second piece of evidence: a provable mechanism by which energy can be trapped in the atmosphere.\nFor our next piece of evidence, we must look at the amount of CO2 in the air. We know from bubbles of air trapped in ice cores that before the industrial revolution, the amount of CO2 in the air was approximately 280 parts per million (ppm). In June 2013, the NOAA Earth System Research Laboratory in Hawaii announced that, for the first time in thousands of years, the amount of CO2 in the air had gone up to 400ppm. That information gives us the next piece of evidence; CO2 has increased by nearly 43% in the last 150 years.\nAtmospheric CO2 levels (Green is Law Dome ice core, Blue is Mauna Loa, Hawaii) and Cumulative CO2 emissions (CDIAC). While atmospheric CO2 levels are usually expressed in parts per million, here they are displayed as the amount of CO2 residing in the atmosphere in gigatonnes. CO2 emissions includes fossil fuel emissions, cement production and emissions from gas flaring.\nThe Smoking Gun\nThe final piece of evidence is ‘the smoking gun’, the proof that CO2 is causing the increases in temperature. CO2 traps energy at very specific wavelengths, while other greenhouse gases trap different wavelengths. In physics, these wavelengths can be measured using a technique called spectroscopy. Here’s an example:\nSpectrum of the greenhouse radiation measured at the surface. Greenhouse effect from water vapor is filtered out, showing the contributions of other greenhouse gases (Evans 2006).\nThe graph shows different wavelengths of energy, measured at the Earth’s surface. Among the spikes you can see energy being radiated back to Earth by ozone (O3), methane (CH4), and nitrous oxide (N20). But the spike for CO2 on the left dwarfs all the other greenhouse gases, and tells us something very important: most of the energy being trapped in the atmosphere corresponds exactly to the wavelength of energy captured by CO2.\nSumming Up\nLike a detective story, first you need a victim, in this case the planet Earth: more energy is remaining in the atmosphere.\nThen you need a method, and ask how the energy could be made to remain. For that, you need a provable mechanism by which energy can be trapped in the atmosphere, and greenhouse gases provide that mechanism.\nNext, you need a ‘motive’. Why has this happened? Because CO2 has increased by nearly 50% in the last 150 years and the increase is from burning fossil fuels.\nAnd finally, the smoking gun, the evidence that proves ‘whodunit’: energy being trapped in the atmosphere corresponds exactly to the wavelengths of energy captured by CO2.\nThe last point is what places CO2 at the scene of the crime. The investigation by science builds up empirical evidence that proves, step by step, that man-made carbon dioxide is causing the Earth to warm up.\nBasic rebuttal written by GPWayne\nAddendum: the opening paragraph was added on 24th October 2013 in response to a criticism by Graeme, a participant on the Coursera Climate Literacy course. He pointed out that the rebuttal did not make explicit that it was man-made CO2 causing the warming, which the new paragraph makes clear. The statement \"...and humans are adding more CO2 all the time\" was also added to the 'what the science says section.\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\n>\nLast updated on 12 July 2015 by MichaelK. View Archives"
  },
  {
   "title": "There's no tropospheric hot spot",
   "paragraph": "Understanding the significance of the tropospheric hot spot\nLink to this page\nWhat the science says...\nSelect a level... Intermediate Advanced\nSatellite measurements match model results apart from in the tropics. There is uncertainty with the tropic data due to how various teams correct for satellite drift. The U.S. Climate Change Science Program conclude the discrepancy is most likely due to data errors.\nClimate Myth...\nThere's no tropospheric hot spot\nThe IPCC confirms that computer modeling predicts the existence of a tropical, mid-troposphere “hot spot” about 10km above the Earth’s surface. Yet in the observed record of the Hadley Centre’s radiosondes, the predicted “hot-spot” signature of anthropogenic greenhouse warming is entirely absent (source: Christopher Monckton)\nThe tropospheric hot spot is due to changes in the lapse rate (Bengtsson 2009, Trenberth 2006, Ramaswamy 2006). As you get higher into the atmosphere, it gets colder. The rate of cooling is called the lapse rate. When the air cools enough for water vapor to condense, latent heat is released. The more moisture in the air, the more heat is released. As it's more moist in the tropics, the air cools at a slower rate compared to the poles. For example, it cools at around 4°C per kilometre at the equator but a much larger 8 to 9°C per kilometre at the subtropics.\nWhen the surface warms, there's more evaporation and more moisture in the air. This decreases the lapse rate - there's less cooling aloft. This means warming aloft is greater than warming at the surface. This amplified trend is the hot spot. It's all to do with changes in the lapse rate, regardless of what's causing the warming. If the warming was caused by a brightening sun or reduced sulphate pollution, you'd still see a hot spot.\nThere's a figure in the IPCC 4th Assessment report that shows the \"temperature signature\" expected from the various forcings that drive climate. This figure is frequently misinterpreted. Let's have a close look:\nFigure 1: Atmospheric temperature change from 1890 to 1990 from (a) solar forcing, (b) volcanoes, (c) greenhouse gases, (d) ozone, (e) sulfate aerosols and (f) sum of all forcing (IPCC AR4).\nThe source of the confusion is box c, showing the modelled temperature change from greenhouse gases. Note the strong hot spot. Does this mean the greenhouse effect causes the hot spot? Not directly. Greenhouse gases cause surface warming which changes the lapse rate leading to the hot spot. The reason the hot spot in box c is so strong is because greenhouse warming is so strong compared to the other forcings.\nThe hot spot is not a unique greenhouse signature and finding the hot spot doesn't prove that humans are causing global warming. Observing the hot spot would tell us we have a good understanding of how the lapse rate changes. As the hot spot is well observed over short timescales (Trenberth 2006, Santer 2005), this increases our confidence that we're on track. That leaves the question of the long-term trend.\nWhat does the full body of evidence tell us? We have satellite data plus weather balloon measurements of temperature and wind strength. The three satellite records from UAH, RSS and UWA give varied results. UAH show tropospheric trends less than surface warming, RSS are roughly the same and UWA show a hot spot. The difference between the three is how they adjust for effects like decaying satellite orbits. The conclusion from the U.S. Climate Change Science Program (co-authored by UAH's John Christy) is the most likely explanation for the discrepancy between model and satellite observations is measurement uncertainty.\nWeather balloon measurements are influenced by effects like the daytime heating of the balloons. When these effects are adjusted for, the weather balloon data is broadly consistent with models (Titchner 2009, Sherwood 2008, Haimberger 2008). Lastly, there is measurements of wind strength from weather balloons. The direct relationship between temperature and wind shear allows us to empirically obtain a temperature profile of the atmosphere. This method finds a hot spot (Allen 2008).\nLooking at all this evidence, the conclusion is, well, a little unsatisfying - there is still much uncertainty in the long-term trend. It's hard when the short-term variability is nearly an order of magnitude greater than the long-term trend. Weather balloons and satellites do a good job of measuring short-term changes and indeed find a hot spot over monthly timescales. There is some evidence of a hot spot over timeframes of decades but there's still much work to be done in this department. Conversely, the data isn't conclusive enough to unequivocally say there is no hot spot.\nThe take-home message is that you first need to understand what's causing the hot spot. \"Changes in the lapse rate\" is not as sexy or intuitive as a greenhouse signature but that's the physical reality. Once you properly understand the cause, you can put the whole issue in proper context. As the hot spot is due to changes in the lapse rate, we expect to see a short-term hot spot. We do.\nWhat about a long-term hot spot? With short-term observations confirming our understanding of the lapse rate, that leaves spurious long-term biases as the most likely culprit. However, as observations improve, if it turns out the long-term hot spot is not as strong as expected, the main question will be why do we see a short-term hot spot but not a long-term hot spot?\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 13 November 2016 by pattimer. View Archives"
  },
  {
   "title": "They changed the name from 'global warming' to 'climate change'",
   "paragraph": "Global warming vs climate change\nLink to this page\nWhat the science says...\nThere have long been claims that some unspecificed \"they\" has \"changed the name from 'global warming' to 'climate change'\". In reality, the two terms mean different things, have both been used for decades, and the only individual to have specifically advocated changing the name in this fashion is a global warming 'skeptic'.\nClimate Myth...\nThey changed the name from 'global warming' to 'climate change'\nThey changed the name from “global warming” to “climate change” after the term global warming just wasn’t working (it was too cold)! (Donald J. Trump)\nGlobal Warming vs. Climate Change\nBoth of the terms in question are used frequently in the scientific literature, because they refer to two different physical phenomena. As the name suggests, 'global warming' refers to the long-term trend of a rising average global temperature, which you can see here:\n'Climate change', again as the name suggests, refers to the changes in the global climate which result from the increasing average global temperature. For example, changes in precipitation patterns, increased prevalence of droughts, heat waves, and other extreme weather, etc. These projections of future global precipitation changes from the 2007 IPCC report are an example of climate change:\nThus while the physical phenomena are causally related, they are not the same thing. Human greenhouse gas emissions are causing global warming, which in turn is causing climate change. However, because the terms are causally related, they are often used interchangeably in normal daily communications.\nBoth Terms Have Long Been Used\nThe argument \"they changed the name\" suggests that the term 'global warming' was previously the norm, and the widespread use of the term 'climate change' is now. However, this is simply untrue. For example, a seminal climate science work is Gilbert Plass' 1956 study 'The Carbon Dioxide Theory of Climatic Change' (which coincidentally estimated the climate sensitivity to a doubling of atmospheric carbon dioxide at 3.6°C, not far off from today's widely accepted most likely value of 3°C). Barrett and Gast published a letter in Science in 1971 entitled simply 'Climate Change'. The journal 'Climatic Change' was created in 1977 (and is still published today). The IPCC was formed in 1988, and of course the 'CC' is 'climate change', not 'global warming'. There are many, many other examples of the use of the term 'climate change' many decades ago. There is nothing new whatsoever about the usage of the term.\nIn fact, according to Google Books, the usage of both terms in books published in the United States has increased at similar rates over the past 40 years:\nAnd a Google Scholar search reveals that the term 'climate change' was in use before the term 'global warming', and has always been the more commonly-used term in scientific literature:\nNo Reason to Change the Term\nThose who perpetuate the \"they changed the name\" myth generally suggest two reasons for the supposed terminology change. Either because (i) the planet supposedly stopped warming, and thus the term 'global warming' is no longer accurate, or (ii) the term 'climate change' is more frightening.\nThe first premise is demonstrably wrong, as the first figure above shows the planet is still warming, and is still accumulating heat. Quite simply, global warming has not stopped.\nThe second premise is also wrong, as demonstrated by perhaps the only individual to actually advocate changing the term from 'global warming' to 'climate change', Republican political strategist Frank Luntz in a controversial memo advising conservative politicians on communicating about the environment:\nIt’s time for us to start talking about “climate change” instead of global warming and “conservation” instead of preservation.\n“Climate change” is less frightening than “global warming”. As one focus group participant noted, climate change “sounds like you’re going from Pittsburgh to Fort Lauderdale.” While global warming has catastrophic connotations attached to it, climate change suggests a more controllable and less emotional challenge.\nSummary\nSo to sum up, although the terms are used interchangeably because they are causally related, 'global warming' and 'climate change' refer to different physical phenomena. The term 'climate change' has been used frequently in the scientific literature for many decades, and the usage of both terms has increased over the past 40 years. Moreover, since the planet continues to warm, there is no reason to change the terminology. Perhaps the only individual to advocate the change was Frank Luntz, a Republican political strategist and global warming skeptic, who used focus group results to determine that the term 'climate change' is less frightening to the general public than 'global warming'. There is simply no factual basis whatsoever to the myth \"they changed the name from global warming to climate change\".\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nAdditional video from the MOOC\nExpert interview with Richard Alley\nLast updated on 8 January 2017 by pattimer. View Archives"
  },
  {
   "title": "Tree-rings diverge from temperature after 1960",
   "paragraph": "Tree-ring proxies and the divergence problem\nLink to this page\nWhat the science says...\nThe divergence problem is a physical phenomenon - tree growth has slowed or declined in the last few decades, mostly in high northern latitudes. The divergence problem is unprecedented, unique to the last few decades, indicating its cause may be anthropogenic. The cause is likely to be a combination of local and global factors such as warming-induced drought and global dimming. Tree-ring proxy reconstructions are reliable before 1960, tracking closely with the instrumental record and other independent proxies.\nClimate Myth...\nTree-rings diverge from temperature after 1960\nActual reconstructions \"diverge\" from the instrumental series in the last part of 20th century. For instance, in the original hockey stick (ending 1980) the last 30-40 years of data points slightly downwards. In order to smooth those time series one needs to \"pad\" the series beyond the end time, and no matter what method one uses, this leads to a smoothed graph pointing downwards in the end whereas the smoothed instrumental series is pointing upwards — a divergence (Climate Audit).\nTree growth is sensitive to temperature. Consequently, tree-ring width and tree-ring density, both indicators of tree growth, serve as useful proxies for temperature. By measuring tree growth in ancient trees, scientists can reconstruct temperature records going back over 1000 years. Comparisons with direct temperature measurements back to 1880 show a high correlation with tree growth. However, in high latitude sites, the correlation breaks down after 1960. At this point, while temperatures rise, tree-ring width shows a falling trend. This divergence between temperature and tree growth is called, imaginatively, the divergence problem.\nThe divergence problem has been discussed in the peer reviewed literature since the mid 1990s when it was noticed that Alaskan trees were showing a weakened temperature signal in recent decades (Jacoby 1995). This work was broadened in 1998 using a network of over 300 tree-ring records across high northern latitudes (Briffa 1998). From 1880 to 1960, there is a high correlation between the instrumental record and tree growth. Over this period, tree-rings are an accurate proxy for climate. However, the correlation drops sharply after 1960. At high latitudes, there has been a major, wide-scale change in tree-growth over the past few decades.\nFigure 1: Twenty-year smoothed plots of tree-ring width (dashed line) and tree-ring density (thick solid line), averaged across a network of mid-northern latitude boreal forest sites and compared with equivalent-area averages of mean April to September temperature anomalies (thin solid line). (Briffa 1998)\nHas this phenomenon happened before? In other words, can we rely on tree-ring growth as a proxy for temperature? Briffa 1998 shows that tree-ring width and density show close agreement with temperature back to 1880. To examine earlier periods, one study split a network of tree sites into northern and southern groups (Cook 2004). While the northern group showed significant divergence after the 1960s, the southern group was consistent with recent warming trends. This has been a general trend with the divergence problem - trees from high northern latitudes show divergence while low latitude trees show little to no divergence. The important result from Cook 2004 was that before the 1960s, the groups tracked each other reasonably well back to the Medieval Warm Period. Thus, the study suggests that the current divergence problem is unique over the past thousand years and is restricted to recent decades.\nThis suggests the decline in tree growth may have an anthropogenic cause. A thorough review of the many peer reviewed studies investigating possible contributing factors can be found in On the ’divergence problem’ in northern forests: A review of the tree-ring evidence and possible causes (D’Arrigo 2008). Some of the findings:\nVarious studies have noted the drop in Alaskan tree-growth coincides with warming-induced drought. By combining temperature and rainfall records, growth declines were found to be more common in the warmer, drier locations.\nStudies in Japan and Bavaria suggest increasing sulfur dioxide emissions were responsible.\nAs the divergence is widespread across high northern latitudes, Briffa 1998 suggests there may be a large scale explanation, possibly related to air pollution effects. A later study by Briffa proposed that falling stratospheric ozone concentration is a possible cause of the divergence, since this observed ozone decline has been linked to an increased incidence of ultraviolet (UV-B) radiation at the ground (Briffa 2004).\nConnected to this is global dimming (a drop in solar radiation reaching the ground). The average amount of sunlight reaching the ground has declined by around 4 to 6% from 1961 to 1990.\nOne study suggests that microsite factors are an influence on whether individual trees are vulnerable to drought stress. Eg - the slope where the tree is located, the depth to permafrost and other localised factors (Wilmking 2008). This paper amusingly refers to the divergence problem as the \"divergence effect\" so as \"to not convey any judgement by the wording\" (you wouldn't want to offend those overly sensitive Alaskan trees).\nThere is evidence for both local and regional causes (e.g. drought stress) as well as global scale causes (e.g. global dimming). It's unlikely there's a single smoking gun to explain the divergence problem. More likely, it's a complex combination of various contributing factors, often unique to different regions and even individual trees.\nOne erroneous characterization is that scientists have been hiding the divergence problem. In fact, tree-ring divergence has been openly discussed in the peer-reviewed literarure since 1995. A perusal of the many peer reviewed papers (conveniently summarised in D’Arrigo 2008) reveal the following:\nThe divergence problem is a physical phenomenon - tree growth has slowed or declined in the last few decades, mostly in high northern latitudes.\nThe divergence problem is unprecedented, unique to the last few decades, indicating its cause may be anthropogenic.\nThe cause is likely to be a combination of local and global factors such as warming-induced drought and global dimming.\nTree-ring proxy reconstructions are reliable before 1960, tracking closely with the instrumental record and other independent proxies.\nIntermediate rebuttal written by John Cook\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nAdditional videos from the MOOC\nInterviews with various experts\nExpert interview with Tim Osborne\nLast updated on 29 October 2016 by pattimer. View Archives"
  },
  {
   "title": "Trenberth can't account for the lack of warming",
   "paragraph": "Trenberth talks about energy flows and global warming\nLink to this page\nWhat the science says...\nSelect a level... Intermediate Advanced\nTrenberth's views are clarified in the paper \"An imperative for climate change planning: tracking Earth's global energy\". We know the planet is continually heating due to increasing carbon dioxide but that surface temperature sometimes have short term cooling periods. This is due to internal variability and Trenberth was lamenting that our observation systems can't comprehensively track all the energy flow through the climate system.\nClimate Myth...\nTrenberth can't account for the lack of warming\nin one e-mail, a top \"warmist\" researcher admits it’s a \"travesty\" that \"we can’t account for the lack of warming at the moment.\" As it happens, the writer of that October 2009 e-mail—Kevin Trenberth, a lead author of the warmist bible, the 2007 Intergovernmental Panel on Climate Change (IPCC) report—told Congress two years ago that evidence for manmade warming is \"unequivocal.\" He claimed \"the planet is running a ’fever’ and the prognosis is that it is apt to get much worse.\" But Trenberth’s \"lack of warming at the moment\" has been going on at least a decade. (Michael Fumento)\nThis has been most commonly interpreted (among skeptics) as climate scientists secretly admitting amongst themselves that global warming really has stopped. Is this what Trenberth is saying? If one takes a little time to understand the science that Trenberth is discussing, his meaning becomes clear.\nIf you read the full email, you learn that Trenberth is actually informing fellow climate scientists about a paper he'd recently published, An imperative for climate change planning: tracking Earth's global energy (Trenberth 2009). The paper discusses the planet's energy budget - how much net energy is flowing into our climate and where it's going. It also discusses the systems we have in place to track energy flow in and out of our climate system.\nTrenberth states unequivocally that our planet is continually heating due to increasing carbon dioxide. This energy imbalance was very small 40 years ago but has steadily increased to around 0.9 W/m2 over the 2000 to 2005 period, as observed by satellites. Preliminary satellite data indicates the energy imbalance has continued to increase from 2006 to 2008. The net result is that the planet is continuously accumulating heat. Global warming is still happening.\nNext, Trenberth wonders with this ever increasing heat, why doesn't surface temperature continuously rise? The standard answer is \"natural variability\". But such a general answer doesn't explain the actual physical processes involved. If the planet is accumulating heat, the energy must go somewhere. Is it going into melting ice? Is it being sequestered deep in the ocean? Did the 2008 La Niña rearrange the configuration of ocean heat? Is it all of the above? Trenberth wants answers!\nSo like an obsessive accountant, Trenberth pores over the energy budget, tallying up the joules accumulating in various parts of the climate. A global energy imbalance of 0.9 W/m2 means the planet is accumulating 145 x 1020 joules per year. The following list gives the amount of energy going into various parts of the climate over the 2004 to 2008 period:\nLand: 2 x 1020 joules per year\nArctic sea Ice: 1 x 1020 joules per year\nIce sheets: 1.4 x 1020 joules per year\nTotal land ice: between 2 to 3 x 1020 joules per year\nOcean: between 20 to 95 x 1020 joules per year\nSun: 16 x 1020 joules per year (eg - the sun has been cooling from 2004 to 2008)\nThese various contributions total between 45 to 115 x 1020 joules per year. This falls well short of the total 145 x 1020 joules per year (although the error bars do overlap). Trenberth expresses frustration that observation systems are inadequate to track the flow of energy. It's not that global warming has stopped. We know global warming has continued because satellites find an energy imbalance. It's that our observation systems need to be more accurate in tracking the energy flows through our climate and closing the energy budget.\nSo what may be causing the discrepancy? As the ocean heat data only goes to 900 metre depth, Trenberth suggests that perhaps heat is being sequestered below 900 metres. There is support for this idea in a later paper von Schuckmann 2009. This paper uses Argo buoy data to calculate ocean heat down to 2000 metres depth. From 2003 to 2008, the world's oceans have been accumulating heat at a rate of 0.77 W/m2. This higher trend for ocean heat would bring the total energy build-up more in line with satellite measurements of net energy imbalance.\nA subsequent study by Balmaseda, Trenberth, and Källén (2013) determined that over the past decade, approximately 30% of ocean warming has occurred in the deeper layers, below 700 meters. This conclusion goes a long way to resolving the 'missing heat' discrepancy. There is still some discrepancy remaining, which could be due to errors in the satellite measurements, the ocean heat content measurements, or both. But the discrepancy is now significantly smaller, and will be addressed in further detail in a follow-up paper by these scientists.\nSummary\nSo to summarise, Trenberth's email says this:\n\"The fact is that we can't account for the lack of warming at the moment and it is a travesty that we can't.\"\nAfter reviewing the discussion in Trenberth 2009, it's apparent that what he meant was this:\n\"Global warming is still happening - our planet is still accumulating heat. But our observation systems aren't able to comprehensively keep track of where all the energy is going. Consequently, we can't definitively explain why surface temperatures have gone down in the last few years. That's a travesty!\"\nSkeptics use Trenberth's email to characterise climate scientists as secretive and deceptive. However, when one takes the trouble to acquaint oneself with the science, the opposite becomes apparent. Trenberth outlines his views in a clear, open manner, frankly articulating his frustrations at the limitations of observation systems. Trenberth's opinions didn't need to be illegally stolen and leaked onto the internet. They were already publicly available in the peer reviewed literature - and much less open to misinterpretation than a quote-mined email.\nIntermediate rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 14 July 2015 by pattimer. View Archives"
  },
  {
   "title": "Tropical thermostat limits sea surface temperature to 30°C",
   "paragraph": "Tropical thermostat and sea surface temperatures\nLink to this page\nWhat the science says...\nIf the troposphere warms, then the sea surface temperature threshold to kick in convection (and related cloud feedbacks) must also increase. In other words, it is the threshold for deep convection that promotes the skewed plot seen in the first figure, not any universal demand for a maximum ocean temperature.\nClimate Myth...\nTropical thermostat limits sea surface temperature to 30°C\n\"It has been known for some time that the “Pacific Warm Pool”, the area just northeast of Australia, has a maximum temperature. It never gets much warmer than around 30 – 31°C. This has been borne out by the Argo floats.\"\n\"...this thermostatic mechanism...is regulated by temperature, and not by forcing. It is insensitive to excess incoming radiation, whether from CO2 or from the sun. During the part of the year when the incoming radiation would be enough to increase the temperature over ~ 30°, the temperature simply stops rising at 30°. It is no longer a function of the forcing.\" (Willis Eschenbach)\nA post at Watts Up With That (WUWT) by Willis Eschenbach, embraced by Roger Pielke Sr., goes into detail concerning a purported \"tropical thermostat\" that sets an upper limit on the ocean temperature. This thermostat could presumably help regulate the response to radiative forcing in a higher CO2 world, constraining ocean temperatures to be no greater than the threshold value.\nThe underlying hypothesis is actually not of WUWT-origin and has some roots that were discussed in the literature dating back a couple decades. WUWT presents a histogram of observed ocean temperatures, which shows a sharp cut off at ~31 °C (diagram reproduced below). The figure indicates that no measurements in the worlds oceans show temperatures much higher than that value. Based on this data, WUWT (as well as several older scientific papers based on similar data) suggested this is a theoretical 'maximum ocean temperature' and is independent of solar or greenhouse forcing. Kleypas et al 2008, for example, based a study on corals on the premise that such data support a maximum ocean temperature.\nFigure 1. A “histogram” shows how many data points fall in each of the 1°C intervals shown along the bottom axis. The maximum is in the interval 28°-29°C. Figure and Caption reproduced from WUWT article.\nIn another example, in a paper on mass extinctions, Veron, 2008 mentioned that:\n\"...the surface temperature of the largest oceans would have been limited by the Thermal Cap of ~31C, widely believed to be the highest temperature large oceans can reach.’\"\nBased on the premise of a maximum ocean temperature, there have been several proposed 'thermostat' mechanisms to explain why tropical sea surface temperatures (SSTs) don't get much greater than ~31°C. Proposals involve negative feedback cloud responses (ex. Ramanathan and Collins 1991) or enhanced evaporation that keeps the SSTs down (ex. Newell, 1979 or Hartmann and Michelsen, 1993).\nBut is this actually \"widely believed\" as Veron, 2008 state? It turns out the answer is no.\nSeveral older papers rebutted the cloud thermostat hypothesis of Ramanathan and Collins (ex. Fu et al., 1992 and later observational papers) and related thermostat arguments have also been refuted a number of times (Wallace, 1992). Pierrehumbert, 1995 discussed the regulation of tropical SSTs and showed that there is no physical basis for an upper temperature bound. More recent papers (Sud et al., 2008; Williams et al., 2009) came to similar conclusions.\nBefore I explain the discrepancies, it is worth reviewing some basic tropical meteorology:\nThe Tropics, loosely ~30 N-30 S latitude (though a number of definitions exist) receive the substantial bulk of Earth's incoming solar radiation, and in fact receive more incoming energy than outgoing infrared radiation to space. This implies that there is a substantial loss of energy by the tropics toward the poles by non-radiative means (atmospheric and ocean transport).\nThe tropics are dynamically distinct than the mid-latitude regions that (I assume) many of us are more familiar with. Instead of heat transport being manifest in 'eddies' (cyclones and anticyclones, and associated warm/cold fronts), the tropics instead fall under a giant overturning circulation called the Hadley cell.\nDue to the weak Coriolis effect, the tropics have very weak horizontal temperature gradients in the atmosphere. Thus, SSTs vary more than atmospheric temperatures in the horizontal. Furthermore, in the tropics the air very close to the surface always has a similar temperature to the SST. In the vertical (from the surface upward) we typically think of the temperature structure in the tropics as being very close to moist adiabatic, especially over the ocean.\nAs mentioned before, several papers find no evidence of a 'maximum SST', so how do we reconcile that with the observed data?\nWhat about the observed Histogram of SSTs?\nAnswering this question essentially boils down to the question of what SST is required for the onset of deep convection (and thus deep cloud formation). In the modern climate, this value occurs around 28°C. In general, it depends on when air near the surface can become buoyant relative to air in the upper atmosphere and thus have enough energy to rise freely. Because atmospheric temperature gradients in the tropics are small, the threshold temperature for convection depends primarily on the local SST. There is a consequence to this: If the troposphere warms, then the SST threshold to kick in convection (and related cloud feedbacks) must also increase. In other words, it is the threshold for deep convection that promotes the skewed plot seen in the first figure, not any universal demand for a maximum ocean temperature.\nThis concept is shown below. The figure shows a model result for the longwave and shortwave fluxes as clouds form. The threshold for deep convection is readily seen in figure 2a (the longwave flux) and occurs when SSTs exceed 25°C or so. When convection starts, lots of clouds form and the absorbed longwave radiation spikes upwards due to the cloud greenhouse effect. As the climate warms (going from the blue to red line), the SST temperature threshold also rises (i.e., moves to the right).\nFigure 2. a) TOA cloud LW flux as a function of SST, b) TOA cloud SW flux as a function of SST; Solid blue and dashed red lines correspond to the ensemble median over years 0–20 and 60–80, respectively, from 15 IPCC AR4 coupled ocean-atmosphere models for the 1% per year scenario. Vertical lines indicate the interquartile range. From Willaims et al (2009)\nFaulty Mechanisms\nIn order to understand why neither clouds nor evaporation act as an inherent buffer in the modern tropical climate, it's worth considering a few more bits of physics:\nIt must be kept in mind that in the modern tropical climate, the cloud shortwave (albedo) and cloud longwave (greenhouse) effects nearly cancel each other at the top of the atmosphere - the net effect is close to zero. That doesn't mean clouds are unimportant. If you could remove clouds altogether from the tropics, you could introduce many subtle impacts on the atmospheric heating distribution, circulation, and differences in SST across the tropics. However, the threshold temperature for deep cloud formation increases in a new climate. And because of the cancellation between the albedo/greenhouse effects of clouds, it is not compelling they have some special place is controlling the absolute tropical SST.\nIn a new climate it is possible that the albedo effect of clouds could win out over the greenhouse effect. This is a seperate argument that cuts into the heart of the climate sensitivity issue. Most studies to date show that the longwave feedback effect is likely to be positive (see ex. Zelinka and Hartmann, 2010; this will also be highly discussed in the IPCC Fifth assessment report, which reserves a whole chapter for cloud and aerosol issues of this sort). Most of the uncertainly in cloud feedback enters into the albedo side of the equation; how the cancellation between the albedo and greenhouse effects of clouds may change in a new climate is a challenging one, but there is no convincing argument as to why this should serve as a strong negative feedback, let alone provide a \"thermal cap\" on the oceans independent of any large forcing.\nThe tropics are partly stabilized by heat transport towards the poles and also by dry regions where infrared radiation more easily escapes to space. Near the equator the large moisture content acts as an infrared 'insulator', but dry regions have a weaker greenhouse effect. Transport of heat into these dry regions lets them act like \"radiator fins\" (Pierrehumbert, 1995) where energy can more readily leak out into space. If it weren't for this heat escape poleward and out of the dry regions then the bulk of the tropics would, in isolation, collapse into a runaway greenhouse state. This, however, doesn't mean that tropical SSTs cannot increase at all beyond ~31 °C.\nEvaporation does not regulate the absolute value of SST, an argument taken up by Pierrehumbert (1995, cited previously) and shown by a number of papers, one recent being Miller, 2011. Instead, evaporation is such a large factor in the tropics that it wipes out the differences between the SST and the overlying air temperature. However, changes in top-of-atmosphere forcing actually impact SST more than changes in the surface forcing, since the whole atmospheric column will regulate its outgoing longwave radiation in response to perturbations. Most longwave radiation escapes in the high atmosphere, and the troposphere is well-mixed by convection such that it warms and cools as unit in order to balance ingoing and outgoing energy. Evaporation, however, is a buffer that takes up the slack between radiation absorbed at the surface and the flux of energy required to keep the SST close to air temperature.\nThe Observed and Past Climate Record\nIn order to cross-validate whether these results are correct, it would be useful to consult the paleoclimate record, in order to establish whether tropical SSTs were warmer than the modern alleged 'threshold' value. There is abundant palaeoclimate evidence that tropical sea temperatures can rise well above present values. Improved understanding of oxygen proxies and the development of new proxies ('thermometers of the past') such as TEX86 and Mg/Ca have shown that Eocene SST's in the tropics could have been even hotter than 35 °C (see ex, Huber, 2008).\nFor modern observations, Johnson and Xie, 2010 find an upward trend in the threshold of deep convection associatd with rising temperatures. This also means that the threshold SST for when hurricanes form will change in a new climate.\nConclusion\nThere have been a number of \"false thermostats\" and incorrect assumptions about a universal and unchanging \"convective threshold\" that kicks in heavy cloud formation. There's been a long history of refuting thermostats of this sort, but apparently this isn't universally appreciated. Note I have said little of the cloud feedback issue relevant for climate sensitivity; rather, there is no compelling physical jusitification to suggest that the tropical sea surface temperatures must be pegged at some maximum value independent of the forcing, or that clouds/evaporation must act as some sort of tropical regulation mechanism. As a further example, the figure below shows a simulation by Sud et al (2008) showing 10N-10S latitude tropical SSTs for present day heating versus the extra heating that would happen if CO2 were doubled:\nFigure 3. Solid lines show the 10 N- 10 S SST for a present-day (green) and 2xCO2.\nLast updated on 19 February 2012 by Chris Colose. View Archives"
  },
  {
   "title": "Tuvalu sea level isn't rising",
   "paragraph": "What's happening to Tuvalu sea level?\nLink to this page\nWhat the science says...\nBetween 1950-2009 sea level at Tuvalu rose at the rate of 5.1 (±0.7) mm per year. This is almost 3 times larger than average global sea level rise over the same period.\nClimate Myth...\nTuvalu sea level isn't rising\nBecause the coral atoll of Funafati, Tuvalu is densely populated and generally less than 3 metres above sea level, this small island nation in the Pacific is often the subject of intense media speculation about the impact of rising seas. The atoll is likely to begin to be overtopped by the sea sometime between mid to late 21st century, however Tuvaluans have often featured in the mainstream media claiming to be already experiencing the detrimental effects of sea level rise. Scientific studies to support these claims has been have been hard to find, but now a recently published study vindicates what many Tuvaluans have insisted all along - sea level has risen rapidly around Tuvalu.\nBecker (2011) has examined sea level rise in the western tropical Pacific Ocean using a combination of tide gauges, satellite-based measurements, ocean modelling and GPS, and found that the region is experiencing sea level rise much larger than the global average. At Funafati Island, the study authors found that between 1950-2009 'total' sea level, which also accounts for the rate of island subsidence or sinking, rose at 5.1 (±0.7) mm per year, almost 3 times larger than the global average over the same period.\nFigure 1- Sea level curves at tide gauge sites since 1950. Time series of reconstructed sea level (black), tide-gauge (red) and altimetry satellite in blue continue and in dash line (when tide gauge records were supplemented using altimetry data). From Becker (2011). Image cropped from original.\nSea level rise is not level\nOf the many things about global warming misunderstood by the public at large, the irregular or lumpy distribution of sea level rise must surely be near the top of the list. When sea level rise is mentioned, this typically refers to the global average or mean, but this obscures the fact that not all areas of the ocean are rising. In a few regions sea level is actually falling, while at others it is rising at a rate much larger than the global average. So even though the total volume of seawater from melting land ice, and thermal expansion from ocean warming are increasing, this isn't being evenly spread around the oceans. See figure 2 below.\nFigure 2 -Location map of the 91 tide gauges (stars) used in the global sea level reconstruction. The background map shows the sea level trends over 1950–2009 from DRAKKAR-based (an ocean model) reconstruction of sea level (uniform trend of 1.8 mm/yr included). From Becker (2011).\nIt just so happens that the western Pacific and Tuvalu in particular, are one such region where there is a large rise in sea level, much greater than the global average. See figure 3.\nFigure 3 - Map of the Pacific Island region interannual sea level trend (linear variation with time) from the reconstruction 1950-2009. Locations of the 27 tide gauges (black circles and stars) used in the study are superimposed. Stars relate to the 7 tide gauges used in the global reconstruction. Dark areas relate to non-significant trends. From Becker (2011).\nMapping sea level rise in the western Pacific\nMeasuring ongoing sea global level rise, which results from the global warming-induced melting of land ice and thermal expansion, is a complicated business. This comparitively small long-term rise is often obscured because sea level can undergo large up-and-down fluctuations over seasonal, annual, and even decade-long time scales. This is especially true in the case of the islands of the western Pacific where the ENSO cycles El Niño and La Niña result in sea level fluctuations up to 20-30 cms, which is around 40-60 times larger than the long-term annual increase (5.1mm) found at Tuvalu. Quite obviously these fluctuations have to accounted for in order to see the underlying long-term trend.\nOvercoming problematic data\nThe tide gauge and satellite altimetry data both have their own shortcomings, for example the tide gauges have long records, but some are plagued by data gaps, and monitoring equipment that has been updated over time. Also, tide gauges are fixed to the seafloor, which can either be sinking or rising dependant on the location (hence the use of co-located GPS equipment at some sites), so that has to be factored in to observations too.\nSatellite altimetry is a vast improvement over the tide gauge network in that it covers the entire oceans, and not just the coastal regions. It can therefore provide a more detailed picture of sea level variations from region to region. See figure 4.\nFigure 4 - Satellite altimtery-based sea level trend patterns in the tropical western Pacific over 1993-2009, on which are superimposed the 27 tide gauges used in the study. Stars correspond to the 7 tide gauges used in the global reconstruction. From Becker (2011)\nBut the downside of satellite observations is they have only been in operation for a short time. They do show a dramatic rise in sea level, in some areas in excess of 10mm per year (Honiara & Yap in figure 4), but the record only began in 1993, and 17 years (to 2009) is hardly long enough to tease out any decades-long natural variability that might exist. In other words, the rapid rate shown in the satellite data may not be indicative of the long-term rate of sea level rise in the region.\nGlobal and regional sea level reconstruction\nBecker (2011) set about reconstructing sea level over the period (1950-2009) by analysing the tide gauge, satellite altimetry, GPS data and use of an Ocean General Circulation Model. Combining a selection of 91 good quality tide gauge records (from 1950-2009) with gridded fields in the ocean circulation model they were able to unravel how sea level evolved in both time and space. To test how well their reconstruction matched observed sea level at a global scale, the authors removed a single tide gauge record from the reconstruction and compared the remaining reconstruction with that at the tide gauge site they removed. This exercise was repeated for each individual tide gauge. Finding good agreement, the authors then compared their reconstruction with the satellite altimetry over the period 1993-2009 in the western Pacific. See figure 5.\nFigure 5 - Reconstructed sea level trend patterns in the tropical western Pacific from 1993-2009, on which are superimposed the 27 tide gauges used in the study. Stars correspond to the 7 tide gauges used in the global reconstruction. From Becker (2011).\nAs seen in figures 4 & 5, the reconstruction method does a good job of describing the actual change of sea level in the western Pacific, although the total amount of sea level rise in the reconstruction is smaller (at maximum & minimum) due to the statistical filtering method employed. See figure 1 for a comparison of the global reconstruction against the tide gauges for the western Pacific not used in the global reconstruction, and against the satellite altimetry.\nTuvalu sea level rises with La Niña and falls with El Niño\nAs seen in the \"Sea level fell in 2010\" rebuttal, short-term global sea level can fluctuate due to the temporary exchange of water and snow between the land and sea, rising during El Niño when the land surface dries out, and falling with La Niña when the land surface recieves extra doses of rain and snow. In the western tropical Pacific, especially around Tuvalu, this trend is exactly the opposite, sea level there falls during El Niño and rises during La Niña. So what's going on?\nThis is largely due to the tilting of the thermocline in the tropical Pacific. See this animation for a great illustration of what this means. During La Niña strengthening of the easterly trade winds straddling the equator pushes warm water toward the western tropical Pacific. This warm water piles up in the western Pacific, which drives warmth down deep (the tilt of the thermocline), but it also causes the ocean to bulge upwards there due to the extra wind-driven water mass (see Timmerman [2010], Merrifield [2011] & Qiu & Chen [2011]). Both have the effect of raising sea levels, with the 'twin peaks' of sea level rise very obvious just north and south of the equator in figures 4 & 5 (the equator would be a horizontal line through Nauru).\nDuring El Niño the trade winds weaken and the warm wind-driven water mass is no longer pushed toward Tuvalu and neighbouring islands. Instead the thermocline tilts back onto a more level plane, warm water begins to accumulate in the central Pacific, and accordingly sea level drops around Tuvalu.\nThe huge drop in sea level around Tuvalu, and nearby islands, shown in figure 1 around 1982-1983 is due to the strong El Niño during that time. Which goes to show not only how variable and sensitive to ENSO sea level in the western tropical Pacific is, but also how much variation there is even within the western Pacific itself. This ENSO/sea level relationship is shown in figure 6 below, however note that this graph accounts for the tropical western Pacific, not just Tuvalu.\nFigure 6 - Reconstructed sea level (RESL=black line) over the tropical western Pacific between 1950-2009 after the 1.8 mm per year trend has been removed (detrended). Gray areas represent RESL uncertainty. Red line=detrended steric (thermal expansion + salinity change) over the same period from ocean heat content data. Blue line= the NINO3 index, which is a measure of the sign (El Niño or La Niña) and strength of ENSO. See global map here for NINO3 region. From Becker (2011)\nSteric sea level is variation which results from changes in ocean heat content (thermal expansion), and changes in salinity (saltiness) which affects the density of seawater. Once the long-term trend has been removed (de-trended) from the reconstructed sea level and steric sea level, the relationship between sea level, the steric component and ENSO is plain to see in figure 6.\nGPS - accounting for vertical land movement\nIn many parts of the world the land surface is either very slowly rising or sinking, and this needs to be factored into reconstructions because, unlike the satellites measuring sea level, the tide gauges are firmly anchored to the sea floor. Becker (2011) sift through the GPS equipped western tropical Pacific tide gauge sites, ending up with 7 sites that are of sufficient quality for analysis. They make the assumption that the rate of subsidence (sinking) observed over the short time of GPS use, holds true back to 1950, and find that subsidence adds an additional 10% to the total sea level rise experienced at Tuvalu.\nSo to sum up:\nSea level rise is not level. Although the oceans are gaining water mass from melting land ice, and the oceans are expanding due to increasing heat content, this water mass and heat is not spread evenly over the oceans.\nIn the tropical western Pacific the dominant short-term influence on sea level is the extremely large fluctuation due to the ENSO events El Niño (falling sea level at Tuvalu) and La Niña (rising sea level at Tuvalu).\nSea level at Tuvalu can vary by 20-30cms from the influence of ENSO. This is 40-60 times larger than the annual rate of sea level rise at Tuvalu.\nThis regional susceptibility to ENSO is because of the weakening (El Niño) or strengthening (La Niña) trade winds near the equator, which push warm water mass toward the tropical western Pacific.\nBecker (2011) uses a combination of tide gauge data, satellite observations, ocean modelling and GPS to assess sea level change around Tuvalu.\nRemoving all the factors which affect short-term sea level fluctuation, the study authors found long-term sea level at Tuvalu from 1950-2009 rose at the rate of 5.1 (±0.7) mm per year. This almost 3 times greater than the global average sea level over that time of 1.8 mm per year.\n10% of this 'total' sea level rise at Tuvalu is due to land subsidence\nBecker (2011) builds on earlier work by Church (2006) , and the South Pacific Sea Level and Climate Monitoring Project, which also confirm that sea level at Tuvalu is rising.\nAdvanced rebuttal written by dana1981\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 4 November 2016 by pattimer. View Archives"
  },
  {
   "title": "Underground temperatures control climate",
   "paragraph": "What influence do underground temperatures have on climate?\nLink to this page\nWhat the science says...\nThe flow of energy outwards from the interior of the Earth is 1/10,000th of the size of the energy flow from the Sun. Furthermore, over the past few million years, the heat flow from deep in the Earth has also remained very steady compared to other climatic factors. Heat from the bowels of the Earth does not influence climate in any significant way.\nClimate Myth...\nUnderground temperatures control climate\n\"There are other possible causes for climate change which could be associated with solar activity or related to variations in the temperature of the liquid core of the Earth, which is about 5,400 degrees Celsius. We don't need a high heat flow - just a high temperature for the core to affect the surface climate. There is massive heat inside the Earth\" (Doug Cotton)\nConsider:\nThe center of the Earth is at a temperature of over 6000°C, hotter than the surface of the Sun.\nWe have all seen pictures of rivers of red-hot magma pouring out of volcanoes.\nMany of us have bathed in natural hot springs.\nThere are plans to exploit geothermal energy as a renewable resource.\nCommon sense might suggest that all that heat must have a big effect on climate. But the science says no: the amount of heat energy coming out of the Earth is actually very small and the rate of flow of that heat is very steady over long time periods. The effect on the climate is in fact too small to be worth considering.\nThe Earth’s heat flow\nWhere does the heat come from?\nThere are radioactive elements in the Earth, mainly potassium, uranium, and thorium, that have long half-lives. When their nuclei decay, they give off heat, as in a nuclear reactor. Some researchers say that \"the vast majority of the heat in Earth's interior—up to 90 percent—is fueled by the decaying of radioactive isotopes\", while other scientists claim that \"heat from radioactive decay contributes about half of Earth’s total heat flux\". More here.\nThe Earth is still hot from the time the planet formed from the agglomeration of smaller bits and pieces. Even more heat was gained as the high-density materials, such as iron and nickel, subsequently separated out and formed the core of the Earth.\nThe mostly solid, rocky outer layers of the Earth, the crust and mantle, have low thermal conductivity, acting as a thermal blanket slowing down the passage of heat to the surface. In the very early stages of the Earth’s history, internal temperatures and heat flows were probably much higher than they are today, partly because the planet had only just started to cool, and partly because the energy flow from radioactive decay was much larger then.\nHow does the heat get to the surface?\nAccording to Stein and Stein (10MByte download) most of the heat energy (about 70%) that makes its way to the surface is transported by the convection of the mantle. This is the process that drives plate tectonics. Most of the rest of the heat flow, 25%, is by conduction. The small remainder is transported by mantle plumes, hot spots associated with certain volcanoes.\nFigure 1: Showing mantle convection cells, which are responsible for transporting most of the Earth’s heat from the interior to the surface. Wikipedia\nMantle convection cells are the super tankers of global tectonics, transporting vast quantities of hot rock but changing speed and direction only gradually. Conduction of heat through the rocks of the Earth’s continental crust is also an unhurried and stable process; with the supply of heat metered by atomic clockwork. There are a few well-known hot spots around the world, where magma and hot water quickly bring heat to the surface but the energy released at these places does not add up to much in the global scheme of things. The rate of heat escape from the Earth is slow and very steady.\nFigure 2: Red indicates the oceanic ridges where mantle convection comes to the surface and where new ocean crust is formed. The colors indicate the age of the oceanic crust, with the purple being the oldest. Source.\nHow do we measure heat flow?\nThe temperature gradient in the upper part of the crust is determined by directly measuring temperatures at different elevations in boreholes. On land, temperature measurements are usually made at depths greater than 100 metres to avoid any effect of variable surface temperatures. In the oceans, water temperatures at the sea bed are generally steady; measurements are made in the uppermost layer of sediments and yield reliable results. Once the thermal conductivity is known (it can be measured in a laboratory) the heat flow can be calculated using Fourier’s equation:\nq = -ku\nWhere q is the heat flow, k is the thermal conductivity, and u is the temperature gradient.\nFigure 3: Heat flow at the surface of the earth, from Davies and Davies (2010). Heat flow units are in mWm-2. Note how the areas of highest heat flow follow the mid-ocean ridges. The largest areas of measurement uncertainty are along the very crests of the ridges and under the Greenland and Antarctic ice caps. The total heat flow for the planet is 47 TW +/- 2TW, which is equivalent to 0.09Wm-2 (90mWm-2).\nTypically, the rate at which temperature increases with depth (the geothermal gradient) is in the range of 25-30°C per kilometer, with higher values at volcanoes, ocean ridges and rifts, and lower values in places that have recently received thick blankets of sediments. The top several hundred metres of boreholes often show changes in the geothermal gradient that are caused by changes in the surface temperature that have modified the temperature of the rocks at these shallow depths. These observations can be inverted to reveal paleoclimate information over the past few hundred years; see Huang et al (2000) and Beltrami et al (2011).\nHow does heat flow from the interior of the Earth compare with other inputs of energy into the climate system?\nFigure 4: The volumes of the cubes are proportional to the magnitude of the energy flow from various sources. The solar irradiance is the incident energy, averaged over the area of the Earth (divided by four); irradiance varies over 11 year cycles and, at the top of recent cycles, can reach 341.7 Wm-2. The increase in anthropogenic forcing since pre-industrial times comes from the IPCC. The heat flow from the Earth’s interior is the 47 TW figure (see Figure 3 caption) averaged over the surface area. The energy flow from the human energy production is based on Flanner (2009). Tidal energy is the total energy input from the gravitational interaction between the Earth, Moon and Sun; a small part of this energy is included in the energy flow from the Earth’s interior (see below for further discussion).\nThe net increase in the amount of planetary energy flow arising from human activities (mainly the emision of carbon dioxide) since the industrial revolution is more than twenty times the steady-state heat flow from the Earth’s interior. Any small changes in the Earth’s heat flow over that time period—and there is no evidence for any change at all—would plainly be inconsequential.\nTidal Energy\nFrom the Skeptical Science comments:\n\"Over the last two weeks I have been doing calculations on borehole data and this very convincingly supports the theory. We see different underground temperatures which are related to latitude, thus confirming that frictional heat (due to the moon) is being generated in the core, more at the equator than at the poles.\"\nThe spinning of the Earth, as well as the rotation of the Moon around the Earth and the orbit of both bodies around the Sun, do indeed have an impact on the energy of the Earth, through tidal friction. The ultimate source of this energy is the Earth’s rotation, to which the Moon and the Sun provide a gentle brake, resulting the generation of frictional heat and the slowing down of the Earth’s rotation (days were two hours shorter 600 million years ago). The Moon gains some energy from this interaction, being gradually boosted into a higher orbit above the Earth. The total Earth energy flow from tidal effects is about 3.7 TW (0.007 Wm-2 ), of which 95% goes into the familiar ocean tides and some 5% (0.2 TW or 0.0004 Wm-2) goes into Earth tides, which are small deformations of up to a few centimetres that occur on twice-daily or longer timescales. Earth tides contribute approximately 0.5% to the heat flow of the Earth.\nFigure 5: From Munk and Wunsch (1998) showing an “impressionistic” (their word) budget of tidal energy fluxes.\nThe energy from tides in the oceans is dissipated as heat in marginal areas (shallow waters) and around ocean ridges and seamounts (the “stir sticks” of the oceans). All of this energy is therefore added immediately to the ocean-atmosphere system. As for the Earth tides, the slight flexing of the crust and mantle is dissipated as heat there. This is a very small amount relative to the heat coming from radioactive decay and from the heat associated with the formation and differentiation of the Earth.\nThe amount of Earth tide energy flow, 200 gigawatts is miniscule by any planetary standard, it hardly varies at all over periods of millions of years and has no significant effect, globally or regionally, on the energy balance of the climate system.\nScience isn’t always common sense\nDiagrams such as the one below and its accompanying article make no mention of geothermal heat, tidal energy or “waste” heat from human fossil or nuclear energy use. Is this because its author, Kevin Trenberth, is negligent and unaware how big these sources of energy are? No, it’s actually because he knows how inconsequential they are.\nFigure 6. The global annual mean Earth’s energy budget for 2000 to 2005 (W m–2). The widths of the columns are proportional to the sizes of the energy flows. From Trenberth et al (2009).\nFor example, on this figure, a line representing geothermal energy flow would have a thickness of 6 microns, the thickness of a strand of spider-web silk; ocean tidal energy, one-tenth of that; Earth tidal energy less than one-tenth even of that. Our intuitions tell us that earthquakes, volcanoes, geysers and tides are mighty forces of nature and, in relation to a human individual, they are. But compared to the transfers of energy within the climate system, they are too puny to merit consideration.\nLast updated on 19 September 2011 by Andy Skuce."
  },
  {
   "title": "Veganism is the best way to reduce carbon emissions",
   "paragraph": "Is Veganism the best way to reduce carbon emissions\nLink to this page\nWhat the science says...\nVeganism has the potential to reduce global GHG emissions; however, it is not the only option and may not be the most achievable.\nClimate Myth...\nVeganism is the best way to reduce carbon emissions\n\"eating vegan foods rather than animal-based ones is the best way to reduce your carbon footprint.\" [PETA]\nAnimal agriculture and GHG emissions\nIt has been estimated that 11-15% of human caused greenhouse gas emissions are a result of animal agriculture and livestock globally (FAO 2013.; Climate watch 2018). These gases include methane from enteric fermentation of cattle, nitrous oxide mainly from manure and carbon dioxide. The levels of each that are contributed vary depending on geographical location and the source of meat. Animal agriculture also uses vast amounts of other resources like land, fertilizer, feed and water, for example 8% of global water usage is taken up by livestock (Schlink 2010).\nFigure 1: Global human caused GHG emissions divided by sector as estimated by the Center for Climate and Energy Solutions 2013\nA film by Kip Anderson; ‘Cowspiracy: The Sustainability Secret (2014)’ rallied for veganism as being the best way to reduce GHG emissions with claims that it is the ‘solution to climate change, to stop eating animals’ and ‘the only way to sustainably and ethically live on this planet with seven billion other people is to live an entirely plant-based vegan diet.’ This message was backed up by unreliable sources, for example quoting work by Goodland (2009) that animal agriculture is responsible for 51% of GHG emissions, when it has been estimated to be 11-15% (FAO 2013., Climate watch 2018). Although veganism does have the potential to reduce GHG emissions associated with diet, it is important to consider other sectors that are also part of the problem.\nAre people willing to change their diets for the environment?\nGraça et al. (2015) showed that 60% of people have intention to change the amount of meat they consume; however, only 48.8% are actually willing to reduce their meat intake and only 44.4% are willing to completely change to a plant-based diet. Compared to other dietary changes people are willing to make to advantage the environment, only 46.9% of people are reducing their meat intake compared to 67.7% buying regional food and 62.9% eating seasonal fruit and vegetables (Tobler et al. 2011).\nEating meat is also perceived to have the least environmental impact compared to produce with excess packaging or inorganic and imported produce. This suggests that many people may not be willing to eat a plant-based diet because they do not perceive eating meat to be a large environmental issue. Global veganism would require a mass change in mindset; people that show an attachment to meat are unlikely to respond well when faced with direct approaches to try and elicit change as they will react in a defensive manner (Graça 2015). Films such as Cowspiracy may not be the most productive way to encourage reduced meat intake and may have the opposite effect of making them firmer in their reasons for eating meat (Graça 2015).\nFigure 2: Willingness and intent of people to switch to a plant based diet using data taken from Graça et al. (2015)\nProblems with the vegan diet\nAbout one-third of all food grown is lost as waste, during transport and processing as well as at the retailer and consumer level (Gustavsson et al. 2011). Food waste is a contributor to food related GHG emissions (Scarborough 2014). Fruit and vegetables make up 39% of food waste with dairy and meat making up 17% and 14% respectively (Conrad et al. 2018). It is currently estimated that 4.2 trillion gallons of irrigation water and 780 million pounds of pesticides are depleted as a result of food losses and waste, most of which are used to produce fruit and vegetables (Conrad et al. 2018). With an increase in fruit and vegetable intake with a rise in vegan diets, the contribution of fruit and vegetable waste to GHG emissions and other depleted resources is likely to rise.\nThe large contribution of this waste attributed to fruit and vegetables is a result of retailer grading on its appearance due to the consumer avoiding disfigured produce. In an attempt to reduce the environmental burden of food, a reduction in the amount of waste on an individual level, specifically fruit and vegetables, is necessary, particularly if there is an increasing amount of these high-waste products due to more people switching to a vegan diet.\nFigure 3: Proportion of waste attributed to different food groups by US consumers. Other comprises of groups all with a percentage of <10% including: candy, soft drinks, salty snacks, soup, potatoes and mixed potato dishes, nuts and seeds, Mexican dishes, eggs and mixed egg dishes, table oils and salad dressing. Data taken from Conrad et al. (2018).\nCarlsson-Kanyama (2009) showed that, ‘foods that commonly have low emissions, such as fruits, when they are transported by air, may have emissions as large as some types of meat.’ For example, domestic pork and chicken generate 9.3 and 4.3 kg of CO2-equivalent per kg of product, respectively, whereas fresh tropical fruit transported by plane generates 11 kg CO2-equivalent per kg of product. Therefore, although eating a vegan diet has the potential to reduce emissions, other types of foods that also have a large environmental impact should be reduced, which are not necessarily facilitated by a vegan diet. Heller (2013) noted the environmental impact of vegetables produced in heated greenhouses can be greater than animal-derived products, showing that it is important to think about the farming and transportation of all produce, reducing intake of out-of-season and tropical produce, not just livestock, to reduce the environmental impact of a diet.\nLand management strategies are an important factor to consider when trying to mitigate some of the emissions associated with growing fruit and vegetables. Zomer (2017) claim that \"land management strategies can increase soil carbon stocks on agricultural lands with practises including addition of organic manures, cover cropping, mulching, conservation tillage, fertility management and rotational grazing.\" The importance of this, as highlighted by Zomer (2017), is that mismanaged land can release carbon from topsoil into the atmosphere, with as estimated 50-70% of carbon soil stocks already lost in cultivated soils. Regard for the use of sustainable land management strategies with the increase in demand for non-meat produce that comes alongside veganism would be important; however, these management strategies, such as rotational grazing and organic manures, require livestock and therefore do not fit with a vegan diet. Perhaps rather than full veganism, it is necessary to consider a balance of sustainably-produced food using the land management strategies suggested by Zomer (2017).\nVegan vs. Vegetarian vs. meat diets\nA vegan diet can reduce GHG emissions by 26% compared to the average meat-eating UK diet and a vegetarian diet could see a 22% reduction (Berners-Lee 2012; Saxe 2013; Scarborough 2014). Recent research by Kim et al. (2019) compared a flexitarian two-thirds vegan diet (which involved eating vegan for 2 meals per day and no restrictions on the third meal) to a fully vegetarian diet and found that in 95% of countries studied, the two-thirds diet produced less GHG emissions than the vegetarian diet. Therefore, there are arguments that a flexitarian diet with moderate amounts of meat is better than a vegetarian diet that cuts out meat completely, showing that stopping meat intake completely does not necessarily reduce dietary GHG emissions and cannot be assumed to do so in a vegan diet.\nWhat are the alternatives?\nIncreasing understanding of livestock emissions may mean there is potential to partially mitigate the high levels of GHG emissions associated with livestock, without abandoning livestock production. For example, adding fumaric acid to goat feed has shown to reduce their methane production by 18-31% (Li 2018). However, this additive has been less successful in cattle, the largest culprits of methane gas production. Alternatively, adding seaweed (Asparagopsis armata) to the diet of dairy cow can reduce methane emissions by up to 67% (Roque et al. 2019)\nGlobal veganism has the potential to reduce global GHG emissions; however, it is not the only option and may not be the most achievable. A combination of moderated red meat, lamb, and dairy intake along with sustainable farming techniques and locally-sourced produce to avoid airfreight would have a positive impact on the environment. However, it is important not to overemphasize one source of GHG emissions, for example livestock, as it can distract from the need to reduce human-caused emissions across a range of sectors. Films like ‘Cowspiracy’ that oversimplify the problem in order to advocate a single policy outcome may be similarly counterproductive as films that deny the problem in the first place.\nRelated research\nRecent work by Allen et al. 2018 has highlighted that standardised measures of greenhouse gases such as GWP100 or GWP 20 can debated as to whether they represent accurate measurement of methane and its effects which may be may be overstated with these metrics. A future blog post discussing this is in the works.\nReferences:\nAllen, MR., Shine, KP., Fuglestvedt, JS., Millar, RJ., Cain, M., Frame, DJ., Macey, AH. (2018). A solution to the misrepresentations of CO2-equivalent emissions of short-lived climate pollutants under ambitious mitigation. Npj Climate and Atmospheric Science, 1:16. [Online].\nBerners-Lee, M., Hoolohan, C., Cammack, H., Hewitt, CN. (2012). The relative greenhouse gas impacts of realistic dietary choices. Energy Policy, 43, 184-190.\nCarlsson-Kanyama, A., González, AD. (2009). Potential contributions of food consumption patterns to climate change. The American Journal of Clinical Nutrition, 89(5), 1704-1709.\nClimate Watch (2018). [Online] Avaliable at: https://www.climatewatchdata.org/sectors/agriculture?emissionType=1&filter=#drivers-of-emissions\nConrad, Z., Niles, MT., Neher, DA., Roy, ED., Tichenor, NE., Jahns, L. (2018). Relationship between food waste, diet quality and environmental sustainability. Plos One. [Online].\nGerber, P., Steinfield, H., Henderson, B., Mottet, A., Opio, C., Dijkman, J., Falcucci, A., Tempio, G. (2013). Tackling Climate change through livestock: a global assessment of emissions and mitigation opportunities. FAO, Rome.\nGoodland, R., Anhang, J. (2009). Livestock and Climate Change: What is the key actors in climate change are cows, pigs, and chickens? World Watch. [Online].\nGraça, J., Abílio, O., Calheiros, MM. (2015). Meat, beyond the plate. Data driven hypotheses for understanding consumer willingness to adopt a more plant-based diet. Appetite, 90, 80-90.\nGraça, J., Calheiros MM., Oliveira, A. (2015) Attached to meat? (Un)Willigness and intentions to adopt a more plant-based diet. Appetite, 95, 113-125.\nGustavsson, J., Cederberg, C., Sonesson, U. (2011). Global Food Losses and Food Waste. Rome: FAO.\nHeller, MC., Keoleian, GA., Willett, WC. (2013). Toward a life cycle-based, diet-level framework for food environment impact and nutritional quality assessment: A critical review. Environmental Science and Technology, 47, 12632-12647.\nKim,BF., Santo, RE., Scatterday, AP., Fry, JP. Synk, CM., et al. (2019). Country-specific dietary shifts to mitigate climate and water crisis. Global Environmental Change. [Online].\nLi, Z., Liu, N., Coa, Y., Jin, C., Li, F., Cai, C., Yao, J. (2018). Effects of fumaric acid supplementation on methane production and rumen fermentation in goats fed diets varying in forage and concentrate particle size. Journal of Animal Science and Biotechnology, 9(21).\nRoque, BM., Slawn, JK., Kinely, R., Kebreab, E. (2019) Inclusion of Asparagopsis armada in lactating dairy cows' diet reduces enteric methane emission by over 50 percent. Journal of Cleaner Production, 234, 132-138.\nSaxe, H., Larsen, TM., Mogensen, L. (2013). The Global warming potential of two healthy Nordic diets compared with the average Danish diet. Climate Change, 116, 249-262.\nScarborough, P., Appleby, PN., Mizdrak, A., Briggs, ADM., Travis, RC., et al. (2014). Dietary greenhouse gas emissions of meat-eaters, fish-eaters, vegetarians and vegans in the UK. Climate Change, 125, 179-192.\nSchlink, AC., Viljoen, GJ. (2010). Water requirements for livestock production: a global perspective. Scientific and Technical Review of the Office International des Epizooties, 29(3), 603-619.\nTobler, C., Visschers, VHM., Siegrist, M. (2011). Eating Green. Consumers’ willingness to adopt ecological food consumption behaviours. Appetite, 57, 674-682.\nZomer, RJ., Bossio, DA., Sommer, R. Verchot, LV. (2017). Global sequestration potential of increased organic carbon in cropland soils. Scientific Reports, 7(15554) [Online].\nLast updated on by dana1981. View Archives"
  },
  {
   "title": "Venus doesn't have a runaway greenhouse effect",
   "paragraph": "The runaway greenhouse effect on Venus\nLink to this page\nWhat the science says...\nVenus very likely underwent a runaway or ‘moist’ greenhouse phase earlier in its history, and today is kept hot by a dense CO2 atmosphere.\nClimate Myth...\nVenus doesn't have a runaway greenhouse effect\nVenus is not hot because of a runaway greenhouse.\nIn keeping with my recent theme of discussing planetary climate, I am revisiting a claim last year made by Steven Goddard at WUWT (here and here, and echoed by him again recently) that “the [runaway greenhouse] theory is beyond absurd,” and that it is pressure, not the greenhouse effect that keeps Venus hot. My focus in this post is not on his alternative theory (discussed here), but to discuss Venus and the runaway greenhouse in general, as a matter of interest and as an educational opportunity. In keeping my skepticism fair, I’d also like to address claims (sometimes thrown out by Jim Hansen in passing by) that burning all the coal, tars, and oil could conceivably initiate a runaway on Earth.\nIt is worth noting that the term runaway greenhouse refers to a specific process when discussed by planetary scientists, and simply having a very hot, high-CO2 atmosphere is not it. It is best thought of as a process that may have happened in Venus’ past (or a large number of exo-planets being discovered close enough to their host star) rather than a circumstance it is currently in.\nA Tutorial of Present-Day Venus\nVenus’ orbit is approximately 70% closer to the sun, which means it receives about 1/0.72 ~ 2 times more solar insolation at the top of the atmosphere than Earth. Venus also has a very high albedo which ends up over-compensating for the distance to the sun, so the absorbed solar energy by Venus is less than that for Earth. The high albedo can be attributed to a host of gaseous sulfur species, along with what water there is, which provide fodder for several globally encircling sulfuric acid (H2SO4) cloud decks. SO2 and H2O are the gaseous precursor of the clouds particles; the lower clouds are formed by condensation of H2SO4 vapor, with SO2 created by photochemistry in the upper clouds. Venus’ atmosphere also has a pressure of ~92 bars, nearly equivalent to what you’d feel swimming under a kilometer of ocean. The dense atmosphere could keep the albedo well above Earth’s even without clouds due to the high Rayleigh scattering (the effect of clouds on Venus and how they could change in time is discussed in Bullock and Grinspoon, 2001). Less than 10% of the incident solar radiation reaches the surface.\nObservations of the vapor content in the Venusian atmosphere show an extremely high heavy to light isotopic ratio (D/H) and is best interpreted as a preferential light hydrogen escape to space, while deuterium escapes less rapidly. A lower limit of at least 100 times its current water content in the past can be inferred (e.g. Selsis et al. 2007 and references therein).\nThe greenhouse effect on Venus is primarily caused by CO2, although water vapor and SO2 are extremely important as well. This makes Venus very opaque throughout the spectrum (figure 1a), and since most of the radiation that makes its way out to space comes from only the very topmost parts of the atmosphere, it can look as cold as Mars from IR imagery. In reality, Venus is even hotter than the dayside of Mercury, at an uncomfortable 735 K (or ~860 F). Like Earth, Venusian clouds also generate a greenhouse effect, although they are not as good infrared absorbers/emitters as water clouds. However, the concentrated sulfuric acid droplets can scatter infrared back to the surface, generating an alternative form of the greenhouse effect that way. In the dense Venusian CO2 atmosphere, pressure broadening from collisions and the presence of a large number of absorption features unimportant on modern Earth can come into play (figure 1b), which means quick and dirty attempts by Goddard to extrapolate the logarithmic dependence between CO2 and radiative forcing make little sense. The typical Myhre et al (1998) equation which suggests every doubling of CO2 reduces the outgoing flux at the tropopause by ~4 W/m2, although even for CO2 concentration typical of post-snowball Earth states this can be substantially enhanced. Figure 1b also shows that CO2 is not saturated, as some skeptics have claimed.\nFigure 1: a) Radiant spectra for the terrestrial planets. Courtesy of David Grisp (Jet Propulsion Laboratory/CIT), from lecture \"Understanding the Remote-Sensing Signatures of Life in Disk-averaged Planetary Spectra: 2\" b) Absorption properties for CO2. The horizontal lines represent the absorption coefficient above which the atmosphere is strongly absorbing. The green (orange) rectangle shows that portion of the spectrum where the atmosphere is optically thick for 300 (1200) ppm. From Pierrehumbert (2011)\nHow to get a Runaway?\nTo get a true runaway greenhouse, you need a conspiracy of solar radiation and the availability of some greenhouse gas in equilibrium with a surface reservoir (whose concentration increases with temperature by the Clausius-Clapeyron relation). For Earth, or Venus in a runaway greenhouse phase, the condensable substance of interest is water— although one can generalize to other atmospheric agents as well.\nThe familiar water vapor feedback can be illustrated in Figure 2, whereby an increase in surface temperature increases the water vapor content, which in turn results in increased atmospheric opacity and greenhouse effect. In a plot of outgoing radiation vs. temperature, this would result in less sensitive change in outgoing flux for a given temperature change (i.e., the outgoing radiation is more linear than one would expect from the σT4 blackbody-relation).\nFigure 2: Graph of the OLR vs. T for different values of the CO2 content and relative humidity. For a fixed RH, the specific humidity increases with temperature. The horizontal lines are the absorbed shortwave radiation, which can be increased from 260-300 W m-2. The water vapor feedback manifests itself as the temperature difference between b’-b and a’-a, since water vapor feedback linearizes the OLR curve. Eventually the OLR asymptotes at the Komabayashi-Ingersoll limit. Adopted from Pierrehumbert (2002)\nOne can imagine an extreme case in which the water vapor feedback becomes sufficiently effective, so that eventually the outgoing radiation is decoupled from surface temperature, and asymptotes into a horizontal line (sometimes called the “Komabayashi-Ingersoll” limit following the work of the authors in the 1960’s, although Nakajima et al (1992) expanded upon this limiting OLR in terms of tropospheric and stratospheric limitations). In order to sustain the runaway, one requires a sufficient supply of absorbed solar radiation, as this prevents the system from reaching radiative equilibrium. Once the absorbed radiation exceeds the limiting outgoing radiation, then a runaway greenhouse ensues and the radiation to space does not increase until the oceans are depleted, or perhaps the planet begins to get hot enough to radiate in near visible wavelengths.\nFigure 3: Qualitative schematic of how the ocean reservoir is depleted in a runaway. From Ch. 4 of R.T. Pierrehumbert’s Principles of Planetary Climate\nOn present-day Earth, a “cold trap” limits significant amounts of water vapor from reaching the high atmosphere, so its fate is ultimately to condense and precipitate out. In a runaway scenario, this “cold trap” is broken and the atmosphere is moist even into the stratosphere. This allows energetic UV radiation to break up H2O and allow for significant hydrogen loss to space, which explains the loss of water over time on Venus. An intermediate case is the “moist greenhouse” (Kasting 1988) in which liquid water can remain on the surface, but the stratosphere is still wet so one can lose large quantities of water that way (note Venus may never actually encountered a true runaway, there is still debate over this). Kasting (1988) explored the nature of the runaway /moist greenhouse, and later in 1993 applied this to understanding habitable zones around main-sequence stars. He found that a planet with a vapor atmosphere can lose no more than ~310 W/m2, which corresponds to 140% of the modern solar constant (note the albedo of a dense H2O atmosphere is higher than the modern), or about 110% of the modern value for the moist greenhouse.\nEarth and the Runaway: Past and Future\nBecause Earth is well under the absorbed solar radiation threshold for a runaway, water is in a regime where it condenses rather than accumulating indefinitely in the atmosphere. The opposite is true for CO2, which builds up indefinitely unless checked by silicate weathering or ocean/biosphere removal processes. In fact, a generalization to the runaway threshold thinking is when the solar radiation is so low, so that CO2 condenses out rather than building up in the atmosphere, as would be the case for very cold Mars-like planets. Note the traditional runaway greenhouse threshold is largely independent of CO2 (figure 2 & 4; also see Kasting 1988), since the IR opacity is swamped by the water vapor effect. This makes it very difficult to justify concerns over an anthropogenic-induced runaway.\nFigure 4: The H2O–CO2 greenhouse. The plot shows the surface temperature as a function of radiated heat for different amounts of atmospheric CO2 (after Abe 1993). The albedo is the fraction of sunlight that is not absorbed (the appropriate albedo to use is the Bond albedo, which refers to all sunlight visible and invisible). Modern Earth has an albedo of 30%. Net insolations for Earth and Venus ca. 4.5 Ga (after the Sun reached the main sequence) are shown at 30% and 40% albedo. Earth entered the runaway greenhouse state only ephemerally after big impacts that generated big pulses of geothermal heat. For example, after the Moon-forming impact the atmosphere would have been in a runaway greenhouse state for ∼2 million years, during which the heat flow would have made up the difference between net insolation and the runaway greenhouse limit. A plausible trajectory takes Earth from ∼100 bars of CO2 and 40% albedo down to 0.1–1 bar and 30% albedo, at which point the oceans ice over and albedo jumps. Note that CO2 does not by itself cause a runaway. Also note that Venus would enter the runaway state when its albedo dropped below 35%. Se e Zahnle et al 2007\nThis immunity to a runaway will not be the case in the long-term. In about a billion years, the sun will brighten enough to push us into a state where hydrogen is lost much more rapidly, and a true runaway greenhouse occurs in several billion years from now, with the large caveat that clouds could increase the albedo and delay this process.\nInteresting, some (e.g.. Zahnle et al 2007) have argued that Earth may have been in a transient runaway greenhouse phase within the first few million years, with geothermal heat and the heat flow from the moon-forming impact making up for the difference between the net solar insolation and the runaway greenhouse threshold, although this would last for only a brief period of time. Because the runaway threshold also represents a maximum heat loss term, it means the planet would take many millions of years to cool off following such magma ocean & steam atmosphere events of the early Hadean, much slower than a no-atmosphere case (figure 5).\nFigure 5: Radiative cooling rates from a steam atmosphere over a magma ocean. The radiated heat is equal to the sum of absorbed sunlight (net insolation) and geothermal heat flow. The plot shows the surface temperature as a function of radiated heat for different amounts of atmospheric H2O (adapted from Abe et al. 2000). The radiated heat is the sum of absorbed sunlight (net insolation) and geothermal heat flow. The different curves are labeled by the amount of H2O in the atmosphere (in bars). The runaway greenhouse threshold is indicated. This is the maximum rate that a steam atmosphere can radiate if condensed water is present. If at least 30 bars of water are present (a tenth of an ocean), the runaway greenhouse threshold applies even over a magma ocean. Note that the radiative cooling rate is always much smaller than the σT4 of a planet without an atmosphere\nConclusions\nVenus likely underwent a runaway or “moist greenhouse” phase associated with rapid water loss and very high temperatures. Once water is gone, silicate weathering reactions that draw down CO2 from the atmosphere are insignificant, and CO2 can then build up to very high values. Today, a dense CO2 atmosphere keeps Venus extremely hot.\nLast updated on 11 April 2011 by Chris Colose."
  },
  {
   "title": "Volcanoes emit more CO2 than humans",
   "paragraph": "Do volcanoes emit more CO2 than humans?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nHumans emit 100 times more CO2 than volcanoes.\nClimate Myth...\nVolcanoes emit more CO2 than humans\n\"Human additions of CO2 to the atmosphere must be taken into perspective.\nOver the past 250 years, humans have added just one part of CO2 in 10,000 to the atmosphere. One volcanic cough can do this in a day.\" (Ian Plimer)\nThe solid Earth contains a huge quantity of carbon, far more than is present in the atmosphere or oceans. Some of this carbon is slowly released from the rocks in the form of carbon dioxide, through vents at volcanoes and hot springs. Volcanic emissions are a small but important part of the global carbon cycle. Published reviews of the scientific literature by Mörner and Etiope (2002) and Kerrick (2001) report a range of emission of 65 to 319 million tonnes of CO2 per year. Counter claims that volcanoes, especially submarine volcanoes, produce vastly greater amounts of CO2 than these estimates are not supported by any papers published by the scientists who study the subject.\nThe burning of fossil fuels and changes in land use results in the emission into the atmosphere of approximately 34 billion tonnes of carbon dioxide per year worldwide, according to the U.S. Energy Information Administration (EIA). The fossil fuels emissions numbers are about 100 times bigger than even the maximum estimated volcanic CO2 fluxes. Our understanding of volcanic discharges would have to be shown to be very mistaken before volcanic CO2 discharges could be considered anything but a bit player in contributing to the recent changes observed in the concentration of CO2 in the Earth's atmosphere.\nVolcanoes can—and do—influence the global climate over time periods of a few years but this is achieved through the injection of sulfate aerosols into the high reaches of the atmosphere during the very large volcanic eruptions that occur sporadically each century. But that's another story...\nRecommended further reading on CO2 and volcanoes can be found here: Terry Gerlach in Earth Magazine ; USGS\nLast updated on 2 June 2017 by John Cook. View Archives"
  },
  {
   "title": "Water levels correlate with sunspots",
   "paragraph": "Water levels and solar activity\nLink to this page\nWhat the science says...\nThere seems to be evidence for a link between solar activity and water levels. However, more direct comparisons between solar activity and global temperature finds that as the sun grew hotter or cooler, Earth's climate followed it with a 10 year lag - presumably due to the dampening effect of the ocean. Also found was that the correlation between solar activity and global temperatures ended around 1975, hence recent warming must have some other cause than solar variations.\nClimate Myth...\nWater levels correlate with sunspots\nSeveral studies have found a link between solar activity and water levels. A South African paper has found a 21 year cycle synchronous with the solar cycle (Alexander 2007). Another paper finds a link between galactic cosmic rays and precipitation in the Midwestern USA (Perry 2007).\nThere is no denying there's a strong link between solar activity and climate - both with the short term (eg - the 11 year cycle) and long term (eg - decadal changes in solar activity). In fact, the close correlation between sun and temperature is what tells us the sun can't be causing recent global warming as solar activity has been steady since the 50's. So what do studies into precipitation tell us?\nSolar cycles and South African water levels\nAlexander 2007 finds a strong correlation between water levels and sunspot numbers. But the correlation is short term - there is little to no correlation in the long term trends.\nFor example, there is no long term trend in Lake Victoria's levels from 1900 to 1940 when solar activity showed long term increase. Any short or long term correlation breaks down between 1930 to 1970. Next, to obtain correlation over 1968 to 2005, they filter out a 29mm per year trend where there's been no long term solar trend.\nIn fact, all the case studies show short term correlation with solar cycles but no long term correlation with decadal solar trends. There's so much noise due to tributary inflows, outflows, sluicing, rainfall and evaporation that while short term correlations with the solar cycle are useful, determining or finding meaning in long term trends is problematic. In short, measuring water levels is a roundabout way of determing the sun's effect on long term global warming.\nGalactic cosmic rays and Mississippi River streamflow\nPerry 2007 purports to link solar activity and Mississippi River flow. The physical process is as follows - the 11 year solar cycle causes a cycle of ocean heat somewhere between the western Pacific Ocean and the Indian Ocean. The heat cycle is transported via the ocean conveyor system, emerging 3 decades later in the North Pacific Ocean. Sea surface temperatures pick up the cycle which travels via a low-level atmospheric vorticity, affecting regional rainfall in the US midwest.\nWhat is the evidence to suggest the solar signal has emerged intact through this decades long process? The authors discovered a high correlation between Total Solar Irradiance (TSI) and river flow - the correlation is highest when TSI was offset 33 years.\nCorrelation for different lag times between TSI, Geomagnetic Index-AA and Mississippi River flow\nThe idea that the amplitude and phasing of the solar signal is preserved throughout a 33 year, multi-step process sounds far-fetched. Even the author concedes \"This lag time was unexpected, and its validity questioned.\" This is especially the case when you consider correlation breaks down along the way - amplitudes don't correlate between tropical Pacific ocean temperatures and North Pacific ocean temperatures.\nIt's no coincidence that the correlation peaks occur at 11 year intervals - the same period as the solar cycle. As time lag increases, the solar cycle synchronises with the river flow cycle every 11 years. As for the high correlation at the 33 year mark, this may be a statistical anomaly or there may be a physical link. Perhaps the dampening effect of the ocean smoothed out the 11 year signal but retains the long term trends. If so, the basic thesis if not the specifics of the paper may be true - the ocean absorbs heat then releases it again 30 years later.\nWhat does this mean for the global warming debate? The answer is to look for a correlation between solar activity and global temperature and if so, is there a lag? In fact, such a study has been done.\nThe link between solar activity and global temperature\nIn 2005, scientists from Finland and Germany compared solar activity & temperatures over the past 1150 years and found a strong correlation between solar activity and global temperature (Usoskin 2005). They found the correlation was highest when temperature lagged solar activity by 10 years.\nCorrelation between TSI and temperature for different time lags\nIn other words, as the sun grew hotter or cooler, Earth's climate followed it with a 10 year lag - presumably due to the dampening effect of the ocean. The other crucial discovery was the correlation between solar activity and global temperatures ended around 1975. At that point, temperatures started rising while solar activity stayed level. This led them to conclude \"during these last 30 years the solar total irradiance, solar UV irradiance and cosmic ray flux has not shown any significant secular trend, so that at least this most recent warming episode must have another source.\" More on the sun...\nLast updated on 26 June 2010 by John Cook."
  },
  {
   "title": "Water vapor in the stratosphere stopped global warming",
   "paragraph": "What is the role of stratospheric water vapor in global warming?\nLink to this page\nWhat the science says...\nThe effect from stratospheric water vapor contributes a fraction of the temperature change imposed from man-made greenhouse gases. Also, it's not yet clear whether changes in stratospheric water vapor are caused by a climate feedback or internal variability (eg - linked to El Nino Southern Oscillation). However, the long term warming trend seems to speak against the possibility of a negative feedback.\nClimate Myth...\nWater vapor in the stratosphere stopped global warming\nA new study authored by Susan Solomon, lead author of the study and a researcher at the National Oceanic and Atmospheric Administration in Boulder, Colo. could explain why atmospheric carbon is not contributing to warming significantly. According to the study, as carbon levels have risen, the cold air at high altitudes over the tropics has actually grown colder. The lower temperatures at this \"coldest point\" have caused global water vapor levels to drop, even as carbon levels rise. Water vapor helps trap heat, and is a far the strongest of the major greenhouse gases, contributing 36–72 percent of the greenhouse effect. However more atmospheric carbon has actually decreased water vapor levels. Thus rather than a \"doomsday\" cycle of runaway warming, Mother Earth appears surprisingly tolerant of carbon, decreasing atmospheric levels of water vapor -- a more effective greenhouse gas -- to compensate. (Daily Tech)\nThe role of stratospheric water vapor is examined in Contributions of Stratospheric Water Vapor to Decadal Changes in the Rate of Global Warming (Solomon 2010). The atmosphere is divided into several layers. The troposphere is the lowest part of the atmosphere. It contains most of the atmosphere's water vapor, predominantly supplied by evaporation from the ocean surface. Through the troposphere, temperature falls as altitude rises. The boundary between the troposphere and stratosphere is called the tropopause. This is known as the \"cold point\", the coldest point in the lower atmosphere. In the stratosphere, temperature actually rises with altitude. It warms as you get higher - the opposite of the troposphere.\nFigure 1: Atmospheric layers: Troposphere, Stratosphere and Mesosphere\nSolomon 2010 looks at the trend of water vapor in the stratosphere. Before 1993, the only observations of stratospheric water vapor were made by weather balloons above Boulder, Colorado (black line in Figure 2). They observed a slight increase from 1980. After 1993, several different satellites also took measurements (coloured circles, squares and diamonds in Figure 2). The various observations all found a significant drop in stratospheric water vapor around 2000. Most of the change in water vapor occurs in the lower stratosphere, just above the tropopause. The greatest changes also occur in the tropics and subtropics.\nFigure 2: Observed changes in stratospheric water vapor. Black line: balloon measurements of water vapor, taken near Boulder Colorado. Blue diamonds: UARS HALOE satellite measurements. Red diamonds: SAGE II instruments. Turquoise squares: Aura MLS satellite measurements. Uncertainties given by colored bars (Solomon 2010).\nWhat effect would this have on climate? Figure 2 shows the change in radiative forcing imposed by changes in stratospheric water vapor. The dotted line is the radiative forcing without the effect of stratospheric water vapor changes. The grey shaded region shows the possible range of contribution from changing stratospheric water vapor. As it's a greenhouse gas, increasing water vapor has a warming effect. Consequently, the steady rise from 1980 to 2000 added some warming to the existing warming from greenhouse gases. The drop in water vapor after 2000 had a cooling effect.\nFigure 3: Impact of changes in stratospheric water vapor on radiative forcing since 1980 due to well-mixed greenhouse gases (WMGHG), aerosols, and stratospheric water vapor. The shaded region shows the stratospheric water contribution (Solomon 2010).\nWhat caused these changes? Water vapor in the stratosphere has two main sources. One is transport of water vapor from the troposphere which occurs mainly as air rises in the tropics. The other is the oxidation of methane which occurs mostly in the upper stratosphere. Most of the change in water vapor occurs in the lower stratosphere in the vicinity of regions affected by the El Nino Southern Oscillation. This seems to point towards convection and internal variability driving the changes. A comparison between stratospheric water vapor and tropical sea surface temperatures show good correlation which corroborates a link with El Nino. However, the correlation breaks down in some periods suggesting other processes may also be important. Consequently, the authors are cautious in coming to a firm conclusion on the cause.\nThere seem to be two major misconceptions arising from this paper. The first is that this paper demonstrates that water vapor is the major driver of global temperatures. In fact, what this paper shows is the effect from stratospheric water vapor contributes a fraction of the temperature change imposed from man-made greenhouse gases. While the stratospheric water vapor is not insignificant, it's hardly the dominant driver of climate being portrayed by some blogs.\nThe other misinterpretation is that this paper proves negative feedback that cancels out global warming. As we've just seen, the magnitude of the effect is small compared to the overall global warming trend. The paper doesn't draw any conclusions regarding cause, stating that it's not clear whether the water vapor changes are caused by a climate feedback or decadal variability (eg - linked to El Nino Southern Oscillation). The radiative forcing changes (Figure 3 above) indicate that the overall effect from stratospheric water vapor is that of warming. The cooling period consists of a stepwise drop around 2000 followed by a resumption of the warming effect. This seems to speak against the possibility of a negative feedback.\nLast updated on 26 June 2010 by John Cook."
  },
  {
   "title": "Water vapor is the most powerful greenhouse gas",
   "paragraph": "Explaining how the water vapor greenhouse effect works\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nIncreased CO2 makes more water vapor, a greenhouse gas which amplifies warming\nClimate Myth...\nWater vapor is the most powerful greenhouse gas\n“Water vapour is the most important greenhouse gas. This is part of the difficulty with the public and the media in understanding that 95% of greenhouse gases are water vapour. The public understand it, in that if you get a fall evening or spring evening and the sky is clear the heat will escape and the temperature will drop and you get frost. If there is a cloud cover, the heat is trapped by water vapour as a greenhouse gas and the temperature stays quite warm. If you go to In Salah in southern Algeria, they recorded at one point a daytime or noon high of 52 degrees Celsius – by midnight that night it was -3.6 degree Celsius. […] That was caused because there is no, or very little, water vapour in the atmosphere and it is a demonstration of water vapour as the most important greenhouse gas.” (Tim Ball)\nWhen skeptics use this argument, they are trying to imply that an increase in CO2 isn't a major problem. If CO2 isn't as powerful as water vapor, which there's already a lot of, adding a little more CO2 couldn't be that bad, right? What this argument misses is the fact that water vapor creates what scientists call a 'positive feedback loop' in the atmosphere — making any temperature changes larger than they would be otherwise.\nHow does this work? The amount of water vapor in the atmosphere exists in direct relation to the temperature. If you increase the temperature, more water evaporates and becomes vapor, and vice versa. So when something else causes a temperature increase (such as extra CO2 from fossil fuels), more water evaporates. Then, since water vapor is a greenhouse gas, this additional water vapor causes the temperature to go up even further—a positive feedback.\nHow much does water vapor amplify CO2 warming? Studies show that water vapor feedback roughly doubles the amount of warming caused by CO2. So if there is a 1°C change caused by CO2, the water vapor will cause the temperature to go up another 1°C. When other feedback loops are included, the total warming from a potential 1°C change caused by CO2 is, in reality, as much as 3°C.\nThe other factor to consider is that water is evaporated from the land and sea and falls as rain or snow all the time. Thus the amount held in the atmosphere as water vapour varies greatly in just hours and days as result of the prevailing weather in any location. So even though water vapour is the greatest greenhouse gas, it is relatively short-lived. On the other hand, CO2 is removed from the air by natural geological-scale processes and these take a long time to work. Consequently CO2 stays in our atmosphere for years and even centuries. A small additional amount has a much more long-term effect.\nSo skeptics are right in saying that water vapor is the dominant greenhouse gas. What they don't mention is that the water vapor feedback loop actually makes temperature changes caused by CO2 even bigger.\nBasic rebuttal written by James Frank\nUpdate July 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 July 2015 by pattimer. View Archives"
  },
  {
   "title": "We're coming out of the Little Ice Age",
   "paragraph": "What ended the Little Ice Age?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate Advanced\nThe sun was warming up then, but the sun hasn’t been warming since 1970.\nClimate Myth...\nWe're coming out of the Little Ice Age\n\"The global temperature has been rising at a steady trend rate of 0.5°C per century since the end of the little ice age in the 1700s (when the Thames River would freeze over every winter; the last time it froze over was 1804) ...\n...\nThe IPCC blames human emissions of carbon dioxide for the last warming. But by general consensus human emissions of carbon dioxide have only been large enough to be significant since 1940—yet the warming trend was in place for well over a century before that.\" (David Evans)\nClimate change sceptics suggest that because the climate has changed dramatically in the past – and without man’s intervention – it is possible that current changes to the Earth’s climate are also a natural event. You may be familiar with paintings depicting Londoners skating on the frozen River Thames, when winters, at least in the northern hemisphere, were more severe. The beginning and end of this period are subject to various interpretations, but the period is referred to as the Little Ice Age (LIA) and occurred between the 16th to 19th centuries.\nLimited History\nIf we are to understand the LIA, we need to figure out what caused it. Scientists have examined several important strands of evidence about the LIA, including the activity of the sun, of volcanoes, and ocean heat circulation, principle drivers of natural climate change.\nThe activity of the sun can be assessed by looking at proxies – processes we know are affected by the sun’s activity. One of these is the formation of the radioactive isotope Carbon-14 in the atmosphere, which plants then absorb. By measuring carbon-14 in tree rings and other materials we know are from a certain period, we can estimate how active the sun was at the time. This graph shows the sun’s activity over the last millennium:\nThe carbon-14 data used in this graph go up to 1950. The graph below gives a fuller picture, showing that in the last three decades, the sun's normal cycle of activity has remained steady, while temperatures have shot up:\nYet while the dips in solar activity correlate well with the LIA, there are other factors that, in combination, may have contributed to the climate change:\nVolcanic activity was high during this period of history, and we know from modern studies of volcanism that eruptions can have strong cooling effects on the climate for several years after an eruption.\nThe ‘ocean conveyor belt’ – thermohaline circulation – might have been slowed down by the introduction of large amounts of fresh water e.g. from the Greenland ice cap, the melting by the previous warm period (the Medieval Warm Period).\nSudden population decreased caused by the Black Death may have resulted in a decrease of agriculture and reforestation of agricultural land.\nCan We Draw a Conclusion?\nIn truth, not really. The Little Ice Age remains for the present the subject of speculation. The most likely influence during this period is variable output from the sun combined with pronounced volcanic activity. We know that from the end of the LIA to the 1950s the sun’s output increased. But since WW2 the sun has slowly grown quieter, yet the temperature on Earth has gone up.\nThe sceptical argument that current warming is a continuation of the same warming that ended the LIA is unlikely. There is a lack of evidence for a suitable forcing (e.g. the sun) and numerous correlations with known natural forcings that can account for the LIA itself, and the subsequent climate recovery. Taken in isolation, the LIA might cast doubt on the theory of climate change. Considered alongside the empirical evidence, model predictions and a century of scientific research into the climate, recovery from the LIA is not a plausible theory to explain the observed evidence and rate of global climate change.\nBasic rebuttal written by GPWayne\nUpdate July 2015:\nHere is the relevant lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 5 July 2015 by skeptickev. View Archives"
  },
  {
   "title": "We're heading into an ice age",
   "paragraph": "Are we heading into a new Ice Age?\nLink to this page\nWhat the science says...\nSelect a level... Basic Intermediate\nWorry about global warming impacts in the next 100 years, not an ice age in over 10,000 years.\nClimate Myth...\nWe're heading into an ice age\n\"One day you'll wake up - or you won't wake up, rather - buried beneath nine stories of snow. It's all part of a dependable, predictable cycle, a natural cycle that returns like clockwork every 11,500 years. And since the last ice age ended almost exactly 11,500 years ago…\" (Ice Age Now)\nAccording to ice cores from Antarctica, the past 400,000 years have been dominated by glacials, also known as ice ages, that last about 100,000. These glacials have been punctuated by interglacials, short warm periods which typically last 11,500 years. Figure 1 below shows how temperatures in Antarctica changed over this period. Because our current interglacial (the Holocene) has already lasted approximately 12,000 years, it has led some to claim that a new ice age is imminent. Is this a valid claim?\nFigure 1: Temperature change at Vostok, Antarctica (Petit 2000). The timing of warmer interglacials is highlighted in green; our current interglacial, the Holocene, is the one on the far right of the graph.\nTo answer this question, it is necessary to understand what has caused the shifts between ice ages and interglacials during this period. The cycle appears to be a response to changes in the Earth’s orbit and tilt, which affect the amount of summer sunlight reaching the northern hemisphere. When this amount declines, the rate of summer melt declines and the ice sheets begin to grow. In turn, this increases the amount of sunlight reflected back into space, increasing (or amplifying) the cooling trend. Eventually a new ice age emerges and lasts for about 100,000 years.\nSo what are today’s conditions like? Changes in both the orbit and tilt of the Earth do indeed indicate that the Earth should be cooling. However, two reasons explain why an ice age is unlikely:\nThese two factors, orbit and tilt, are weak and are not acting within the same timescale – they are out of phase by about 10,000 years. This means that their combined effect would probably be too weak to trigger an ice age. You have to go back 430,000 years to find an interglacial with similar conditions, and this interglacial lasted about 30,000 years.\nThe warming effect from CO2 and other greenhouse gases is greater than the cooling effect expected from natural factors. Without human interference, the Earth’s orbit and tilt, a slight decline in solar output since the 1950s and volcanic activity would have led to global cooling. Yet global temperatures are definitely on the rise.\nIt can therefore be concluded that with CO2 concentrations set to continue to rise, a return to ice age conditions seems very unlikely. Instead, temperatures are increasing and this increase may come at a considerable cost with few or no benefits.\nBasic rebuttal written by Anne-Marie Blackburn\nUpdate August 2015:\nHere is a related lecture-video from Denial101x - Making Sense of Climate Science Denial\nLast updated on 7 August 2015 by MichaelK. View Archives"
  },
  {
   "title": "Wildfires are not caused by global warming",
   "paragraph": "How human-caused global warming worsens wildfires\nLink to this page\nWhat the science says...\nGlobal warming worsens wildfires by drying vegetation and soil, creating more fuel for fires to spread further and faster. In some areas like southeastern Australia and California, altered atmospheric patterns may also be creating stronger and/or more frequent high pressure systems, resulting in less precipitation and thus both dryer conditions and longer fire seasons.\nClimate Myth...\nWildfires are not caused by global warming\n\"it’s not climate change that has caused today’s [bushfire] disaster, but the criminal negligence of governments that have tried to buy green votes by locking up vast tracts of land as national parks, yet failed to spend the money needed to control ground fuel and maintain fire trails ... We can’t dial down the Earth’s temperature any more than we can lock up every teenage arsonist.\" [Miranda Divine, NY Post]\nHeat worsens wildfires\nThe clearest connection between global warming and worsening wildfires occurs through increasing evapotranspiration and the vapor-pressure deficit. In simple terms. vegetation and soil dry out, creating more fuel for fires to expand further and faster. This is particularly a problem in Mediterranean climates that are prone to drought, like in California and Australia, as climate scientist Kevin Trenberth explains in the interview below with videographer Peter Sinclair.\nFor example, California's record-breaking wildfire season in 2018 came at the culmination of the state's five hottest years on record (2014–2018) and a record-breaking drought (2012–2017). Australia's record-breaking bushfire season of 2019–2020 followed the continent's two hottest and driest years on record, and expanded during a record-breaking heatwave that included an average country-wide high temperature of 41.9°C (107.4°F) on 18 December 2019.\nBecause of the long-term warming trend, the Fourth National Climate Assessment Report concluded,\n“Climate change has led to an increase in the area burned by wildfire in the western United States. Analyses estimate that the area burned by wildfire from 1984 to 2015 was twice what would have burned had climate change not occurred. Furthermore, the area burned from 1916 to 2003 was more closely related to climate factors than to fire suppression, local fire management, or other non-climate factors.\nClimate change has driven the wildfire increase, particularly by drying forests and making them more susceptible to burning.”\nAcres burned by wildfires in California 1987–2019, with the linear trend shown. Data from Cal Fire.\nCumulative forest area burned in the western United States 1984–2015, and attribution to human-caused climate change. Source: Fourth National Climate Assessment.\nChanging atmospheric circulations\nA second, though more scientifically uncertain connection between climate and worsening wildfires involves changing atmospheric circulation patterns.\nCalifornia's aforementioned record drought was exacerbated by a high-pressure ridge sitting off the Pacific coast, coined the “Ridiculously Resilient Ridge.” That ridge diverted storm systems to the north of California; the resulting period of low precipitation combined with record high temperatures to create dangerously dry wildfire conditions.\nRutgers climate scientist Jennifer Francis over the past decade has been researching the connection between changes in the Arctic and extreme weather patterns throughout the Northern Hemisphere. In recent years a growing number of climate scientists have found evidence supporting her groundbreaking research. In a 2017 paper in Nature Communications, researchers led by Ivana Cvijanovic and Ben Santer found evidence of a connection between disappearing Arctic sea ice and these high-pressure ridges in the Pacific. And in an October 2018 paper in Science Advances, scientists Michael Mann and Stefan Rahmstorf and colleagues found that depending on how human fossil fuel pollution changes in the coming years, the frequency of wavy jet stream events that often lead to high-pressure ridges off the California coast could triple by the end of the century.\nThe situation in Australia is again strikingly similar to that in California. Researchers have shown that global warming is expanding an atmospheric circulation pattern known as the Hadley cell. This circulation is caused by hot air at the equator rising and spreading toward the poles, where it begins to cool and descend, forming high pressure ridges. In Australia, this process creates what’s known as the subtropical ridge, which as CSIRO notes, has become more intense as a result of global warming expanding the Hadley cell circulation. A 2014 study, CSIRO’s David Post and colleagues reported that stronger high-pressure ridges have been decreasing rainfall in southeastern Australia in the autumn and winter. The lack of rainfall creates more dry fuel for fires and lengthens the bushfire season.\nBased on this scientific research, the latest IPCC report found in 2014 that “fire weather is projected to increase in most of southern Australia,” with days experiencing very high and extreme fire danger increasing 5–100% by 2050. And a 2015 CSIRO report concluded, “Extreme fire weather days have increased at 24 out of 38 Australian sites from 1973-2010, due to warmer and drier conditions … [forest fire danger index] increase across southeast Australia is characterised by an extension of the fire season further into spring and autumn … partly driven by temperature increases that are attributable to climate change.”\nGlobal warming will keep worsening wildfires\nSome are quick to point out that droughts and wildfires happen naturally, and the latter are often sparked by human activities. While that's true, it's also the case that human-caused climate change is responsible for making wildfires spread further and faster, by creating drier conditions and likely by changing atmospheric circulation patterns that result in less rainfall in some fire-prone regions like California and Australia.\nLast updated on by dana1981. View Archives"
  }
 ]
}